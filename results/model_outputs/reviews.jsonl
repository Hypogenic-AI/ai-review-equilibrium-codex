{"paper_id": "o9YC0B6P2m", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel formulation that extends traditional scaling laws to account for learning rate annealing effects throughout the entire training trajectory, not just at final steps", "Practical utility: the proposed method requires fitting only 1-2 training curves to predict losses under different LR schedules, significantly reducing computational cost", "Comprehensive experimental validation across different hyperparameters, model architectures, and extends to model size scaling", "Clear mathematical formulation with interpretable components (S1 for cumulative learning, S2 for annealing effects)", "Addresses a practical gap in scaling law research by incorporating LR scheduling dynamics"], "weaknesses": ["The theoretical justification for the specific functional form L(s) = L0 + A\u00b7S1^(-\u03b1) - C\u00b7S2 is not rigorously derived; appears primarily empirical", "Limited discussion of when the scaling law breaks down or its boundary conditions (e.g., very small models, extreme LR schedules)", "The relationship between S1 (area under LR curve) and S2 (LR annealing area) needs clearer definition - potential overlap or ambiguity in how these are computed", "Missing comparisons with other recent scaling law formulations that might account for training dynamics", "Insufficient analysis of the fitted parameters (L0, A, C, \u03b1) - what do they represent physically/theoretically?", "The claim of 'democratizing scaling laws' may be overstated without demonstrating accessibility improvements for smaller research groups"], "suggestions": ["Provide theoretical derivation or at least intuitive justification for why the loss should follow this specific functional form", "Include ablation studies showing what happens when S1 or S2 terms are removed to validate their necessity", "Add failure case analysis: identify scenarios where the scaling law predictions deviate significantly from actual losses", "Compare prediction accuracy against simpler baselines (e.g., linear interpolation, previous scaling laws)", "Provide clearer mathematical definitions of S1 and S2 with worked examples for common LR schedules (cosine, linear, step decay)", "Discuss the stability and uniqueness of parameter fitting - do different initialization lead to different fitted parameters?", "Include experiments on larger models (100B+ parameters) if possible to validate scalability claims", "Add practical guidelines: how many steps are needed for reliable fitting? What's the minimum training budget required?"], "summary": "This paper proposes an extended scaling law that incorporates learning rate annealing effects, formulated as L(s) = L0 + A\u00b7S1^(-\u03b1) - C\u00b7S2. The key contribution is enabling loss prediction across the entire training trajectory using only 1-2 fitted curves, rather than just at final steps. While the empirical validation is comprehensive and the practical utility is clear, the work would benefit from stronger theoretical grounding, clearer mathematical definitions, and more thorough analysis of when and why the formulation works. The paper makes a solid contribution to understanding LLM training dynamics but needs refinement in presentation and theoretical justification."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel formulation that extends traditional scaling laws to account for learning rate annealing effects throughout the entire training trajectory, not just at final steps\",\n\"Practical utility: the proposed method requires fitting only 1-2 training curves to predict losses under different LR schedules, significantly reducing computational cost\",\n\"Comprehensive experimental validation across different hyperparameters, model architectures, and extends to model size scaling\",\n\"Clear mathematical formulation with interpretable components (S1 for cumulative learning, S2 for annealing effects)\",\n\"Addresses a practical gap in scaling law research by incorporating LR scheduling dynamics\"\n],\n\"weaknesses\": [\n\"The theoretical justification for the specific functional form L(s) = L0 + A\u00b7S1^(-\u03b1) - C\u00b7S2 is not rigorously derived; appears primarily empirical\",\n\"Limited discussion of when the scaling law breaks down or its boundary conditions (e.g., very small models, extreme LR schedules)\",\n\"The relationship between S1 (area under LR curve) and S2 (LR annealing area) needs clearer definition - potential overlap or ambiguity in how these are computed\",\n\"Missing comparisons with other recent scaling law formulations that might account for training dynamics\",\n\"Insufficient analysis of the fitted parameters (L0, A, C, \u03b1) - what do they represent physically/theoretically?\",\n\"The claim of 'democratizing scaling laws' may be overstated without demonstrating accessibility improvements for smaller research groups\"\n],\n\"suggestions\": [\n\"Provide theoretical derivation or at least intuitive justification for why the loss should follow this specific functional form\",\n\"Include ablation studies showing what happens when S1 or S2 terms are removed to validate their necessity\",\n\"Add failure case analysis: identify scenarios where the scaling law predictions deviate significantly from actual losses\",\n\"Compare prediction accuracy against simpler baselines (e.g., linear interpolation, previous scaling laws)\",\n\"Provide clearer mathematical definitions of S1 and S2 with worked examples for common LR schedules (cosine, linear, step decay)\",\n\"Discuss the stability and uniqueness of parameter fitting - do different initialization lead to different fitted parameters?\",\n\"Include experiments on larger models (100B+ parameters) if possible to validate scalability claims\",\n\"Add practical guidelines: how many steps are needed for reliable fitting? What's the minimum training budget required?\"\n],\n\"summary\": \"This paper proposes an extended scaling law that incorporates learning rate annealing effects, formulated as L(s) = L0 + A\u00b7S1^(-\u03b1) - C\u00b7S2. The key contribution is enabling loss prediction across the entire training trajectory using only 1-2 fitted curves, rather than just at final steps. While the empirical validation is comprehensive and the practical utility is clear, the work would benefit from stronger theoretical grounding, clearer mathematical definitions, and more thorough analysis of when and why the formulation works. The paper makes a solid contribution to understanding LLM training dynamics but needs refinement in presentation and theoretical justification.\"\n}\n```", "usage": {"prompt_tokens": 456, "completion_tokens": 712, "total_tokens": 1168}, "timestamp": "2025-12-29T05:51:42.770052"}
{"paper_id": "o9YC0B6P2m", "model": "google/gemini-2.5-pro", "response": {"score": 7, "strengths": ["The paper proposes a novel and ambitious goal: to"]}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"The paper proposes a novel and ambitious goal: to\"\n  ]\n}\n```", "usage": {"prompt_tokens": 445, "completion_tokens": 1216, "total_tokens": 1661}, "timestamp": "2025-12-29T05:53:24.194932"}
{"paper_id": "708lti8yfI", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important problem in scientific machine learning by providing theoretical foundations for neural network approximations of PDE solutions", "Extends prior work (Chen et al. 2021) to more general elliptic and parabolic equations, including time-dependent cases", "Provides explicit dimension-dependent complexity bounds for Barron norms, which is valuable for understanding the curse of dimensionality", "Uses rigorous mathematical framework based on Green's functions and well-established PDE theory", "Establishes sufficient conditions for solutions to belong to Barron space rather than just approximating them"], "weaknesses": ["The abstract lacks clarity on the practical implications and numerical validation of the theoretical results", "No mention of experimental results or comparisons with existing methods to validate the theoretical improvements", "The conditions on PDE coefficients for solutions to belong to Barron space are not clearly stated in the abstract", "Limited discussion of how the results translate to actual neural network training and approximation quality", "The relationship between the theoretical complexity bounds and practical computational efficiency is unclear", "Does not address nonlinear PDEs, which are common in many applications"], "suggestions": ["Include numerical experiments demonstrating the practical utility of the theoretical bounds", "Provide concrete examples of PDE classes that satisfy the sufficient conditions", "Discuss the gap between theoretical Barron norm bounds and actual approximation errors in practice", "Compare the dimension-dependence of your bounds with existing results more explicitly", "Add discussion on how the results guide neural network architecture design for PDE solving", "Consider extending the framework to at least some classes of nonlinear PDEs in future work", "Clarify the computational cost of verifying whether a given PDE satisfies the sufficient conditions"], "summary": "This paper provides theoretical complexity estimates for Barron norms of solutions to linear second-order PDEs using Green's function representations. While the theoretical contributions extend prior work to more general equation classes and provide improved bounds, the paper would benefit from numerical validation and clearer discussion of practical implications. The work is mathematically rigorous but needs stronger connection to actual neural network training and performance."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important problem in scientific machine learning by providing theoretical foundations for neural network approximations of PDE solutions\",\n\"Extends prior work (Chen et al. 2021) to more general elliptic and parabolic equations, including time-dependent cases\",\n\"Provides explicit dimension-dependent complexity bounds for Barron norms, which is valuable for understanding the curse of dimensionality\",\n\"Uses rigorous mathematical framework based on Green's functions and well-established PDE theory\",\n\"Establishes sufficient conditions for solutions to belong to Barron space rather than just approximating them\"\n],\n\"weaknesses\": [\n\"The abstract lacks clarity on the practical implications and numerical validation of the theoretical results\",\n\"No mention of experimental results or comparisons with existing methods to validate the theoretical improvements\",\n\"The conditions on PDE coefficients for solutions to belong to Barron space are not clearly stated in the abstract\",\n\"Limited discussion of how the results translate to actual neural network training and approximation quality\",\n\"The relationship between the theoretical complexity bounds and practical computational efficiency is unclear\",\n\"Does not address nonlinear PDEs, which are common in many applications\"\n],\n\"suggestions\": [\n\"Include numerical experiments demonstrating the practical utility of the theoretical bounds\",\n\"Provide concrete examples of PDE classes that satisfy the sufficient conditions\",\n\"Discuss the gap between theoretical Barron norm bounds and actual approximation errors in practice\",\n\"Compare the dimension-dependence of your bounds with existing results more explicitly\",\n\"Add discussion on how the results guide neural network architecture design for PDE solving\",\n\"Consider extending the framework to at least some classes of nonlinear PDEs in future work\",\n\"Clarify the computational cost of verifying whether a given PDE satisfies the sufficient conditions\"\n],\n\"summary\": \"This paper provides theoretical complexity estimates for Barron norms of solutions to linear second-order PDEs using Green's function representations. While the theoretical contributions extend prior work to more general equation classes and provide improved bounds, the paper would benefit from numerical validation and clearer discussion of practical implications. The work is mathematically rigorous but needs stronger connection to actual neural network training and performance.\"\n}\n```", "usage": {"prompt_tokens": 339, "completion_tokens": 500, "total_tokens": 839}, "timestamp": "2025-12-29T05:53:57.277658"}
{"paper_id": "708lti8yfI", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["The paper provides a significant theoretical contribution by establishing"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"The paper provides a significant theoretical contribution by establishing\"\n  ]\n}\n```", "usage": {"prompt_tokens": 341, "completion_tokens": 1346, "total_tokens": 1687}, "timestamp": "2025-12-29T05:54:12.082288"}
{"paper_id": "X5qi6fnnw7", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses a practical and important problem: performance degradation of forward-backward (FB) representations when trained on small, non-diverse datasets", "Clear motivation for incorporating conservatism from offline RL into world model learning", "Comprehensive experimental evaluation across multiple datasets, domains, and tasks", "Achieves significant performance improvements (150% of vanilla FB) in the limited data regime", "Conservative methods show no degradation on full datasets, suggesting little downside", "Code availability for reproducibility"], "weaknesses": ["The abstract lacks clarity on what 'conservative world models' specifically means - the connection between conservatism in offline RL and FB representations needs better explanation", "Limited technical detail in the abstract about the proposed methods - unclear what modifications were made to FB representations", "The claim of 'outperforming task-specific baseline despite lacking reward labels' needs more context - this seems counterintuitive and requires explanation", "No discussion of computational costs or scalability of the proposed approach", "Missing details about what constitutes 'small datasets' and 'lack of diversity' - these are critical parameters", "The 150% improvement metric is presented without confidence intervals or statistical significance testing", "No mention of limitations or failure cases of the proposed approach"], "suggestions": ["Provide a clearer definition of 'conservative world models' in the abstract, explaining the specific mechanism by which conservatism is incorporated", "Include a brief technical description of the key algorithmic contributions", "Clarify the experimental setup: define what constitutes small vs. large datasets, and quantify diversity metrics", "Add statistical significance measures (confidence intervals, p-values) to the performance claims", "Explain the counterintuitive result of outperforming task-specific baselines without reward labels", "Discuss computational overhead and sample efficiency trade-offs", "Include a limitations section discussing when conservative FB might not be appropriate", "Provide more details on the 'family of methods' mentioned - how many variants were tested?"], "summary": "This paper proposes incorporating conservatism from offline RL into forward-backward representations to improve zero-shot RL performance on small, non-diverse datasets. While the work addresses a practical problem and shows promising empirical results (150% improvement over vanilla FB), the abstract lacks sufficient technical detail and clarity about the proposed methods. The claims are strong but need better support through statistical analysis and clearer explanation of the mechanisms. The work would benefit from more precise problem formulation and discussion of limitations."}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"Addresses a practical and important problem: performance degradation of forward-backward (FB) representations when trained on small, non-diverse datasets\",\n    \"Clear motivation for incorporating conservatism from offline RL into world model learning\",\n    \"Comprehensive experimental evaluation across multiple datasets, domains, and tasks\",\n    \"Achieves significant performance improvements (150% of vanilla FB) in the limited data regime\",\n    \"Conservative methods show no degradation on full datasets, suggesting little downside\",\n    \"Code availability for reproducibility\"\n  ],\n  \"weaknesses\": [\n    \"The abstract lacks clarity on what 'conservative world models' specifically means - the connection between conservatism in offline RL and FB representations needs better explanation\",\n    \"Limited technical detail in the abstract about the proposed methods - unclear what modifications were made to FB representations\",\n    \"The claim of 'outperforming task-specific baseline despite lacking reward labels' needs more context - this seems counterintuitive and requires explanation\",\n    \"No discussion of computational costs or scalability of the proposed approach\",\n    \"Missing details about what constitutes 'small datasets' and 'lack of diversity' - these are critical parameters\",\n    \"The 150% improvement metric is presented without confidence intervals or statistical significance testing\",\n    \"No mention of limitations or failure cases of the proposed approach\"\n  ],\n  \"suggestions\": [\n    \"Provide a clearer definition of 'conservative world models' in the abstract, explaining the specific mechanism by which conservatism is incorporated\",\n    \"Include a brief technical description of the key algorithmic contributions\",\n    \"Clarify the experimental setup: define what constitutes small vs. large datasets, and quantify diversity metrics\",\n    \"Add statistical significance measures (confidence intervals, p-values) to the performance claims\",\n    \"Explain the counterintuitive result of outperforming task-specific baselines without reward labels\",\n    \"Discuss computational overhead and sample efficiency trade-offs\",\n    \"Include a limitations section discussing when conservative FB might not be appropriate\",\n    \"Provide more details on the 'family of methods' mentioned - how many variants were tested?\"\n  ],\n  \"summary\": \"This paper proposes incorporating conservatism from offline RL into forward-backward representations to improve zero-shot RL performance on small, non-diverse datasets. While the work addresses a practical problem and shows promising empirical results (150% improvement over vanilla FB), the abstract lacks sufficient technical detail and clarity about the proposed methods. The claims are strong but need better support through statistical analysis and clearer explanation of the mechanisms. The work would benefit from more precise problem formulation and discussion of limitations.\"\n}\n```", "usage": {"prompt_tokens": 334, "completion_tokens": 612, "total_tokens": 946}, "timestamp": "2025-12-29T05:54:44.726281"}
{"paper_id": "X5qi6fnnw7", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["The paper addresses a practical and important problem"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"The paper addresses a practical and important problem\"\n  ]\n}\n```", "usage": {"prompt_tokens": 360, "completion_tokens": 1328, "total_tokens": 1688}, "timestamp": "2025-12-29T05:54:58.671084"}
{"paper_id": "4dHyH42ha7", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses a challenging and practical problem of editing 4D scenes from casual monocular videos without requiring pre-computed camera poses", "Proposes a comprehensive framework with two key modules (TPE and SPE) that tackle temporal and spatial consistency issues in scene editing", "Integrates pose-free 4D Gaussian Splatting, eliminating dependency on external SfM libraries which is a practical advantage", "Claims to provide both quantitative evaluation and user studies to validate the approach", "Progressive editing strategy appears well-motivated for handling monocular video constraints"], "weaknesses": ["Abstract lacks specific technical details about how TPE and SPE modules actually work - the mechanisms are described only at a high level", "No quantitative results or comparisons are provided in the abstract to support effectiveness claims", "The relationship between pose-free 4DGS and the editing modules is unclear - how do they interact in the progressive framework?", "Missing discussion of computational costs and runtime, which is important for practical applications", "No mention of limitations or failure cases of the proposed approach", "The claim of 'extensive experiments' is vague without specific metrics, datasets, or baseline comparisons mentioned", "Unclear how the method handles complex motions or significant camera viewpoint changes in casual videos"], "suggestions": ["Include at least one key quantitative result in the abstract to substantiate effectiveness claims (e.g., improvement percentage over baselines)", "Provide more technical detail on the TPE and SPE mechanisms - what makes them effective for consistency?", "Clarify the progressive framework: what are the stages and how do pose estimation, reconstruction, and editing interleave?", "Specify the datasets used for evaluation and mention key baseline methods compared against", "Add a brief discussion of computational requirements or efficiency compared to existing methods", "Mention at least one limitation or challenging scenario for the method to provide balanced perspective", "Explain how auxiliary novel views are generated in SPE and how they improve spatial consistency"], "summary": "This paper presents 4DEditPro, a framework for editing 4D scenes from monocular videos using text prompts without requiring pre-computed camera poses. The approach introduces Temporally Propagated Editing (TPE) and Spatially Propagated Editing (SPE) modules to ensure consistency, combined with pose-free 4D Gaussian Splatting. While the problem is well-motivated and the overall approach appears novel, the abstract lacks sufficient technical depth and concrete evidence of effectiveness. Key concerns include missing quantitative results, vague descriptions of core technical components, and absence of limitation discussion. The paper would benefit from more specific technical details and at least preliminary quantitative evidence in the abstract."}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"Addresses a challenging and practical problem of editing 4D scenes from casual monocular videos without requiring pre-computed camera poses\",\n    \"Proposes a comprehensive framework with two key modules (TPE and SPE) that tackle temporal and spatial consistency issues in scene editing\",\n    \"Integrates pose-free 4D Gaussian Splatting, eliminating dependency on external SfM libraries which is a practical advantage\",\n    \"Claims to provide both quantitative evaluation and user studies to validate the approach\",\n    \"Progressive editing strategy appears well-motivated for handling monocular video constraints\"\n  ],\n  \"weaknesses\": [\n    \"Abstract lacks specific technical details about how TPE and SPE modules actually work - the mechanisms are described only at a high level\",\n    \"No quantitative results or comparisons are provided in the abstract to support effectiveness claims\",\n    \"The relationship between pose-free 4DGS and the editing modules is unclear - how do they interact in the progressive framework?\",\n    \"Missing discussion of computational costs and runtime, which is important for practical applications\",\n    \"No mention of limitations or failure cases of the proposed approach\",\n    \"The claim of 'extensive experiments' is vague without specific metrics, datasets, or baseline comparisons mentioned\",\n    \"Unclear how the method handles complex motions or significant camera viewpoint changes in casual videos\"\n  ],\n  \"suggestions\": [\n    \"Include at least one key quantitative result in the abstract to substantiate effectiveness claims (e.g., improvement percentage over baselines)\",\n    \"Provide more technical detail on the TPE and SPE mechanisms - what makes them effective for consistency?\",\n    \"Clarify the progressive framework: what are the stages and how do pose estimation, reconstruction, and editing interleave?\",\n    \"Specify the datasets used for evaluation and mention key baseline methods compared against\",\n    \"Add a brief discussion of computational requirements or efficiency compared to existing methods\",\n    \"Mention at least one limitation or challenging scenario for the method to provide balanced perspective\",\n    \"Explain how auxiliary novel views are generated in SPE and how they improve spatial consistency\"\n  ],\n  \"summary\": \"This paper presents 4DEditPro, a framework for editing 4D scenes from monocular videos using text prompts without requiring pre-computed camera poses. The approach introduces Temporally Propagated Editing (TPE) and Spatially Propagated Editing (SPE) modules to ensure consistency, combined with pose-free 4D Gaussian Splatting. While the problem is well-motivated and the overall approach appears novel, the abstract lacks sufficient technical depth and concrete evidence of effectiveness. Key concerns include missing quantitative results, vague descriptions of core technical components, and absence of limitation discussion. The paper would benefit from more specific technical details and at least preliminary quantitative evidence in the abstract.\"\n}\n```", "usage": {"prompt_tokens": 427, "completion_tokens": 657, "total_tokens": 1084}, "timestamp": "2025-12-29T05:55:31.779681"}
{"paper_id": "4dHyH42ha7", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["Addresses a challenging and relevant problem: text-guided"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"Addresses a challenging and relevant problem: text-guided\"\n  ]\n}\n```", "usage": {"prompt_tokens": 417, "completion_tokens": 1055, "total_tokens": 1472}, "timestamp": "2025-12-29T05:55:43.622106"}
{"paper_id": "qnGir4dyu9", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel two-stage framework (V2P and P2V) that enables flexible video editing through natural language descriptions without requiring labor-intensive manual annotations", "Multi-granular spatiotemporal pooling strategy effectively captures both holistic context and fine-grained object details in video descriptions", "Comprehensive evaluation showing significant improvements: 9.4% absolute improvement in human evaluations for V2P and 49.7% relative improvement in FVD for video editing", "Versatile framework supporting multiple editing operations (removal, addition, modification) through a unified pipeline", "Auto-generated narratives reduce user burden and can be integrated with other state-of-the-art video generative models"], "weaknesses": ["The abstract lacks clarity on technical implementation details, particularly regarding the multi-granular spatiotemporal pooling mechanism", "Limited discussion of computational costs and scalability - the two-stage pipeline may be computationally expensive", "Unclear how the method handles temporal consistency and coherence across edited video frames", "The phrase 'plans to imagine new objects' suggests incomplete implementation or future work rather than current capabilities", "Missing comparison with recent video editing methods and insufficient baseline details", "No discussion of failure cases or limitations of the approach", "Evaluation metrics are mentioned but not clearly defined (e.g., what specific aspects are measured in human evaluations?)"], "suggestions": ["Provide more technical details about the multi-granular spatiotemporal pooling strategy in the abstract or ensure they are thoroughly covered in the main paper", "Include computational cost analysis and runtime comparisons with existing methods", "Clarify the temporal consistency mechanisms and provide quantitative metrics for temporal coherence", "Add ablation studies to demonstrate the contribution of each component (V2P stage, P2V stage, auto-generated narratives)", "Include more comprehensive comparisons with recent video editing baselines and state-of-the-art methods", "Discuss failure cases and limitations explicitly to provide a balanced view", "Provide clearer definitions of evaluation metrics and include more quantitative results in the abstract", "Remove or clarify future work statements ('plans to imagine') - focus on current capabilities"], "summary": "RACCooN presents an interesting two-stage framework for video editing through auto-generated natural language descriptions. The approach shows promise with significant improvements in both video description generation and editing quality. However, the paper would benefit from more technical clarity, comprehensive comparisons, computational cost analysis, and explicit discussion of limitations. The work addresses an important problem in making video editing more accessible, but needs stronger technical exposition and evaluation to meet ICLR standards."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel two-stage framework (V2P and P2V) that enables flexible video editing through natural language descriptions without requiring labor-intensive manual annotations\",\n\"Multi-granular spatiotemporal pooling strategy effectively captures both holistic context and fine-grained object details in video descriptions\",\n\"Comprehensive evaluation showing significant improvements: 9.4% absolute improvement in human evaluations for V2P and 49.7% relative improvement in FVD for video editing\",\n\"Versatile framework supporting multiple editing operations (removal, addition, modification) through a unified pipeline\",\n\"Auto-generated narratives reduce user burden and can be integrated with other state-of-the-art video generative models\"\n],\n\"weaknesses\": [\n\"The abstract lacks clarity on technical implementation details, particularly regarding the multi-granular spatiotemporal pooling mechanism\",\n\"Limited discussion of computational costs and scalability - the two-stage pipeline may be computationally expensive\",\n\"Unclear how the method handles temporal consistency and coherence across edited video frames\",\n\"The phrase 'plans to imagine new objects' suggests incomplete implementation or future work rather than current capabilities\",\n\"Missing comparison with recent video editing methods and insufficient baseline details\",\n\"No discussion of failure cases or limitations of the approach\",\n\"Evaluation metrics are mentioned but not clearly defined (e.g., what specific aspects are measured in human evaluations?)\"\n],\n\"suggestions\": [\n\"Provide more technical details about the multi-granular spatiotemporal pooling strategy in the abstract or ensure they are thoroughly covered in the main paper\",\n\"Include computational cost analysis and runtime comparisons with existing methods\",\n\"Clarify the temporal consistency mechanisms and provide quantitative metrics for temporal coherence\",\n\"Add ablation studies to demonstrate the contribution of each component (V2P stage, P2V stage, auto-generated narratives)\",\n\"Include more comprehensive comparisons with recent video editing baselines and state-of-the-art methods\",\n\"Discuss failure cases and limitations explicitly to provide a balanced view\",\n\"Provide clearer definitions of evaluation metrics and include more quantitative results in the abstract\",\n\"Remove or clarify future work statements ('plans to imagine') - focus on current capabilities\"\n],\n\"summary\": \"RACCooN presents an interesting two-stage framework for video editing through auto-generated natural language descriptions. The approach shows promise with significant improvements in both video description generation and editing quality. However, the paper would benefit from more technical clarity, comprehensive comparisons, computational cost analysis, and explicit discussion of limitations. The work addresses an important problem in making video editing more accessible, but needs stronger technical exposition and evaluation to meet ICLR standards.\"\n}\n```", "usage": {"prompt_tokens": 503, "completion_tokens": 610, "total_tokens": 1113}, "timestamp": "2025-12-29T05:56:14.583786"}
{"paper_id": "qnGir4dyu9", "model": "google/gemini-2.5-pro", "response": {"score": 7, "strengths": ["The proposed Video-to-Paragraph-to"]}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"The proposed Video-to-Paragraph-to\"\n  ]\n}\n```", "usage": {"prompt_tokens": 501, "completion_tokens": 1084, "total_tokens": 1585}, "timestamp": "2025-12-29T05:56:27.198400"}
{"paper_id": "wVmShpwtY0", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel integration of Hamiltonian dynamics with Bayesian optimization for protein engineering, which is an interesting and potentially impactful approach", "Structure-aware methodology that addresses a key limitation of sequence-only methods by incorporating structural constraints", "Two-stage encoder-decoder framework for learning structure-function relationships appears well-motivated", "Comprehensive experimental evaluation across multiple protein optimization tasks", "Clear motivation addressing epistasis effects and high-dimensional complexity in protein optimization"], "weaknesses": ["The position discretization procedure from continuous Hamiltonian dynamics to discrete sequences needs more rigorous theoretical justification and analysis of information loss", "Limited discussion of computational complexity and scalability - Hamiltonian dynamics simulations can be expensive, especially with structure prediction", "Unclear how the method handles cases where structure prediction is unreliable or when multiple conformations exist", "The approximated posterior formulation and its relationship to the true posterior needs more theoretical grounding", "Missing important experimental details: how many HMC steps, what are the hyperparameters, sensitivity analysis", "No wet-lab validation or discussion of experimental feasibility of proposed sequences", "Comparison baselines may not include recent structure-aware protein design methods (e.g., ProteinMPNN, ESM-based approaches)"], "suggestions": ["Provide theoretical analysis of the discretization procedure and bounds on approximation error", "Include computational cost comparisons (wall-clock time, memory) with baselines", "Add ablation studies isolating the contribution of: (1) Hamiltonian dynamics vs standard MCMC, (2) structure-awareness, (3) two-stage architecture", "Discuss failure modes and when structure-based optimization might not be beneficial", "Include analysis of the diversity-fitness tradeoff in proposed sequences", "Add visualization of the learned landscape and trajectories in latent space", "Compare with recent structure-aware protein design methods", "Provide guidelines on hyperparameter selection and sensitivity analysis", "Discuss potential for experimental validation and any preliminary wet-lab results if available"], "summary": "This paper presents HADES, a structure-aware Bayesian optimization method for protein engineering that uses Hamiltonian dynamics to efficiently explore the sequence space. While the core idea of combining Hamiltonian Monte Carlo with structure-aware modeling is novel and promising, the paper would benefit from stronger theoretical justification of the discretization procedure, more comprehensive computational analysis, and comparison with recent structure-based protein design methods. The experimental results show promise, but the lack of wet-lab validation and limited analysis of computational costs are concerns for practical applicability."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel integration of Hamiltonian dynamics with Bayesian optimization for protein engineering, which is an interesting and potentially impactful approach\",\n\"Structure-aware methodology that addresses a key limitation of sequence-only methods by incorporating structural constraints\",\n\"Two-stage encoder-decoder framework for learning structure-function relationships appears well-motivated\",\n\"Comprehensive experimental evaluation across multiple protein optimization tasks\",\n\"Clear motivation addressing epistasis effects and high-dimensional complexity in protein optimization\"\n],\n\"weaknesses\": [\n\"The position discretization procedure from continuous Hamiltonian dynamics to discrete sequences needs more rigorous theoretical justification and analysis of information loss\",\n\"Limited discussion of computational complexity and scalability - Hamiltonian dynamics simulations can be expensive, especially with structure prediction\",\n\"Unclear how the method handles cases where structure prediction is unreliable or when multiple conformations exist\",\n\"The approximated posterior formulation and its relationship to the true posterior needs more theoretical grounding\",\n\"Missing important experimental details: how many HMC steps, what are the hyperparameters, sensitivity analysis\",\n\"No wet-lab validation or discussion of experimental feasibility of proposed sequences\",\n\"Comparison baselines may not include recent structure-aware protein design methods (e.g., ProteinMPNN, ESM-based approaches)\"\n],\n\"suggestions\": [\n\"Provide theoretical analysis of the discretization procedure and bounds on approximation error\",\n\"Include computational cost comparisons (wall-clock time, memory) with baselines\",\n\"Add ablation studies isolating the contribution of: (1) Hamiltonian dynamics vs standard MCMC, (2) structure-awareness, (3) two-stage architecture\",\n\"Discuss failure modes and when structure-based optimization might not be beneficial\",\n\"Include analysis of the diversity-fitness tradeoff in proposed sequences\",\n\"Add visualization of the learned landscape and trajectories in latent space\",\n\"Compare with recent structure-aware protein design methods\",\n\"Provide guidelines on hyperparameter selection and sensitivity analysis\",\n\"Discuss potential for experimental validation and any preliminary wet-lab results if available\"\n],\n\"summary\": \"This paper presents HADES, a structure-aware Bayesian optimization method for protein engineering that uses Hamiltonian dynamics to efficiently explore the sequence space. While the core idea of combining Hamiltonian Monte Carlo with structure-aware modeling is novel and promising, the paper would benefit from stronger theoretical justification of the discretization procedure, more comprehensive computational analysis, and comparison with recent structure-based protein design methods. The experimental results show promise, but the lack of wet-lab validation and limited analysis of computational costs are concerns for practical applicability.\"\n}\n```", "usage": {"prompt_tokens": 313, "completion_tokens": 607, "total_tokens": 920}, "timestamp": "2025-12-29T05:57:02.124562"}
{"paper_id": "wVmShpwtY0", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["The proposed method"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"The proposed method\"\n  ]\n}\n```", "usage": {"prompt_tokens": 303, "completion_tokens": 1331, "total_tokens": 1634}, "timestamp": "2025-12-29T05:57:17.364233"}
{"paper_id": "nS2DBNydCC", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses two fundamental problems in vector quantization (training instability and codebook collapse) with a principled approach", "Uses Wasserstein distance for distribution matching, which is theoretically well-motivated for aligning feature and codebook distributions", "Claims to achieve near 100% codebook utilization, which would be a significant practical improvement", "Promises both empirical and theoretical validation of the approach"], "weaknesses": ["Abstract lacks concrete details about the proposed method - how exactly is Wasserstein distance incorporated into the training objective?", "No experimental results or comparisons with existing methods (e.g., VQ-VAE, residual VQ, product quantization) are provided", "Missing information about computational overhead of computing Wasserstein distance during training", "The claim of 'near 100% codebook utilization' needs quantitative support and comparison with baselines", "Unclear how the method handles the discrete optimization problem inherent in vector quantization", "No discussion of how this approach compares to other solutions for codebook collapse (e.g., codebook reset, exponential moving average updates)", "The connection between distribution mismatch and gradient gap is asserted but not clearly explained"], "suggestions": ["Provide specific formulation of the loss function incorporating Wasserstein distance", "Include quantitative results on standard benchmarks (e.g., image reconstruction, speech compression)", "Compare codebook utilization rates with VQ-VAE and other baseline methods", "Discuss computational complexity and training time compared to standard VQ methods", "Clarify whether you use optimal transport, sliced Wasserstein, or another variant", "Add ablation studies showing the contribution of distribution matching vs. other components", "Provide theoretical analysis or bounds on quantization error under distribution matching", "Include visualizations of learned codebooks and feature distributions"], "summary": "This paper proposes using Wasserstein distance to match feature and codebook distributions in vector quantization, aiming to solve training instability and codebook collapse. While the core idea is interesting and theoretically motivated, the abstract lacks sufficient technical detail and empirical evidence to evaluate the contribution. The paper would benefit from concrete methodology description, comprehensive experiments, and clear comparisons with existing approaches to vector quantization."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses two fundamental problems in vector quantization (training instability and codebook collapse) with a principled approach\",\n\"Uses Wasserstein distance for distribution matching, which is theoretically well-motivated for aligning feature and codebook distributions\",\n\"Claims to achieve near 100% codebook utilization, which would be a significant practical improvement\",\n\"Promises both empirical and theoretical validation of the approach\"\n],\n\"weaknesses\": [\n\"Abstract lacks concrete details about the proposed method - how exactly is Wasserstein distance incorporated into the training objective?\",\n\"No experimental results or comparisons with existing methods (e.g., VQ-VAE, residual VQ, product quantization) are provided\",\n\"Missing information about computational overhead of computing Wasserstein distance during training\",\n\"The claim of 'near 100% codebook utilization' needs quantitative support and comparison with baselines\",\n\"Unclear how the method handles the discrete optimization problem inherent in vector quantization\",\n\"No discussion of how this approach compares to other solutions for codebook collapse (e.g., codebook reset, exponential moving average updates)\",\n\"The connection between distribution mismatch and gradient gap is asserted but not clearly explained\"\n],\n\"suggestions\": [\n\"Provide specific formulation of the loss function incorporating Wasserstein distance\",\n\"Include quantitative results on standard benchmarks (e.g., image reconstruction, speech compression)\",\n\"Compare codebook utilization rates with VQ-VAE and other baseline methods\",\n\"Discuss computational complexity and training time compared to standard VQ methods\",\n\"Clarify whether you use optimal transport, sliced Wasserstein, or another variant\",\n\"Add ablation studies showing the contribution of distribution matching vs. other components\",\n\"Provide theoretical analysis or bounds on quantization error under distribution matching\",\n\"Include visualizations of learned codebooks and feature distributions\"\n],\n\"summary\": \"This paper proposes using Wasserstein distance to match feature and codebook distributions in vector quantization, aiming to solve training instability and codebook collapse. While the core idea is interesting and theoretically motivated, the abstract lacks sufficient technical detail and empirical evidence to evaluate the contribution. The paper would benefit from concrete methodology description, comprehensive experiments, and clear comparisons with existing approaches to vector quantization.\"\n}\n```", "usage": {"prompt_tokens": 277, "completion_tokens": 536, "total_tokens": 813}, "timestamp": "2025-12-29T05:57:46.140049"}
{"paper_id": "nS2DBNydCC", "model": "google/gemini-2.5-pro", "response": {"score": 7, "strengths": ["Addresses two critical and well-known problems"]}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses two critical and well-known problems\"\n  ]\n}\n```", "usage": {"prompt_tokens": 289, "completion_tokens": 1306, "total_tokens": 1595}, "timestamp": "2025-12-29T05:57:59.409695"}
{"paper_id": "DiWRG9JTWZ", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important and understudied problem of spurious correlation in few-shot classification, which is highly relevant for real-world deployment", "Introduces MetaCoCo, a new benchmark dataset collected from real-world scenarios to evaluate spurious-correlation shifts in FSC", "Proposes a novel metric using CLIP to quantify the extent of spurious-correlation shifts", "Provides comprehensive experimental evaluation of state-of-the-art methods across multiple paradigms (FSC, cross-domain, self-supervised learning)", "Open-sources the benchmark and code to facilitate future research"], "weaknesses": ["The paper lacks clarity in distinguishing between spurious correlation and domain shift - the conceptual boundary seems blurred in some examples", "Limited details on the dataset construction process, including how spurious correlations were identified and validated in real-world scenarios", "The proposed CLIP-based metric for quantifying spurious correlation needs more theoretical justification and validation", "Missing ablation studies on the metric design choices and sensitivity analysis", "The paper does not propose new methods to address spurious correlation in FSC, only benchmarking existing approaches", "Insufficient discussion on what makes certain spurious correlations more challenging than others", "Limited analysis on why existing methods fail and what properties would make methods more robust to spurious correlations"], "suggestions": ["Provide clearer theoretical framework distinguishing spurious correlation from domain shift with formal definitions", "Include more detailed documentation of dataset construction methodology, annotation process, and quality control measures", "Validate the proposed CLIP-based metric against human judgments or alternative metrics", "Add ablation studies examining different components of the metric and their contributions", "Provide baseline methods or adaptations specifically designed to handle spurious correlations", "Include analysis of dataset characteristics: what types of spurious correlations exist, their difficulty levels, and failure modes", "Discuss potential solutions or directions for developing robust FSC methods based on the benchmark findings", "Add statistical analysis of the benchmark properties and comparison with existing FSC benchmarks"], "summary": "This paper introduces MetaCoCo, a new benchmark for evaluating few-shot classification under spurious-correlation shifts. While the problem is important and the benchmark addresses a gap in existing evaluation protocols, the paper would benefit from clearer conceptual framing, more rigorous dataset construction documentation, and deeper analysis of the results. The contribution is primarily empirical (a new benchmark) rather than methodological, which limits its impact. The CLIP-based metric is interesting but needs stronger validation. Overall, this is a solid benchmark paper that could facilitate future research, but it needs refinement in presentation and analysis."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important and understudied problem of spurious correlation in few-shot classification, which is highly relevant for real-world deployment\",\n\"Introduces MetaCoCo, a new benchmark dataset collected from real-world scenarios to evaluate spurious-correlation shifts in FSC\",\n\"Proposes a novel metric using CLIP to quantify the extent of spurious-correlation shifts\",\n\"Provides comprehensive experimental evaluation of state-of-the-art methods across multiple paradigms (FSC, cross-domain, self-supervised learning)\",\n\"Open-sources the benchmark and code to facilitate future research\"\n],\n\"weaknesses\": [\n\"The paper lacks clarity in distinguishing between spurious correlation and domain shift - the conceptual boundary seems blurred in some examples\",\n\"Limited details on the dataset construction process, including how spurious correlations were identified and validated in real-world scenarios\",\n\"The proposed CLIP-based metric for quantifying spurious correlation needs more theoretical justification and validation\",\n\"Missing ablation studies on the metric design choices and sensitivity analysis\",\n\"The paper does not propose new methods to address spurious correlation in FSC, only benchmarking existing approaches\",\n\"Insufficient discussion on what makes certain spurious correlations more challenging than others\",\n\"Limited analysis on why existing methods fail and what properties would make methods more robust to spurious correlations\"\n],\n\"suggestions\": [\n\"Provide clearer theoretical framework distinguishing spurious correlation from domain shift with formal definitions\",\n\"Include more detailed documentation of dataset construction methodology, annotation process, and quality control measures\",\n\"Validate the proposed CLIP-based metric against human judgments or alternative metrics\",\n\"Add ablation studies examining different components of the metric and their contributions\",\n\"Provide baseline methods or adaptations specifically designed to handle spurious correlations\",\n\"Include analysis of dataset characteristics: what types of spurious correlations exist, their difficulty levels, and failure modes\",\n\"Discuss potential solutions or directions for developing robust FSC methods based on the benchmark findings\",\n\"Add statistical analysis of the benchmark properties and comparison with existing FSC benchmarks\"\n],\n\"summary\": \"This paper introduces MetaCoCo, a new benchmark for evaluating few-shot classification under spurious-correlation shifts. While the problem is important and the benchmark addresses a gap in existing evaluation protocols, the paper would benefit from clearer conceptual framing, more rigorous dataset construction documentation, and deeper analysis of the results. The contribution is primarily empirical (a new benchmark) rather than methodological, which limits its impact. The CLIP-based metric is interesting but needs stronger validation. Overall, this is a solid benchmark paper that could facilitate future research, but it needs refinement in presentation and analysis.\"\n}\n```", "usage": {"prompt_tokens": 474, "completion_tokens": 615, "total_tokens": 1089}, "timestamp": "2025-12-29T05:58:32.773994"}
{"paper_id": "DiWRG9JTWZ", "model": "google/gemini-2.5-pro", "response": {"score": 7, "strengths": ["Addresses a critical and under-explored problem in"]}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses a critical and under-explored problem in\"\n  ]\n}\n```", "usage": {"prompt_tokens": 473, "completion_tokens": 868, "total_tokens": 1341}, "timestamp": "2025-12-29T05:58:42.803163"}
{"paper_id": "ZtTgoomrT1", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses a well-motivated limitation of FNO by identifying its bias toward low-frequency components through both empirical observation and theoretical analysis", "Proposes a novel integration of translational equivariant attention mechanisms into the Fourier Neural Operator framework", "Demonstrates improved performance across multiple PDE benchmarks including multiscale elliptic equations and Navier-Stokes equations", "Particularly effective for PDEs with rapidly varying coefficients and highly oscillatory solutions, which is a practically important problem class", "Provides both theoretical justification and empirical validation for the proposed approach"], "weaknesses": ["The theoretical analysis of why FNO is limited to low-frequency approximation could be more rigorous and detailed", "Computational complexity and overhead of the attention mechanism compared to standard FNO is not thoroughly discussed", "Limited discussion of failure cases or scenarios where the proposed method may not provide significant improvements", "The connection between translational equivariance and high-frequency feature capture could be explained more clearly", "Ablation studies on different components of the attention mechanism appear to be missing or insufficient", "Comparison with other recent neural operator variants beyond FNO is limited"], "suggestions": ["Provide a more detailed theoretical analysis with formal proofs or lemmas explaining the low-frequency bias of FNO", "Include comprehensive computational cost analysis comparing TE-FNO with FNO and other baselines in terms of training time, inference time, and memory usage", "Add ablation studies systematically removing or modifying components of the equivariant attention to understand their individual contributions", "Compare with other recent neural operator methods (e.g., DeepONet, Graph Neural Operators) to better position the contribution", "Discuss limitations and failure modes of the proposed approach more explicitly", "Provide visualization of learned attention patterns to give intuition about how the method captures high-frequency features", "Include experiments on a wider range of PDE types to demonstrate generalizability"], "summary": "This paper proposes TE-FNO, which enhances Fourier Neural Operators by incorporating translational equivariant attention to better capture high-frequency features in PDE solutions. The work is motivated by identifying FNO's limitation in approximating high-frequency components. While the approach shows promising empirical results on several benchmarks, particularly for PDEs with rapidly varying coefficients, the paper would benefit from more rigorous theoretical analysis, comprehensive computational cost evaluation, and broader experimental comparisons. The core idea is sound and addresses a real limitation, but the execution and presentation could be strengthened."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses a well-motivated limitation of FNO by identifying its bias toward low-frequency components through both empirical observation and theoretical analysis\",\n\"Proposes a novel integration of translational equivariant attention mechanisms into the Fourier Neural Operator framework\",\n\"Demonstrates improved performance across multiple PDE benchmarks including multiscale elliptic equations and Navier-Stokes equations\",\n\"Particularly effective for PDEs with rapidly varying coefficients and highly oscillatory solutions, which is a practically important problem class\",\n\"Provides both theoretical justification and empirical validation for the proposed approach\"\n],\n\"weaknesses\": [\n\"The theoretical analysis of why FNO is limited to low-frequency approximation could be more rigorous and detailed\",\n\"Computational complexity and overhead of the attention mechanism compared to standard FNO is not thoroughly discussed\",\n\"Limited discussion of failure cases or scenarios where the proposed method may not provide significant improvements\",\n\"The connection between translational equivariance and high-frequency feature capture could be explained more clearly\",\n\"Ablation studies on different components of the attention mechanism appear to be missing or insufficient\",\n\"Comparison with other recent neural operator variants beyond FNO is limited\"\n],\n\"suggestions\": [\n\"Provide a more detailed theoretical analysis with formal proofs or lemmas explaining the low-frequency bias of FNO\",\n\"Include comprehensive computational cost analysis comparing TE-FNO with FNO and other baselines in terms of training time, inference time, and memory usage\",\n\"Add ablation studies systematically removing or modifying components of the equivariant attention to understand their individual contributions\",\n\"Compare with other recent neural operator methods (e.g., DeepONet, Graph Neural Operators) to better position the contribution\",\n\"Discuss limitations and failure modes of the proposed approach more explicitly\",\n\"Provide visualization of learned attention patterns to give intuition about how the method captures high-frequency features\",\n\"Include experiments on a wider range of PDE types to demonstrate generalizability\"\n],\n\"summary\": \"This paper proposes TE-FNO, which enhances Fourier Neural Operators by incorporating translational equivariant attention to better capture high-frequency features in PDE solutions. The work is motivated by identifying FNO's limitation in approximating high-frequency components. While the approach shows promising empirical results on several benchmarks, particularly for PDEs with rapidly varying coefficients, the paper would benefit from more rigorous theoretical analysis, comprehensive computational cost evaluation, and broader experimental comparisons. The core idea is sound and addresses a real limitation, but the execution and presentation could be strengthened.\"\n}\n```", "usage": {"prompt_tokens": 385, "completion_tokens": 605, "total_tokens": 990}, "timestamp": "2025-12-29T05:59:12.455153"}
{"paper_id": "ZtTgoomrT1", "model": "google/gemini-2.5-pro", "response": {"score": 7, "strengths": ["The paper identifies a clear and relevant limitation"]}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"The paper identifies a clear and relevant limitation\"\n  ]\n}\n```", "usage": {"prompt_tokens": 356, "completion_tokens": 1178, "total_tokens": 1534}, "timestamp": "2025-12-29T05:59:24.801534"}
{"paper_id": "sOte83GogU", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important and underexplored problem of downsampling in group equivariant CNNs with proper anti-aliasing", "Provides a principled mathematical framework grounded in sampling theory and group representation theory", "The approach generalizes classical downsampling - recovers standard low-pass filtering for cyclic groups", "Includes both theoretical contributions (subgroup selection algorithm, bandlimited signal definition) and empirical validation", "Demonstrates improvements in accuracy, equivariance preservation, and model efficiency on image classification tasks"], "weaknesses": ["Limited experimental validation - only image classification tasks are tested; broader applications would strengthen claims", "The subgroup selection algorithm (part a) lacks detailed complexity analysis and may not scale to very large groups", "Unclear how the method performs compared to other recent equivariant downsampling approaches in the literature", "The notion of 'bandlimited-ness' for general finite groups may be difficult to verify or interpret in practice", "Missing ablation studies to isolate the contribution of anti-aliasing vs. subgroup selection", "Computational overhead of the proposed anti-aliasing operation is not thoroughly analyzed"], "suggestions": ["Provide computational complexity analysis for the subgroup selection algorithm and anti-aliasing operation", "Include comparisons with other equivariant downsampling methods beyond standard pooling", "Add ablation studies separating the effects of subgroup choice and anti-aliasing filtering", "Extend experiments to other domains (e.g., 3D data, graphs) to demonstrate generality", "Provide more intuition and visualization of what 'bandlimited' means for non-cyclic groups", "Discuss limitations and failure cases where the method may not be applicable or beneficial", "Include runtime and memory comparisons with baseline methods"], "summary": "This paper presents a theoretically grounded approach to downsampling in group equivariant CNNs that incorporates anti-aliasing. The work makes solid theoretical contributions by providing algorithms for subgroup selection and defining bandlimited signals on finite groups, generalizing classical sampling theory. The experimental results show promise, but the evaluation is somewhat limited in scope. The paper would benefit from more comprehensive experiments, complexity analysis, and comparisons with related work. Overall, this is a good contribution to equivariant deep learning with room for strengthening the empirical validation."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important and underexplored problem of downsampling in group equivariant CNNs with proper anti-aliasing\",\n\"Provides a principled mathematical framework grounded in sampling theory and group representation theory\",\n\"The approach generalizes classical downsampling - recovers standard low-pass filtering for cyclic groups\",\n\"Includes both theoretical contributions (subgroup selection algorithm, bandlimited signal definition) and empirical validation\",\n\"Demonstrates improvements in accuracy, equivariance preservation, and model efficiency on image classification tasks\"\n],\n\"weaknesses\": [\n\"Limited experimental validation - only image classification tasks are tested; broader applications would strengthen claims\",\n\"The subgroup selection algorithm (part a) lacks detailed complexity analysis and may not scale to very large groups\",\n\"Unclear how the method performs compared to other recent equivariant downsampling approaches in the literature\",\n\"The notion of 'bandlimited-ness' for general finite groups may be difficult to verify or interpret in practice\",\n\"Missing ablation studies to isolate the contribution of anti-aliasing vs. subgroup selection\",\n\"Computational overhead of the proposed anti-aliasing operation is not thoroughly analyzed\"\n],\n\"suggestions\": [\n\"Provide computational complexity analysis for the subgroup selection algorithm and anti-aliasing operation\",\n\"Include comparisons with other equivariant downsampling methods beyond standard pooling\",\n\"Add ablation studies separating the effects of subgroup choice and anti-aliasing filtering\",\n\"Extend experiments to other domains (e.g., 3D data, graphs) to demonstrate generality\",\n\"Provide more intuition and visualization of what 'bandlimited' means for non-cyclic groups\",\n\"Discuss limitations and failure cases where the method may not be applicable or beneficial\",\n\"Include runtime and memory comparisons with baseline methods\"\n],\n\"summary\": \"This paper presents a theoretically grounded approach to downsampling in group equivariant CNNs that incorporates anti-aliasing. The work makes solid theoretical contributions by providing algorithms for subgroup selection and defining bandlimited signals on finite groups, generalizing classical sampling theory. The experimental results show promise, but the evaluation is somewhat limited in scope. The paper would benefit from more comprehensive experiments, complexity analysis, and comparisons with related work. Overall, this is a good contribution to equivariant deep learning with room for strengthening the empirical validation.\"\n}\n```", "usage": {"prompt_tokens": 361, "completion_tokens": 559, "total_tokens": 920}, "timestamp": "2025-12-29T05:59:54.237457"}
{"paper_id": "sOte83GogU", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["The paper addresses a well-defined and"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"The paper addresses a well-defined and\"\n  ]\n}\n```", "usage": {"prompt_tokens": 351, "completion_tokens": 1309, "total_tokens": 1660}, "timestamp": "2025-12-29T06:00:09.386476"}
{"paper_id": "OUkZXbbwQr", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel approach combining debate-based reward modeling with reinforcement learning for justifiable decision-making", "Addresses an important problem of accountability and interpretability in high-stakes domains like healthcare", "Demonstrates practical application in sepsis treatment, a clinically relevant domain", "Shows that debate-based feedback can approximate ideal judge feedback while using less information", "Comprehensive experimental evaluation comparing multiple baselines and ablations"], "weaknesses": ["Limited theoretical analysis of the debate game convergence properties and optimality guarantees", "The proxy judge model is not well-specified - unclear how it differs from human judgment and what biases it may introduce", "Scalability concerns: debate games may be computationally expensive for complex domains", "Evaluation relies heavily on proxy judges rather than actual human evaluations, limiting claims about alignment with human preferences", "Missing details on how the debate game handles ties or ambiguous evidence", "The paper does not adequately discuss failure modes or when the debate mechanism might produce misleading justifications"], "suggestions": ["Provide theoretical analysis of debate game properties (e.g., Nash equilibrium characterization, convergence guarantees)", "Include actual human evaluation studies to validate the proxy judge assumptions", "Discuss computational complexity and provide runtime comparisons", "Add ablation studies on debate length, number of evidence pieces, and judge model architecture", "Analyze failure cases where debate-based rewards lead to worse or misleading policies", "Compare with other interpretability methods (e.g., attention mechanisms, saliency maps) more thoroughly", "Provide more details on the state representation and what information is available to debaters vs. judges", "Discuss potential adversarial attacks on the debate mechanism"], "summary": "This paper presents an interesting approach to learning justifiable policies through debate-based reward modeling. The core idea of using adversarial debate to surface relevant evidence for decision justification is novel and addresses important concerns in accountable AI. The application to sepsis treatment demonstrates practical relevance. However, the work would benefit from stronger theoretical foundations, more rigorous human evaluation, and deeper analysis of failure modes and limitations. The reliance on proxy judges rather than human evaluation is a significant limitation for claims about human alignment."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel approach combining debate-based reward modeling with reinforcement learning for justifiable decision-making\",\n\"Addresses an important problem of accountability and interpretability in high-stakes domains like healthcare\",\n\"Demonstrates practical application in sepsis treatment, a clinically relevant domain\",\n\"Shows that debate-based feedback can approximate ideal judge feedback while using less information\",\n\"Comprehensive experimental evaluation comparing multiple baselines and ablations\"\n],\n\"weaknesses\": [\n\"Limited theoretical analysis of the debate game convergence properties and optimality guarantees\",\n\"The proxy judge model is not well-specified - unclear how it differs from human judgment and what biases it may introduce\",\n\"Scalability concerns: debate games may be computationally expensive for complex domains\",\n\"Evaluation relies heavily on proxy judges rather than actual human evaluations, limiting claims about alignment with human preferences\",\n\"Missing details on how the debate game handles ties or ambiguous evidence\",\n\"The paper does not adequately discuss failure modes or when the debate mechanism might produce misleading justifications\"\n],\n\"suggestions\": [\n\"Provide theoretical analysis of debate game properties (e.g., Nash equilibrium characterization, convergence guarantees)\",\n\"Include actual human evaluation studies to validate the proxy judge assumptions\",\n\"Discuss computational complexity and provide runtime comparisons\",\n\"Add ablation studies on debate length, number of evidence pieces, and judge model architecture\",\n\"Analyze failure cases where debate-based rewards lead to worse or misleading policies\",\n\"Compare with other interpretability methods (e.g., attention mechanisms, saliency maps) more thoroughly\",\n\"Provide more details on the state representation and what information is available to debaters vs. judges\",\n\"Discuss potential adversarial attacks on the debate mechanism\"\n],\n\"summary\": \"This paper presents an interesting approach to learning justifiable policies through debate-based reward modeling. The core idea of using adversarial debate to surface relevant evidence for decision justification is novel and addresses important concerns in accountable AI. The application to sepsis treatment demonstrates practical relevance. However, the work would benefit from stronger theoretical foundations, more rigorous human evaluation, and deeper analysis of failure modes and limitations. The reliance on proxy judges rather than human evaluation is a significant limitation for claims about human alignment.\"\n}\n```", "usage": {"prompt_tokens": 441, "completion_tokens": 517, "total_tokens": 958}, "timestamp": "2025-12-29T06:00:46.800166"}
{"paper_id": "OUkZXbbwQr", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Addresses the critical and timely problem of justifiable and accountable"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses the critical and timely problem of justifiable and accountable\"\n  ]\n}\n```", "usage": {"prompt_tokens": 441, "completion_tokens": 1219, "total_tokens": 1660}, "timestamp": "2025-12-29T06:00:59.991465"}
{"paper_id": "cNi2EJ8OCh", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important and underexplored problem: functional data classification under local differential privacy (LDP)", "Proposes novel techniques including 'model reversal' and strategic sample allocation for weak classifier construction", "Considers both single-server and multi-server (federated) settings, providing comprehensive coverage", "Leverages the natural structure of functional data (basis function representation) to handle infinite dimensionality", "Experimental validation demonstrates performance improvements"], "weaknesses": ["The abstract lacks clarity on what 'model reversal' actually means - this key contribution needs better explanation", "No theoretical analysis or privacy guarantees are mentioned in the abstract - crucial for LDP work", "The connection between basis function approximation and LDP mechanism design is unclear", "Missing details on computational complexity and scalability", "No comparison baseline mentioned - unclear what 'significantly boost' means quantitatively", "The heterogeneous multi-server setting is mentioned but not well-motivated or explained", "Potential privacy leakage through model averaging and federated learning is not discussed"], "suggestions": ["Provide a clear, intuitive explanation of 'model reversal' in the abstract or introduction", "Include theoretical privacy guarantees (epsilon values, composition bounds) and utility analysis", "Clarify how the basis function approximation error interacts with LDP noise", "Add computational complexity analysis and discuss scalability to high-dimensional functional data", "Include specific baseline comparisons and quantitative performance metrics in the abstract", "Better motivate the multi-server setting - when is this realistic and why is heterogeneity important?", "Discuss privacy implications of model averaging and federated learning more thoroughly", "Consider adding convergence guarantees for the federated learning approach", "Provide ablation studies to isolate the contribution of each component (model reversal, allocation strategy, etc.)"], "summary": "This paper proposes methods for functional data classification under local differential privacy, introducing novel techniques like 'model reversal' and strategic sample allocation. While the problem is important and the approach shows promise, the paper would benefit from clearer explanations of key concepts, stronger theoretical foundations including privacy and utility guarantees, and more rigorous experimental evaluation with proper baselines. The multi-server setting needs better motivation and the privacy implications of the proposed techniques require deeper analysis."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important and underexplored problem: functional data classification under local differential privacy (LDP)\",\n\"Proposes novel techniques including 'model reversal' and strategic sample allocation for weak classifier construction\",\n\"Considers both single-server and multi-server (federated) settings, providing comprehensive coverage\",\n\"Leverages the natural structure of functional data (basis function representation) to handle infinite dimensionality\",\n\"Experimental validation demonstrates performance improvements\"\n],\n\"weaknesses\": [\n\"The abstract lacks clarity on what 'model reversal' actually means - this key contribution needs better explanation\",\n\"No theoretical analysis or privacy guarantees are mentioned in the abstract - crucial for LDP work\",\n\"The connection between basis function approximation and LDP mechanism design is unclear\",\n\"Missing details on computational complexity and scalability\",\n\"No comparison baseline mentioned - unclear what 'significantly boost' means quantitatively\",\n\"The heterogeneous multi-server setting is mentioned but not well-motivated or explained\",\n\"Potential privacy leakage through model averaging and federated learning is not discussed\"\n],\n\"suggestions\": [\n\"Provide a clear, intuitive explanation of 'model reversal' in the abstract or introduction\",\n\"Include theoretical privacy guarantees (epsilon values, composition bounds) and utility analysis\",\n\"Clarify how the basis function approximation error interacts with LDP noise\",\n\"Add computational complexity analysis and discuss scalability to high-dimensional functional data\",\n\"Include specific baseline comparisons and quantitative performance metrics in the abstract\",\n\"Better motivate the multi-server setting - when is this realistic and why is heterogeneity important?\",\n\"Discuss privacy implications of model averaging and federated learning more thoroughly\",\n\"Consider adding convergence guarantees for the federated learning approach\",\n\"Provide ablation studies to isolate the contribution of each component (model reversal, allocation strategy, etc.)\"\n],\n\"summary\": \"This paper proposes methods for functional data classification under local differential privacy, introducing novel techniques like 'model reversal' and strategic sample allocation. While the problem is important and the approach shows promise, the paper would benefit from clearer explanations of key concepts, stronger theoretical foundations including privacy and utility guarantees, and more rigorous experimental evaluation with proper baselines. The multi-server setting needs better motivation and the privacy implications of the proposed techniques require deeper analysis.\"\n}\n```", "usage": {"prompt_tokens": 345, "completion_tokens": 547, "total_tokens": 892}, "timestamp": "2025-12-29T06:01:34.541656"}
{"paper_id": "cNi2EJ8OCh", "model": "google/gemini-2.5-pro", "response": {"score": 4, "strengths": ["The paper addresses a novel and important problem: functional"]}, "raw": "```json\n{\n  \"score\": 4,\n  \"strengths\": [\n    \"The paper addresses a novel and important problem: functional\"\n  ]\n}\n```", "usage": {"prompt_tokens": 360, "completion_tokens": 1156, "total_tokens": 1516}, "timestamp": "2025-12-29T06:01:47.456151"}
{"paper_id": "55oi1LCdDL", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important problem of catastrophic forgetting in domain-incremental learning with pre-trained models", "Proposes a dual consolidation approach that operates at both representation and classifier levels, which is a comprehensive solution", "Demonstrates state-of-the-art performance across four benchmark datasets", "The method is well-motivated with clear identification of the representation-classifier mismatch problem", "Includes both backbone merging and classifier consolidation using semantic information"], "weaknesses": ["The abstract lacks clarity on the technical details of how backbone merging is performed and what specific mechanism is used", "No discussion of computational overhead or memory requirements for maintaining and merging multiple backbones", "Missing details on how 'class-wise semantic information' is obtained and utilized for classifier weight estimation", "The novelty over existing model merging techniques is not clearly articulated in the abstract", "No mention of limitations or failure cases of the proposed approach", "Unclear how the method scales with the number of domains - does it require storing all historical backbones?"], "suggestions": ["Provide more technical details about the backbone merging mechanism in the abstract", "Include quantitative metrics on computational and memory costs", "Clarify the source and usage of semantic information for classifier consolidation", "Add ablation study results to demonstrate the contribution of each component", "Discuss the scalability of the approach as the number of domains increases", "Compare with recent model merging and continual learning baselines more explicitly", "Include discussion of when and why the method might fail"], "summary": "This paper proposes Duct, a dual consolidation approach for domain-incremental learning that addresses catastrophic forgetting by consolidating knowledge at both representation and classifier levels. While the approach shows strong empirical results and tackles an important problem, the abstract lacks sufficient technical detail about the merging mechanisms and scalability considerations. The work would benefit from clearer exposition of the technical contributions and more thorough analysis of computational costs and limitations."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important problem of catastrophic forgetting in domain-incremental learning with pre-trained models\",\n\"Proposes a dual consolidation approach that operates at both representation and classifier levels, which is a comprehensive solution\",\n\"Demonstrates state-of-the-art performance across four benchmark datasets\",\n\"The method is well-motivated with clear identification of the representation-classifier mismatch problem\",\n\"Includes both backbone merging and classifier consolidation using semantic information\"\n],\n\"weaknesses\": [\n\"The abstract lacks clarity on the technical details of how backbone merging is performed and what specific mechanism is used\",\n\"No discussion of computational overhead or memory requirements for maintaining and merging multiple backbones\",\n\"Missing details on how 'class-wise semantic information' is obtained and utilized for classifier weight estimation\",\n\"The novelty over existing model merging techniques is not clearly articulated in the abstract\",\n\"No mention of limitations or failure cases of the proposed approach\",\n\"Unclear how the method scales with the number of domains - does it require storing all historical backbones?\"\n],\n\"suggestions\": [\n\"Provide more technical details about the backbone merging mechanism in the abstract\",\n\"Include quantitative metrics on computational and memory costs\",\n\"Clarify the source and usage of semantic information for classifier consolidation\",\n\"Add ablation study results to demonstrate the contribution of each component\",\n\"Discuss the scalability of the approach as the number of domains increases\",\n\"Compare with recent model merging and continual learning baselines more explicitly\",\n\"Include discussion of when and why the method might fail\"\n],\n\"summary\": \"This paper proposes Duct, a dual consolidation approach for domain-incremental learning that addresses catastrophic forgetting by consolidating knowledge at both representation and classifier levels. While the approach shows strong empirical results and tackles an important problem, the abstract lacks sufficient technical detail about the merging mechanisms and scalability considerations. The work would benefit from clearer exposition of the technical contributions and more thorough analysis of computational costs and limitations.\"\n}\n```", "usage": {"prompt_tokens": 359, "completion_tokens": 471, "total_tokens": 830}, "timestamp": "2025-12-29T06:02:21.790313"}
{"paper_id": "55oi1LCdDL", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["The paper addresses a well-defined and"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"The paper addresses a well-defined and\"\n  ]\n}\n```", "usage": {"prompt_tokens": 361, "completion_tokens": 1275, "total_tokens": 1636}, "timestamp": "2025-12-29T06:02:35.364432"}
{"paper_id": "WX9cd9iII4", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important and timely problem of fairness in off-policy learning, which has received less attention compared to fairness in supervised learning", "Provides a comprehensive framework that handles multiple fairness notions (demographic parity, equalized odds, etc.) in the off-policy setting", "Includes theoretical contributions in the form of generalization bounds for the proposed framework", "Demonstrates empirical effectiveness through experiments on both simulated and real-world datasets", "The problem formulation clearly distinguishes between the behavioral policy (potentially discriminatory) and the target policy (fair)"], "weaknesses": ["The abstract lacks specificity about which fairness notions are considered and how they are formalized in the off-policy setting", "No discussion of computational complexity or scalability of the proposed neural network-based approach", "The relationship between the theoretical guarantees and practical performance is unclear from the abstract", "Missing details about the experimental setup, baselines used for comparison, and magnitude of improvements", "Does not address potential trade-offs between fairness and policy performance/utility", "Unclear how the framework handles distribution shift between observational data and deployment settings"], "suggestions": ["Explicitly state which fairness notions are considered (e.g., demographic parity, equalized odds, equal opportunity) in the abstract", "Provide at least one concrete example or application domain to ground the abstract", "Include quantitative results from experiments in the abstract to demonstrate effectiveness", "Discuss the fairness-utility trade-off and how the framework balances these competing objectives", "Clarify the assumptions made about the observational data and behavioral policy", "Add discussion of computational requirements and scalability to large-scale problems", "Explain how the generalization bounds relate to practical guarantees on fairness and performance"], "summary": "This paper proposes a framework for learning fair decision policies from observational data collected under potentially discriminatory behavioral policies. While the problem is important and the approach appears comprehensive with both theoretical and empirical contributions, the abstract lacks sufficient detail about the specific methods, fairness notions, experimental results, and practical considerations. The work would benefit from more concrete details and discussion of trade-offs between fairness and utility."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important and timely problem of fairness in off-policy learning, which has received less attention compared to fairness in supervised learning\",\n\"Provides a comprehensive framework that handles multiple fairness notions (demographic parity, equalized odds, etc.) in the off-policy setting\",\n\"Includes theoretical contributions in the form of generalization bounds for the proposed framework\",\n\"Demonstrates empirical effectiveness through experiments on both simulated and real-world datasets\",\n\"The problem formulation clearly distinguishes between the behavioral policy (potentially discriminatory) and the target policy (fair)\"\n],\n\"weaknesses\": [\n\"The abstract lacks specificity about which fairness notions are considered and how they are formalized in the off-policy setting\",\n\"No discussion of computational complexity or scalability of the proposed neural network-based approach\",\n\"The relationship between the theoretical guarantees and practical performance is unclear from the abstract\",\n\"Missing details about the experimental setup, baselines used for comparison, and magnitude of improvements\",\n\"Does not address potential trade-offs between fairness and policy performance/utility\",\n\"Unclear how the framework handles distribution shift between observational data and deployment settings\"\n],\n\"suggestions\": [\n\"Explicitly state which fairness notions are considered (e.g., demographic parity, equalized odds, equal opportunity) in the abstract\",\n\"Provide at least one concrete example or application domain to ground the abstract\",\n\"Include quantitative results from experiments in the abstract to demonstrate effectiveness\",\n\"Discuss the fairness-utility trade-off and how the framework balances these competing objectives\",\n\"Clarify the assumptions made about the observational data and behavioral policy\",\n\"Add discussion of computational requirements and scalability to large-scale problems\",\n\"Explain how the generalization bounds relate to practical guarantees on fairness and performance\"\n],\n\"summary\": \"This paper proposes a framework for learning fair decision policies from observational data collected under potentially discriminatory behavioral policies. While the problem is important and the approach appears comprehensive with both theoretical and empirical contributions, the abstract lacks sufficient detail about the specific methods, fairness notions, experimental results, and practical considerations. The work would benefit from more concrete details and discussion of trade-offs between fairness and utility.\"\n}\n```", "usage": {"prompt_tokens": 301, "completion_tokens": 515, "total_tokens": 816}, "timestamp": "2025-12-29T06:03:31.398398"}
{"paper_id": "WX9cd9iII4", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["Addresses the important and timely problem of learning fair decision"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"Addresses the important and timely problem of learning fair decision\"\n  ]\n}\n```", "usage": {"completion_tokens": 1055, "total_tokens": 1369, "prompt_tokens": 314}, "timestamp": "2025-12-29T06:03:44.353435"}
{"paper_id": "iOy2pITOoH", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 7, "strengths": ["Novel approach combining sparsity in both FFN and attention mechanisms within a unified framework, with predictors trained jointly rather than post-hoc", "Significant computational efficiency gains: 3.1\u00d7 FLOPs reduction, 1.70\u00d7 prefill speedup, and 1.79\u00d7 decoding speedup on CPU", "Maintains parameter count while achieving sparsity, avoiding the overhead of additional parameters common in other sparse methods", "Statistical top-k algorithm is hardware-friendly and minimizes training slowdown compared to exact top-k operations", "Comprehensive evaluation on standard benchmarks (MMLU, HellaSwag, etc.) demonstrating competitive performance with dense baselines", "Clear presentation of the methodology with detailed architectural descriptions and ablation studies"], "weaknesses": ["Limited evaluation on GPU accelerators - only CPU speedups are reported, which may not reflect performance on more common training/inference hardware", "Lack of comparison with other recent sparse transformer methods (e.g., Mixture of Experts, learned sparsity patterns) beyond basic baselines", "The 8% FFN activation sparsity seems relatively modest compared to some MoE approaches that achieve higher sparsity levels", "Missing analysis of memory consumption and bandwidth requirements, which are critical for practical deployment", "Limited discussion of how the method scales to different model sizes beyond the tested configurations", "Ablation studies could be more comprehensive - e.g., varying sparsity levels, different predictor architectures, impact of joint vs. separate training"], "suggestions": ["Include GPU benchmarks (A100, H100) to demonstrate practical speedups on common training hardware", "Add detailed comparison with MoE variants and other sparse attention methods (e.g., Reformer, Longformer) on the same benchmarks", "Provide memory footprint analysis and peak memory consumption during training and inference", "Conduct scaling experiments across different model sizes (1B, 7B, 13B parameters) to validate the approach generalizes", "Extend ablation studies to explore: (1) different sparsity ratios, (2) predictor capacity trade-offs, (3) impact of statistical vs. exact top-k", "Analyze the learned sparsity patterns - which tokens/features are consistently activated? Are there interpretable patterns?", "Include training cost comparison (total FLOPs, wall-clock time) versus dense baseline to quantify end-to-end efficiency gains"], "summary": "This paper presents Spark Transformer, a sparse transformer architecture that achieves 3.1\u00d7 FLOPs reduction through joint training of sparsity predictors for both FFN and attention mechanisms. The approach is technically sound and demonstrates competitive performance with significant efficiency gains on CPU. However, the evaluation would benefit from GPU benchmarks, more comprehensive comparisons with recent sparse methods, and deeper analysis of scaling properties and learned patterns. The work makes a solid contribution to efficient transformer architectures but needs stronger empirical validation across diverse hardware and model scales."}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"Novel approach combining sparsity in both FFN and attention mechanisms within a unified framework, with predictors trained jointly rather than post-hoc\",\n    \"Significant computational efficiency gains: 3.1\u00d7 FLOPs reduction, 1.70\u00d7 prefill speedup, and 1.79\u00d7 decoding speedup on CPU\",\n    \"Maintains parameter count while achieving sparsity, avoiding the overhead of additional parameters common in other sparse methods\",\n    \"Statistical top-k algorithm is hardware-friendly and minimizes training slowdown compared to exact top-k operations\",\n    \"Comprehensive evaluation on standard benchmarks (MMLU, HellaSwag, etc.) demonstrating competitive performance with dense baselines\",\n    \"Clear presentation of the methodology with detailed architectural descriptions and ablation studies\"\n  ],\n  \"weaknesses\": [\n    \"Limited evaluation on GPU accelerators - only CPU speedups are reported, which may not reflect performance on more common training/inference hardware\",\n    \"Lack of comparison with other recent sparse transformer methods (e.g., Mixture of Experts, learned sparsity patterns) beyond basic baselines\",\n    \"The 8% FFN activation sparsity seems relatively modest compared to some MoE approaches that achieve higher sparsity levels\",\n    \"Missing analysis of memory consumption and bandwidth requirements, which are critical for practical deployment\",\n    \"Limited discussion of how the method scales to different model sizes beyond the tested configurations\",\n    \"Ablation studies could be more comprehensive - e.g., varying sparsity levels, different predictor architectures, impact of joint vs. separate training\"\n  ],\n  \"suggestions\": [\n    \"Include GPU benchmarks (A100, H100) to demonstrate practical speedups on common training hardware\",\n    \"Add detailed comparison with MoE variants and other sparse attention methods (e.g., Reformer, Longformer) on the same benchmarks\",\n    \"Provide memory footprint analysis and peak memory consumption during training and inference\",\n    \"Conduct scaling experiments across different model sizes (1B, 7B, 13B parameters) to validate the approach generalizes\",\n    \"Extend ablation studies to explore: (1) different sparsity ratios, (2) predictor capacity trade-offs, (3) impact of statistical vs. exact top-k\",\n    \"Analyze the learned sparsity patterns - which tokens/features are consistently activated? Are there interpretable patterns?\",\n    \"Include training cost comparison (total FLOPs, wall-clock time) versus dense baseline to quantify end-to-end efficiency gains\"\n  ],\n  \"summary\": \"This paper presents Spark Transformer, a sparse transformer architecture that achieves 3.1\u00d7 FLOPs reduction through joint training of sparsity predictors for both FFN and attention mechanisms. The approach is technically sound and demonstrates competitive performance with significant efficiency gains on CPU. However, the evaluation would benefit from GPU benchmarks, more comprehensive comparisons with recent sparse methods, and deeper analysis of scaling properties and learned patterns. The work makes a solid contribution to efficient transformer architectures but needs stronger empirical validation across diverse hardware and model scales.\"\n}\n```", "usage": {"prompt_tokens": 429, "completion_tokens": 725, "total_tokens": 1154}, "timestamp": "2025-12-29T06:04:15.325731"}
{"paper_id": "iOy2pITOoH", "model": "google/gemini-2.5-pro", "response": {"score": 7, "strengths": ["The proposed method, Spark Transformer, addresses the"]}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"The proposed method, Spark Transformer, addresses the\"\n  ]\n}\n```", "usage": {"completion_tokens": 1100, "total_tokens": 1512, "prompt_tokens": 412}, "timestamp": "2025-12-29T06:04:27.867214"}
{"paper_id": "EXnDAXyVxw", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel perspective connecting quantization-aware training to domain generalization through the lens of flat minima, which is an interesting and underexplored direction", "Comprehensive experimental evaluation across multiple DG benchmarks (PACS, VLCS, OfficeHome, TerraIncognita, DomainNet) with various architectures", "Practical approach that leverages model compression benefits while improving generalization, enabling efficient ensemble methods", "Clear presentation of the main idea with both analytical motivation and empirical validation", "Demonstrates compatibility with existing DG methods, showing the approach can be combined with other techniques"], "weaknesses": ["The theoretical analysis connecting quantization to flat minima is somewhat superficial - the claim relies primarily on the noise injection perspective which is well-known, and the connection to SAM-like objectives needs more rigorous treatment", "Limited novelty in the core technique - quantization-aware training is standard, and the main contribution is applying it to DG rather than developing new quantization methods", "The paper lacks comparison with other regularization techniques that also promote flat minima (e.g., SAM, weight noise injection) to isolate the specific benefits of quantization", "Insufficient analysis of why quantization-induced noise is particularly suited for DG compared to other forms of regularization", "The ensemble results, while strong, make it difficult to assess the contribution of individual quantized models versus the ensemble effect", "Missing ablation studies on key hyperparameters like quantization bit-width selection and its impact on the flatness-generalization trade-off", "No discussion of potential failure cases or limitations of the approach"], "suggestions": ["Provide more rigorous theoretical analysis, potentially using PAC-Bayes bounds or other frameworks to formalize the connection between quantization, flatness, and generalization", "Include direct comparisons with SAM and explicit weight noise injection to better understand what makes quantization special for DG", "Add ablation studies varying bit-widths and analyzing the trade-off between quantization level, flatness metrics, and DG performance", "Separate the analysis of single quantized model performance from ensemble effects to clarify individual contributions", "Discuss computational costs during training (not just inference) and compare training time with baseline methods", "Analyze failure modes: are there domains or architectures where QT-DoG underperforms, and why?", "Provide visualization of loss landscapes comparing quantized vs. non-quantized models to empirically validate flatness claims", "Include experiments on larger-scale models and datasets to assess scalability"], "summary": "This paper proposes using quantization-aware training as a regularization technique for domain generalization, arguing that quantization leads to flatter minima and better cross-domain generalization. While the core idea is interesting and experimental results are comprehensive, the theoretical justification is somewhat shallow and the novelty is limited since it primarily applies existing quantization techniques to a new problem. The paper would benefit from deeper theoretical analysis, more thorough comparisons with alternative regularization methods, and clearer separation of quantization effects from ensemble effects. The work makes a reasonable contribution but needs strengthening in several areas to be truly compelling."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel perspective connecting quantization-aware training to domain generalization through the lens of flat minima, which is an interesting and underexplored direction\",\n\"Comprehensive experimental evaluation across multiple DG benchmarks (PACS, VLCS, OfficeHome, TerraIncognita, DomainNet) with various architectures\",\n\"Practical approach that leverages model compression benefits while improving generalization, enabling efficient ensemble methods\",\n\"Clear presentation of the main idea with both analytical motivation and empirical validation\",\n\"Demonstrates compatibility with existing DG methods, showing the approach can be combined with other techniques\"\n],\n\"weaknesses\": [\n\"The theoretical analysis connecting quantization to flat minima is somewhat superficial - the claim relies primarily on the noise injection perspective which is well-known, and the connection to SAM-like objectives needs more rigorous treatment\",\n\"Limited novelty in the core technique - quantization-aware training is standard, and the main contribution is applying it to DG rather than developing new quantization methods\",\n\"The paper lacks comparison with other regularization techniques that also promote flat minima (e.g., SAM, weight noise injection) to isolate the specific benefits of quantization\",\n\"Insufficient analysis of why quantization-induced noise is particularly suited for DG compared to other forms of regularization\",\n\"The ensemble results, while strong, make it difficult to assess the contribution of individual quantized models versus the ensemble effect\",\n\"Missing ablation studies on key hyperparameters like quantization bit-width selection and its impact on the flatness-generalization trade-off\",\n\"No discussion of potential failure cases or limitations of the approach\"\n],\n\"suggestions\": [\n\"Provide more rigorous theoretical analysis, potentially using PAC-Bayes bounds or other frameworks to formalize the connection between quantization, flatness, and generalization\",\n\"Include direct comparisons with SAM and explicit weight noise injection to better understand what makes quantization special for DG\",\n\"Add ablation studies varying bit-widths and analyzing the trade-off between quantization level, flatness metrics, and DG performance\",\n\"Separate the analysis of single quantized model performance from ensemble effects to clarify individual contributions\",\n\"Discuss computational costs during training (not just inference) and compare training time with baseline methods\",\n\"Analyze failure modes: are there domains or architectures where QT-DoG underperforms, and why?\",\n\"Provide visualization of loss landscapes comparing quantized vs. non-quantized models to empirically validate flatness claims\",\n\"Include experiments on larger-scale models and datasets to assess scalability\"\n],\n\"summary\": \"This paper proposes using quantization-aware training as a regularization technique for domain generalization, arguing that quantization leads to flatter minima and better cross-domain generalization. While the core idea is interesting and experimental results are comprehensive, the theoretical justification is somewhat shallow and the novelty is limited since it primarily applies existing quantization techniques to a new problem. The paper would benefit from deeper theoretical analysis, more thorough comparisons with alternative regularization methods, and clearer separation of quantization effects from ensemble effects. The work makes a reasonable contribution but needs strengthening in several areas to be truly compelling.\"\n}\n```", "usage": {"prompt_tokens": 382, "completion_tokens": 734, "total_tokens": 1116}, "timestamp": "2025-12-29T06:05:05.334466"}
{"paper_id": "EXnDAXyVxw", "model": "google/gemini-2.5-pro", "response": {"score": 7, "strengths": ["The core idea of repurposing"]}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"The core idea of repurposing\"\n  ]\n}\n```", "usage": {"completion_tokens": 1036, "total_tokens": 1394, "prompt_tokens": 358}, "timestamp": "2025-12-29T06:05:18.802129"}
{"paper_id": "aYx7JR20sI", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel theoretical framework connecting tropical geometry to neural network expressivity, providing a fresh mathematical perspective on understanding network capacity", "Concrete algorithmic contribution with an open-source OSCAR library implementation for computing exact number of linear regions", "Comprehensive experimental validation across multiple network architectures demonstrating practical applicability of the theoretical framework", "Well-motivated problem addressing the important question of neural network expressivity through the lens of linear regions", "Interdisciplinary approach bridging algebraic geometry and deep learning with potential for future research directions"], "weaknesses": ["Limited scalability analysis - unclear how the proposed methods scale to modern deep networks with millions of parameters", "Experimental validation primarily on small networks and toy problems; lacks demonstration on practical deep learning benchmarks", "The connection between tropical geometry insights and practical network design/training is not clearly established", "Missing comparison with existing methods for counting linear regions or analyzing network expressivity", "Computational complexity of the proposed algorithms is not thoroughly analyzed or compared to alternatives", "The paper assumes familiarity with tropical geometry, which may limit accessibility to the broader ML community", "Unclear practical implications - how do the theoretical insights translate to improved network architectures or training procedures?"], "suggestions": ["Include scalability experiments showing how the methods perform on larger, more realistic networks (e.g., ResNets, VGG)", "Provide runtime comparisons with existing methods for computing linear regions or analyzing expressivity", "Add a section discussing practical implications and how insights from tropical geometry can guide network design", "Include more accessible introduction to tropical geometry concepts for readers unfamiliar with algebraic geometry", "Demonstrate the framework on at least one real-world application or standard benchmark dataset", "Discuss computational complexity bounds and provide empirical runtime analysis", "Add ablation studies showing which aspects of the tropical geometric framework contribute most to the insights gained", "Consider adding a case study showing how the framework led to a concrete improvement in network design or understanding"], "summary": "This paper presents an interesting theoretical contribution connecting tropical geometry to neural network expressivity analysis, with a focus on counting linear regions. While the mathematical framework is novel and the open-source implementation is valuable, the work suffers from limited demonstration of practical applicability and scalability. The experiments are primarily on small networks, and the connection between theoretical insights and practical network design remains unclear. The paper would benefit from stronger empirical validation on realistic problems and clearer discussion of how the tropical geometric perspective can inform practical deep learning."}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"Novel theoretical framework connecting tropical geometry to neural network expressivity, providing a fresh mathematical perspective on understanding network capacity\",\n    \"Concrete algorithmic contribution with an open-source OSCAR library implementation for computing exact number of linear regions\",\n    \"Comprehensive experimental validation across multiple network architectures demonstrating practical applicability of the theoretical framework\",\n    \"Well-motivated problem addressing the important question of neural network expressivity through the lens of linear regions\",\n    \"Interdisciplinary approach bridging algebraic geometry and deep learning with potential for future research directions\"\n  ],\n  \"weaknesses\": [\n    \"Limited scalability analysis - unclear how the proposed methods scale to modern deep networks with millions of parameters\",\n    \"Experimental validation primarily on small networks and toy problems; lacks demonstration on practical deep learning benchmarks\",\n    \"The connection between tropical geometry insights and practical network design/training is not clearly established\",\n    \"Missing comparison with existing methods for counting linear regions or analyzing network expressivity\",\n    \"Computational complexity of the proposed algorithms is not thoroughly analyzed or compared to alternatives\",\n    \"The paper assumes familiarity with tropical geometry, which may limit accessibility to the broader ML community\",\n    \"Unclear practical implications - how do the theoretical insights translate to improved network architectures or training procedures?\"\n  ],\n  \"suggestions\": [\n    \"Include scalability experiments showing how the methods perform on larger, more realistic networks (e.g., ResNets, VGG)\",\n    \"Provide runtime comparisons with existing methods for computing linear regions or analyzing expressivity\",\n    \"Add a section discussing practical implications and how insights from tropical geometry can guide network design\",\n    \"Include more accessible introduction to tropical geometry concepts for readers unfamiliar with algebraic geometry\",\n    \"Demonstrate the framework on at least one real-world application or standard benchmark dataset\",\n    \"Discuss computational complexity bounds and provide empirical runtime analysis\",\n    \"Add ablation studies showing which aspects of the tropical geometric framework contribute most to the insights gained\",\n    \"Consider adding a case study showing how the framework led to a concrete improvement in network design or understanding\"\n  ],\n  \"summary\": \"This paper presents an interesting theoretical contribution connecting tropical geometry to neural network expressivity analysis, with a focus on counting linear regions. While the mathematical framework is novel and the open-source implementation is valuable, the work suffers from limited demonstration of practical applicability and scalability. The experiments are primarily on small networks, and the connection between theoretical insights and practical network design remains unclear. The paper would benefit from stronger empirical validation on realistic problems and clearer discussion of how the tropical geometric perspective can inform practical deep learning.\"\n}\n```", "usage": {"prompt_tokens": 358, "completion_tokens": 597, "total_tokens": 955}, "timestamp": "2025-12-29T06:05:53.302319"}
{"paper_id": "aYx7JR20sI", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Provides a novel and theoretically deep framework for analyzing neural"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Provides a novel and theoretically deep framework for analyzing neural\"\n  ]\n}\n```", "usage": {"completion_tokens": 953, "total_tokens": 1324, "prompt_tokens": 371}, "timestamp": "2025-12-29T06:06:04.528162"}
{"paper_id": "xoBPfUyLWj", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 5, "strengths": ["Novel application of diffusion models to time-series anomaly detection, which is a relatively underexplored area", "Incorporation of heterogeneous side information to enhance detection performance is an interesting contribution", "Evaluation on three diverse datasets (IoT, DDoS, network transactions) demonstrates broad applicability", "Unsupervised approach eliminates the need for labeled anomaly data, which is practical for real-world scenarios"], "weaknesses": ["Abstract lacks technical depth - does not specify what type of diffusion model is used or how side information is incorporated", "No quantitative results or performance metrics provided in the abstract to support claims of 'state-of-the-art performance'", "The connection between 'time series imputation techniques' and OoD detection is unclear and needs better motivation", "Missing comparison with existing baseline methods - unclear what makes this approach superior to existing anomaly detection methods", "The term 'heterogeneous side information' is vague - needs concrete examples of what this entails", "No discussion of computational complexity or scalability of the diffusion-based approach", "Unclear how the method handles different types of anomalies (point, contextual, collective)"], "suggestions": ["Provide specific details about the diffusion model architecture and training procedure", "Include quantitative performance metrics (e.g., F1-score, AUC-ROC) in the abstract or introduction", "Clarify the relationship between imputation and anomaly detection - explain why reconstruction error is a good proxy for anomalies", "Add concrete examples of the 'domain-specific side information' used in each dataset", "Include ablation studies to demonstrate the contribution of each component (diffusion model, side information, imputation)", "Compare with strong baselines including recent deep learning methods for time-series anomaly detection", "Discuss computational requirements and inference time compared to existing methods", "Provide theoretical justification or intuition for why diffusion models are well-suited for this task"], "summary": "The paper proposes using diffusion models with side information for unsupervised time-series anomaly detection. While the application is novel and the multi-domain evaluation is promising, the abstract lacks technical depth and quantitative evidence. Key concerns include unclear methodology details, missing performance comparisons, and insufficient justification for design choices. The paper would benefit from more rigorous technical exposition and comprehensive experimental validation."}, "raw": "```json\n{\n\"score\": 5,\n\"strengths\": [\n\"Novel application of diffusion models to time-series anomaly detection, which is a relatively underexplored area\",\n\"Incorporation of heterogeneous side information to enhance detection performance is an interesting contribution\",\n\"Evaluation on three diverse datasets (IoT, DDoS, network transactions) demonstrates broad applicability\",\n\"Unsupervised approach eliminates the need for labeled anomaly data, which is practical for real-world scenarios\"\n],\n\"weaknesses\": [\n\"Abstract lacks technical depth - does not specify what type of diffusion model is used or how side information is incorporated\",\n\"No quantitative results or performance metrics provided in the abstract to support claims of 'state-of-the-art performance'\",\n\"The connection between 'time series imputation techniques' and OoD detection is unclear and needs better motivation\",\n\"Missing comparison with existing baseline methods - unclear what makes this approach superior to existing anomaly detection methods\",\n\"The term 'heterogeneous side information' is vague - needs concrete examples of what this entails\",\n\"No discussion of computational complexity or scalability of the diffusion-based approach\",\n\"Unclear how the method handles different types of anomalies (point, contextual, collective)\"\n],\n\"suggestions\": [\n\"Provide specific details about the diffusion model architecture and training procedure\",\n\"Include quantitative performance metrics (e.g., F1-score, AUC-ROC) in the abstract or introduction\",\n\"Clarify the relationship between imputation and anomaly detection - explain why reconstruction error is a good proxy for anomalies\",\n\"Add concrete examples of the 'domain-specific side information' used in each dataset\",\n\"Include ablation studies to demonstrate the contribution of each component (diffusion model, side information, imputation)\",\n\"Compare with strong baselines including recent deep learning methods for time-series anomaly detection\",\n\"Discuss computational requirements and inference time compared to existing methods\",\n\"Provide theoretical justification or intuition for why diffusion models are well-suited for this task\"\n],\n\"summary\": \"The paper proposes using diffusion models with side information for unsupervised time-series anomaly detection. While the application is novel and the multi-domain evaluation is promising, the abstract lacks technical depth and quantitative evidence. Key concerns include unclear methodology details, missing performance comparisons, and insufficient justification for design choices. The paper would benefit from more rigorous technical exposition and comprehensive experimental validation.\"\n}\n```", "usage": {"prompt_tokens": 314, "completion_tokens": 565, "total_tokens": 879}, "timestamp": "2025-12-29T06:06:35.869297"}
{"paper_id": "xoBPfUyLWj", "model": "google/gemini-2.5-pro", "response": {"score": 5, "strengths": ["The application of diffusion models to time-series"]}, "raw": "```json\n{\n  \"score\": 5,\n  \"strengths\": [\n    \"The application of diffusion models to time-series\"\n  ]\n}\n```", "usage": {"completion_tokens": 1119, "total_tokens": 1437, "prompt_tokens": 318}, "timestamp": "2025-12-29T06:06:48.938232"}
{"paper_id": "0t1O8ziRZp", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important practical problem in chip design with significant real-world impact", "Novel approach of using retrieval-based similarity to modulate pre-trained agent recommendations via an alpha parameter", "Strong empirical results showing 24.8% QoR improvement and 9x runtime reduction compared to state-of-the-art", "Identifies an important failure mode where pre-trained agents can harm performance on novel designs", "Comprehensive evaluation across different hardware design types"], "weaknesses": ["The abstract lacks clarity on the technical details of how the alpha parameter is computed and applied", "Insufficient description of the baseline methods and experimental setup in the abstract", "The connection between nearest neighbor retrieval and the alpha parameter adjustment mechanism is not clearly explained", "Missing details on the training dataset composition and what constitutes 'similarity' in this context", "No discussion of limitations or failure cases of the proposed approach", "The term 'meticulously tuned' is vague and doesn't convey the actual methodology"], "suggestions": ["Provide a clearer mathematical or algorithmic description of how alpha is computed from similarity scores", "Include more details about the baseline methods (what are the current state-of-the-art techniques?)", "Clarify the retrieval mechanism: what features are used for similarity computation?", "Add information about the dataset size and diversity used for training and evaluation", "Discuss when ABC-RL might fail or perform poorly compared to baselines", "Include statistical significance testing or confidence intervals for the reported improvements", "Explain the relationship between the 24.8% QoR improvement and 9x runtime reduction - are these on the same benchmark set?"], "summary": "This paper presents ABC-RL, a retrieval-guided reinforcement learning approach for optimizing logic synthesis in chip design. The key innovation is using similarity-based retrieval to compute an alpha parameter that modulates pre-trained agent recommendations, addressing the problem of agents performing poorly on novel designs. While the empirical results are strong and the problem is important, the abstract lacks sufficient technical clarity about the methodology, baseline comparisons, and experimental setup. The core idea is promising but needs better articulation of the technical approach and more rigorous experimental description."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important practical problem in chip design with significant real-world impact\",\n\"Novel approach of using retrieval-based similarity to modulate pre-trained agent recommendations via an alpha parameter\",\n\"Strong empirical results showing 24.8% QoR improvement and 9x runtime reduction compared to state-of-the-art\",\n\"Identifies an important failure mode where pre-trained agents can harm performance on novel designs\",\n\"Comprehensive evaluation across different hardware design types\"\n],\n\"weaknesses\": [\n\"The abstract lacks clarity on the technical details of how the alpha parameter is computed and applied\",\n\"Insufficient description of the baseline methods and experimental setup in the abstract\",\n\"The connection between nearest neighbor retrieval and the alpha parameter adjustment mechanism is not clearly explained\",\n\"Missing details on the training dataset composition and what constitutes 'similarity' in this context\",\n\"No discussion of limitations or failure cases of the proposed approach\",\n\"The term 'meticulously tuned' is vague and doesn't convey the actual methodology\"\n],\n\"suggestions\": [\n\"Provide a clearer mathematical or algorithmic description of how alpha is computed from similarity scores\",\n\"Include more details about the baseline methods (what are the current state-of-the-art techniques?)\",\n\"Clarify the retrieval mechanism: what features are used for similarity computation?\",\n\"Add information about the dataset size and diversity used for training and evaluation\",\n\"Discuss when ABC-RL might fail or perform poorly compared to baselines\",\n\"Include statistical significance testing or confidence intervals for the reported improvements\",\n\"Explain the relationship between the 24.8% QoR improvement and 9x runtime reduction - are these on the same benchmark set?\"\n],\n\"summary\": \"This paper presents ABC-RL, a retrieval-guided reinforcement learning approach for optimizing logic synthesis in chip design. The key innovation is using similarity-based retrieval to compute an alpha parameter that modulates pre-trained agent recommendations, addressing the problem of agents performing poorly on novel designs. While the empirical results are strong and the problem is important, the abstract lacks sufficient technical clarity about the methodology, baseline comparisons, and experimental setup. The core idea is promising but needs better articulation of the technical approach and more rigorous experimental description.\"\n}\n```", "usage": {"prompt_tokens": 421, "completion_tokens": 524, "total_tokens": 945}, "timestamp": "2025-12-29T06:07:20.039925"}
{"paper_id": "0t1O8ziRZp", "model": "google/gemini-2.5-pro", "response": {"parse_error": true, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses a well-motivated and high"}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses a well-motivated and high", "usage": {"completion_tokens": 1592, "total_tokens": 2002, "prompt_tokens": 410}, "timestamp": "2025-12-29T06:07:36.970278"}
{"paper_id": "ukidfml68f", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses a concrete and important problem in high-resolution 3D generation: the uncontrolled gradient flow through frozen VAE encoders in latent diffusion models", "Proposes a simple and computationally efficient solution (Pixel-wise Gradient Clipping) that can be integrated into existing methods", "The motivation is well-articulated - identifying that backpropagation through frozen components creates unregulated gradients that harm texture learning", "Claims extensive experimental validation demonstrating improvements in synthesis quality"], "weaknesses": ["The abstract lacks technical specificity about how PGC actually works - what exactly is being clipped, at what threshold, and why pixel-wise rather than other granularities?", "No quantitative results or comparisons provided in the abstract to support claims of efficacy", "The relationship between gradient magnitude control and 'preserving crucial texture-related gradient directions' is unclear - how are important directions identified and preserved?", "Missing details on what 'minimal extra cost' means quantitatively (computational overhead, memory requirements)", "The scope of 'extensive experiments' is not specified - what datasets, baselines, and metrics were used?", "Does not discuss potential limitations or failure cases of the proposed approach", "The claim that the gradient propagation pathway 'has never been optimized' seems strong and may need more careful justification"], "suggestions": ["Provide a concrete mathematical formulation or algorithmic description of PGC in the abstract", "Include at least one quantitative result comparing PGC-enhanced methods to baselines", "Clarify the mechanism for preserving texture-related gradient directions while clipping magnitudes", "Specify the computational overhead introduced by PGC (e.g., percentage increase in training time/memory)", "Add a brief discussion of when PGC is most beneficial and any identified limitations", "Consider showing a visual comparison or mentioning specific quality metrics improved", "Provide more context on how this relates to or differs from standard gradient clipping techniques used in deep learning"], "summary": "This paper proposes Pixel-wise Gradient Clipping (PGC) to improve high-resolution 3D generation by controlling gradients backpropagated through frozen VAE encoders in latent diffusion models. While the problem identification is clear and the proposed solution appears simple and practical, the abstract lacks sufficient technical detail and quantitative evidence to fully evaluate the contribution. The core idea of controlling gradient flow through frozen components is interesting, but more specificity is needed about the method, its theoretical justification, and empirical performance gains."}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"Addresses a concrete and important problem in high-resolution 3D generation: the uncontrolled gradient flow through frozen VAE encoders in latent diffusion models\",\n    \"Proposes a simple and computationally efficient solution (Pixel-wise Gradient Clipping) that can be integrated into existing methods\",\n    \"The motivation is well-articulated - identifying that backpropagation through frozen components creates unregulated gradients that harm texture learning\",\n    \"Claims extensive experimental validation demonstrating improvements in synthesis quality\"\n  ],\n  \"weaknesses\": [\n    \"The abstract lacks technical specificity about how PGC actually works - what exactly is being clipped, at what threshold, and why pixel-wise rather than other granularities?\",\n    \"No quantitative results or comparisons provided in the abstract to support claims of efficacy\",\n    \"The relationship between gradient magnitude control and 'preserving crucial texture-related gradient directions' is unclear - how are important directions identified and preserved?\",\n    \"Missing details on what 'minimal extra cost' means quantitatively (computational overhead, memory requirements)\",\n    \"The scope of 'extensive experiments' is not specified - what datasets, baselines, and metrics were used?\",\n    \"Does not discuss potential limitations or failure cases of the proposed approach\",\n    \"The claim that the gradient propagation pathway 'has never been optimized' seems strong and may need more careful justification\"\n  ],\n  \"suggestions\": [\n    \"Provide a concrete mathematical formulation or algorithmic description of PGC in the abstract\",\n    \"Include at least one quantitative result comparing PGC-enhanced methods to baselines\",\n    \"Clarify the mechanism for preserving texture-related gradient directions while clipping magnitudes\",\n    \"Specify the computational overhead introduced by PGC (e.g., percentage increase in training time/memory)\",\n    \"Add a brief discussion of when PGC is most beneficial and any identified limitations\",\n    \"Consider showing a visual comparison or mentioning specific quality metrics improved\",\n    \"Provide more context on how this relates to or differs from standard gradient clipping techniques used in deep learning\"\n  ],\n  \"summary\": \"This paper proposes Pixel-wise Gradient Clipping (PGC) to improve high-resolution 3D generation by controlling gradients backpropagated through frozen VAE encoders in latent diffusion models. While the problem identification is clear and the proposed solution appears simple and practical, the abstract lacks sufficient technical detail and quantitative evidence to fully evaluate the contribution. The core idea of controlling gradient flow through frozen components is interesting, but more specificity is needed about the method, its theoretical justification, and empirical performance gains.\"\n}\n```", "usage": {"prompt_tokens": 449, "completion_tokens": 629, "total_tokens": 1078}, "timestamp": "2025-12-29T06:08:12.917532"}
{"paper_id": "ukidfml68f", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["The paper identifies a specific and plausible issue in modern"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"The paper identifies a specific and plausible issue in modern\"\n  ]\n}\n```", "usage": {"completion_tokens": 1406, "total_tokens": 1827, "prompt_tokens": 421}, "timestamp": "2025-12-29T06:08:27.317687"}
{"paper_id": "0uFTqvQhML", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel pipeline that inverts the traditional reconstruction-then-generation paradigm by first generating videos and then reconstructing 3D scenes, which is an interesting and potentially impactful contribution", "Comprehensive multi-condition control supporting BEV maps, 3D objects, and text descriptions, providing flexible scene generation capabilities", "Addresses practical challenges in generated content through deformable Gaussian splatting with monocular depth initialization and appearance modeling for exposure discrepancies", "Demonstrates practical utility by showing improvements in downstream tasks like BEV segmentation", "Tackles an important problem in autonomous driving simulation where high-quality 3D scene generation is needed but data acquisition is expensive"], "weaknesses": ["The abstract lacks quantitative results and specific metrics to support claims of 'high-quality' and 'superior performance'", "Limited technical details in the abstract about the video generation model architecture and training procedure", "Unclear how the method handles dynamic objects and temporal consistency in the generated 3D scenes", "The approach of generating videos first then reconstructing may accumulate errors from both stages, but error propagation analysis is not discussed", "Validation only on nuScenes dataset raises questions about generalization to other autonomous driving datasets or street scene scenarios", "The relationship and comparison with existing 3D generation methods (e.g., NeRF-based, diffusion-based 3D generation) is not clearly articulated", "Computational costs and scalability of the two-stage pipeline (generation + reconstruction) are not addressed"], "suggestions": ["Include specific quantitative metrics (e.g., FID, LPIPS, depth accuracy) in the abstract to substantiate quality claims", "Provide more details on the video generation model architecture and how it ensures multi-view consistency", "Discuss the trade-offs between the proposed generate-then-reconstruct approach versus traditional reconstruct-then-generate methods with concrete examples", "Add ablation studies showing the contribution of each component (deformable Gaussian splatting, monocular depth initialization, appearance modeling)", "Evaluate on additional datasets beyond nuScenes to demonstrate generalization capabilities", "Include computational cost analysis and runtime comparisons with baseline methods", "Provide more details on how dynamic objects are handled and whether the method can generate temporally consistent scenes", "Clarify the limitations of the approach, particularly regarding scene complexity and failure cases"], "summary": "MagicDrive3D presents an interesting inversion of the typical 3D generation pipeline by generating videos first and then reconstructing 3D scenes, with multi-condition control for autonomous driving scenarios. While the approach is novel and addresses a relevant problem, the paper would benefit from more quantitative validation, clearer technical exposition, broader experimental evaluation, and discussion of computational costs and limitations. The core idea shows promise but needs stronger empirical support and more thorough comparison with existing methods."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel pipeline that inverts the traditional reconstruction-then-generation paradigm by first generating videos and then reconstructing 3D scenes, which is an interesting and potentially impactful contribution\",\n\"Comprehensive multi-condition control supporting BEV maps, 3D objects, and text descriptions, providing flexible scene generation capabilities\",\n\"Addresses practical challenges in generated content through deformable Gaussian splatting with monocular depth initialization and appearance modeling for exposure discrepancies\",\n\"Demonstrates practical utility by showing improvements in downstream tasks like BEV segmentation\",\n\"Tackles an important problem in autonomous driving simulation where high-quality 3D scene generation is needed but data acquisition is expensive\"\n],\n\"weaknesses\": [\n\"The abstract lacks quantitative results and specific metrics to support claims of 'high-quality' and 'superior performance'\",\n\"Limited technical details in the abstract about the video generation model architecture and training procedure\",\n\"Unclear how the method handles dynamic objects and temporal consistency in the generated 3D scenes\",\n\"The approach of generating videos first then reconstructing may accumulate errors from both stages, but error propagation analysis is not discussed\",\n\"Validation only on nuScenes dataset raises questions about generalization to other autonomous driving datasets or street scene scenarios\",\n\"The relationship and comparison with existing 3D generation methods (e.g., NeRF-based, diffusion-based 3D generation) is not clearly articulated\",\n\"Computational costs and scalability of the two-stage pipeline (generation + reconstruction) are not addressed\"\n],\n\"suggestions\": [\n\"Include specific quantitative metrics (e.g., FID, LPIPS, depth accuracy) in the abstract to substantiate quality claims\",\n\"Provide more details on the video generation model architecture and how it ensures multi-view consistency\",\n\"Discuss the trade-offs between the proposed generate-then-reconstruct approach versus traditional reconstruct-then-generate methods with concrete examples\",\n\"Add ablation studies showing the contribution of each component (deformable Gaussian splatting, monocular depth initialization, appearance modeling)\",\n\"Evaluate on additional datasets beyond nuScenes to demonstrate generalization capabilities\",\n\"Include computational cost analysis and runtime comparisons with baseline methods\",\n\"Provide more details on how dynamic objects are handled and whether the method can generate temporally consistent scenes\",\n\"Clarify the limitations of the approach, particularly regarding scene complexity and failure cases\"\n],\n\"summary\": \"MagicDrive3D presents an interesting inversion of the typical 3D generation pipeline by generating videos first and then reconstructing 3D scenes, with multi-condition control for autonomous driving scenarios. While the approach is novel and addresses a relevant problem, the paper would benefit from more quantitative validation, clearer technical exposition, broader experimental evaluation, and discussion of computational costs and limitations. The core idea shows promise but needs stronger empirical support and more thorough comparison with existing methods.\"\n}\n```", "usage": {"prompt_tokens": 348, "completion_tokens": 666, "total_tokens": 1014}, "timestamp": "2025-12-29T06:09:09.211570"}
{"paper_id": "0uFTqvQhML", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["The proposed 'generate-then-reconstruct'"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"The proposed 'generate-then-reconstruct'\"\n  ]\n}\n```", "usage": {"completion_tokens": 1074, "total_tokens": 1420, "prompt_tokens": 346}, "timestamp": "2025-12-29T06:09:21.307126"}
{"paper_id": "GkJCgUmIqA", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 7, "strengths": ["Novel approach combining trust-region SQP with PINNs to address hard constraints, which is theoretically well-motivated", "Comprehensive experimental validation showing 1-3 orders of magnitude improvement over existing hard-constrained methods", "Addresses computational bottleneck through quasi-Newton updates instead of full Hessian computation", "Introduction of a pretraining step that generalizes to other hard-constrained methods", "Demonstrates robustness to hyperparameters and problem-specific parameters", "Clear presentation of the methodology with good mathematical formulation"], "weaknesses": ["Limited theoretical analysis of convergence properties and conditions under which the method is guaranteed to work", "Computational cost comparison with baseline methods is not thoroughly discussed - second-order methods typically require more computation per iteration", "The pretraining step, while effective, appears somewhat ad-hoc and lacks theoretical justification", "Scalability to very high-dimensional PDEs or large-scale problems is not demonstrated", "Limited discussion of failure cases or problem classes where the method might struggle", "The trust-region radius adaptation strategy could benefit from more rigorous analysis"], "suggestions": ["Provide theoretical convergence guarantees or at least discuss conditions under which convergence is expected", "Include wall-clock time comparisons and computational cost analysis, not just accuracy metrics", "Provide more ablation studies on the pretraining step - how sensitive is performance to pretraining duration and strategy?", "Test on higher-dimensional PDEs (4D, 5D+) to demonstrate scalability", "Compare with more recent PINN variants and hard-constrained methods from 2023-2024", "Provide guidelines or heuristics for setting the trust-region parameters for new problems", "Include failure case analysis to better understand method limitations", "Discuss the relationship between the soft-constrained loss used for trust-region adaptation and the hard-constrained objective"], "summary": "This paper presents trSQP-PINN, a trust-region sequential quadratic programming approach for physics-informed neural networks that treats PDE constraints as hard constraints rather than soft penalties. The method shows significant empirical improvements (1-3 orders of magnitude) over existing hard-constrained approaches. While the experimental results are strong and the approach is novel, the paper would benefit from stronger theoretical analysis, more thorough computational cost comparisons, and demonstration of scalability to higher-dimensional problems. The pretraining step is a useful practical contribution but needs better justification. Overall, this is solid work that makes meaningful contributions to addressing PINN failure modes, though some aspects require further development."}, "raw": "```json\n{\n\"score\": 7,\n\"strengths\": [\n\"Novel approach combining trust-region SQP with PINNs to address hard constraints, which is theoretically well-motivated\",\n\"Comprehensive experimental validation showing 1-3 orders of magnitude improvement over existing hard-constrained methods\",\n\"Addresses computational bottleneck through quasi-Newton updates instead of full Hessian computation\",\n\"Introduction of a pretraining step that generalizes to other hard-constrained methods\",\n\"Demonstrates robustness to hyperparameters and problem-specific parameters\",\n\"Clear presentation of the methodology with good mathematical formulation\"\n],\n\"weaknesses\": [\n\"Limited theoretical analysis of convergence properties and conditions under which the method is guaranteed to work\",\n\"Computational cost comparison with baseline methods is not thoroughly discussed - second-order methods typically require more computation per iteration\",\n\"The pretraining step, while effective, appears somewhat ad-hoc and lacks theoretical justification\",\n\"Scalability to very high-dimensional PDEs or large-scale problems is not demonstrated\",\n\"Limited discussion of failure cases or problem classes where the method might struggle\",\n\"The trust-region radius adaptation strategy could benefit from more rigorous analysis\"\n],\n\"suggestions\": [\n\"Provide theoretical convergence guarantees or at least discuss conditions under which convergence is expected\",\n\"Include wall-clock time comparisons and computational cost analysis, not just accuracy metrics\",\n\"Provide more ablation studies on the pretraining step - how sensitive is performance to pretraining duration and strategy?\",\n\"Test on higher-dimensional PDEs (4D, 5D+) to demonstrate scalability\",\n\"Compare with more recent PINN variants and hard-constrained methods from 2023-2024\",\n\"Provide guidelines or heuristics for setting the trust-region parameters for new problems\",\n\"Include failure case analysis to better understand method limitations\",\n\"Discuss the relationship between the soft-constrained loss used for trust-region adaptation and the hard-constrained objective\"\n],\n\"summary\": \"This paper presents trSQP-PINN, a trust-region sequential quadratic programming approach for physics-informed neural networks that treats PDE constraints as hard constraints rather than soft penalties. The method shows significant empirical improvements (1-3 orders of magnitude) over existing hard-constrained approaches. While the experimental results are strong and the approach is novel, the paper would benefit from stronger theoretical analysis, more thorough computational cost comparisons, and demonstration of scalability to higher-dimensional problems. The pretraining step is a useful practical contribution but needs better justification. Overall, this is solid work that makes meaningful contributions to addressing PINN failure modes, though some aspects require further development.\"\n}\n```", "usage": {"prompt_tokens": 462, "completion_tokens": 613, "total_tokens": 1075}, "timestamp": "2025-12-29T06:09:53.722518"}
{"paper_id": "GkJCgUmIqA", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["The paper introduces a novel and principled optimization framework"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"The paper introduces a novel and principled optimization framework\"\n  ]\n}\n```", "usage": {"completion_tokens": 1305, "total_tokens": 1745, "prompt_tokens": 440}, "timestamp": "2025-12-29T06:10:07.044695"}
{"paper_id": "4X9RpKH4Ls", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel application domain: Applying transformers to enumerative geometry and \u03c8-class intersection numbers is highly original and addresses a computationally challenging problem with factorial complexity", "Technical innovation: The Dynamic Range Activator (DRA) is a creative solution to handle the extreme range of values (10^-45 to 10^45) and heteroscedasticity inherent in the problem", "Rigorous uncertainty quantification: Using Conformal Prediction with dynamic sliding windows shows careful attention to precision requirements", "Strong interpretability analysis: The discovery that the network implicitly learns Virasoro constraints and large-genus asymptotics through purely data-driven means is compelling and well-investigated through multiple methods (probing, causal inference, abductive testing)"], "weaknesses": ["Insufficient methodological details: The paper lacks crucial implementation details about the DRA activation function, network architecture, training procedures, and dataset construction", "Limited baseline comparisons: No comparison with existing computational methods or simpler neural network architectures to demonstrate the necessity of the proposed approach", "Unclear generalization: It's not clear how well the method generalizes beyond the training distribution, especially for extreme genus values or unseen intersection number configurations", "Validation concerns: The paper doesn't clearly explain how ground truth is obtained for training/validation given that traditional methods are computationally intractable", "Missing ablation studies: No systematic ablation of the DRA component or other architectural choices to validate their individual contributions"], "suggestions": ["Provide complete architectural details including network depth, width, attention mechanisms, and training hyperparameters", "Include ablation studies comparing DRA against standard activations (ReLU, GELU, etc.) and quantifying its contribution to performance", "Add baseline comparisons with traditional computational methods (where tractable) and simpler ML approaches (MLPs, RNNs)", "Clarify the data generation process: how are ground truth intersection numbers computed for training? What is the size and distribution of the dataset?", "Provide more rigorous evaluation of the emergent representations: statistical tests for the claimed asymptotic behavior, quantitative metrics for Virasoro constraint satisfaction", "Discuss computational costs and scalability compared to traditional methods", "Include error analysis showing where the method fails and why", "Add visualizations of the learned representations and attention patterns to support interpretability claims"], "summary": "This paper presents an innovative application of transformers to computational enumerative geometry, specifically computing \u03c8-class intersection numbers. The introduction of the Dynamic Range Activator and the interpretability findings regarding Virasoro constraints are notable contributions. However, the paper suffers from insufficient methodological detail, lack of baselines, and unclear validation procedures. While the core ideas are promising and the problem domain is novel, the execution needs significant strengthening to meet publication standards. The work would benefit from more rigorous experimental validation, clearer exposition of methods, and deeper analysis of when and why the approach works."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel application domain: Applying transformers to enumerative geometry and \u03c8-class intersection numbers is highly original and addresses a computationally challenging problem with factorial complexity\",\n\"Technical innovation: The Dynamic Range Activator (DRA) is a creative solution to handle the extreme range of values (10^-45 to 10^45) and heteroscedasticity inherent in the problem\",\n\"Rigorous uncertainty quantification: Using Conformal Prediction with dynamic sliding windows shows careful attention to precision requirements\",\n\"Strong interpretability analysis: The discovery that the network implicitly learns Virasoro constraints and large-genus asymptotics through purely data-driven means is compelling and well-investigated through multiple methods (probing, causal inference, abductive testing)\"\n],\n\"weaknesses\": [\n\"Insufficient methodological details: The paper lacks crucial implementation details about the DRA activation function, network architecture, training procedures, and dataset construction\",\n\"Limited baseline comparisons: No comparison with existing computational methods or simpler neural network architectures to demonstrate the necessity of the proposed approach\",\n\"Unclear generalization: It's not clear how well the method generalizes beyond the training distribution, especially for extreme genus values or unseen intersection number configurations\",\n\"Validation concerns: The paper doesn't clearly explain how ground truth is obtained for training/validation given that traditional methods are computationally intractable\",\n\"Missing ablation studies: No systematic ablation of the DRA component or other architectural choices to validate their individual contributions\"\n],\n\"suggestions\": [\n\"Provide complete architectural details including network depth, width, attention mechanisms, and training hyperparameters\",\n\"Include ablation studies comparing DRA against standard activations (ReLU, GELU, etc.) and quantifying its contribution to performance\",\n\"Add baseline comparisons with traditional computational methods (where tractable) and simpler ML approaches (MLPs, RNNs)\",\n\"Clarify the data generation process: how are ground truth intersection numbers computed for training? What is the size and distribution of the dataset?\",\n\"Provide more rigorous evaluation of the emergent representations: statistical tests for the claimed asymptotic behavior, quantitative metrics for Virasoro constraint satisfaction\",\n\"Discuss computational costs and scalability compared to traditional methods\",\n\"Include error analysis showing where the method fails and why\",\n\"Add visualizations of the learned representations and attention patterns to support interpretability claims\"\n],\n\"summary\": \"This paper presents an innovative application of transformers to computational enumerative geometry, specifically computing \u03c8-class intersection numbers. The introduction of the Dynamic Range Activator and the interpretability findings regarding Virasoro constraints are notable contributions. However, the paper suffers from insufficient methodological detail, lack of baselines, and unclear validation procedures. While the core ideas are promising and the problem domain is novel, the execution needs significant strengthening to meet publication standards. The work would benefit from more rigorous experimental validation, clearer exposition of methods, and deeper analysis of when and why the approach works.\"\n}\n```", "usage": {"prompt_tokens": 428, "completion_tokens": 687, "total_tokens": 1115}, "timestamp": "2025-12-29T06:10:49.143323"}
{"paper_id": "4X9RpKH4Ls", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Addresses a novel and highly challenging problem in computational"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses a novel and highly challenging problem in computational\"\n  ]\n}\n```", "usage": {"completion_tokens": 1439, "total_tokens": 1856, "prompt_tokens": 417}, "timestamp": "2025-12-29T06:11:04.145880"}
{"paper_id": "RFqeoVfLHa", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses a timely and important problem in LLM development regarding the potential pitfalls of self-improvement methods", "Introduces the novel concept of 'self-improvement reversal' which highlights an underexplored phenomenon in post-training", "Proposes a comprehensive evaluative framework for assessing true progress versus superficial metric improvements", "Tackles a critical gap between aggregate performance metrics and actual capability improvements"], "weaknesses": ["Abstract lacks concrete details about experimental setup, datasets used, and specific tasks evaluated", "No quantitative results or specific examples of self-improvement reversal are provided in the abstract", "The proposed 'comprehensive evaluative framework' is mentioned but not described, making it difficult to assess the contribution", "Unclear what constitutes 'broader, essential capabilities' versus task-specific improvements", "Missing details on which post-training methods were studied and how they differ in their reversal patterns", "The scope of 'diverse problem-solving tasks' is vague - need specificity on domains covered"], "suggestions": ["Include at least one concrete example with numbers showing the reversal phenomenon (e.g., 'X% improvement on task A but Y% decline on capability B')", "Specify the post-training methods examined (e.g., DPO, RLHF, iterative preference learning variants)", "Clarify the evaluation framework components and how it differs from existing evaluation approaches", "Define operational metrics for 'broader, essential capabilities' to make the concept more concrete", "Provide details on model sizes, families, and number of iterations studied", "Include information about whether reversals are consistent across different model architectures or training regimes", "Discuss potential mechanisms or hypotheses for why self-improvement reversal occurs"], "summary": "This paper investigates an important phenomenon where LLMs may show improved task-specific metrics through self-improvement post-training while experiencing declines in broader capabilities. While the core concept of 'self-improvement reversal' is novel and addresses a critical concern in LLM development, the abstract lacks sufficient concrete details about methodology, results, and the proposed framework to fully evaluate the contribution. The work would benefit from more specificity about experimental design, quantitative findings, and clearer definitions of key concepts."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses a timely and important problem in LLM development regarding the potential pitfalls of self-improvement methods\",\n\"Introduces the novel concept of 'self-improvement reversal' which highlights an underexplored phenomenon in post-training\",\n\"Proposes a comprehensive evaluative framework for assessing true progress versus superficial metric improvements\",\n\"Tackles a critical gap between aggregate performance metrics and actual capability improvements\"\n],\n\"weaknesses\": [\n\"Abstract lacks concrete details about experimental setup, datasets used, and specific tasks evaluated\",\n\"No quantitative results or specific examples of self-improvement reversal are provided in the abstract\",\n\"The proposed 'comprehensive evaluative framework' is mentioned but not described, making it difficult to assess the contribution\",\n\"Unclear what constitutes 'broader, essential capabilities' versus task-specific improvements\",\n\"Missing details on which post-training methods were studied and how they differ in their reversal patterns\",\n\"The scope of 'diverse problem-solving tasks' is vague - need specificity on domains covered\"\n],\n\"suggestions\": [\n\"Include at least one concrete example with numbers showing the reversal phenomenon (e.g., 'X% improvement on task A but Y% decline on capability B')\",\n\"Specify the post-training methods examined (e.g., DPO, RLHF, iterative preference learning variants)\",\n\"Clarify the evaluation framework components and how it differs from existing evaluation approaches\",\n\"Define operational metrics for 'broader, essential capabilities' to make the concept more concrete\",\n\"Provide details on model sizes, families, and number of iterations studied\",\n\"Include information about whether reversals are consistent across different model architectures or training regimes\",\n\"Discuss potential mechanisms or hypotheses for why self-improvement reversal occurs\"\n],\n\"summary\": \"This paper investigates an important phenomenon where LLMs may show improved task-specific metrics through self-improvement post-training while experiencing declines in broader capabilities. While the core concept of 'self-improvement reversal' is novel and addresses a critical concern in LLM development, the abstract lacks sufficient concrete details about methodology, results, and the proposed framework to fully evaluate the contribution. The work would benefit from more specificity about experimental design, quantitative findings, and clearer definitions of key concepts.\"\n}\n```", "usage": {"prompt_tokens": 323, "completion_tokens": 534, "total_tokens": 857}, "timestamp": "2025-12-29T06:12:43.370313"}
{"paper_id": "RFqeoVfLHa", "model": "google/gemini-2.5-pro", "response": {"score": 7, "strengths": ["Addresses a timely and critical problem: the potential"]}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses a timely and critical problem: the potential\"\n  ]\n}\n```", "usage": {"completion_tokens": 1282, "prompt_tokens": 319, "total_tokens": 1601}, "timestamp": "2025-12-29T06:12:57.960723"}
{"paper_id": "iriEqxFB4y", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Clear motivation: The paper identifies an important limitation of uncertainty-based outlier sampling - that it can be biased towards certain types of outliers - and proposes diversity as a complementary criterion", "Simple and practical method: DOS is straightforward to implement, combining clustering with uncertainty-based selection within each cluster", "Comprehensive experiments: The paper includes extensive experiments across multiple benchmarks (CIFAR-10, CIFAR-100) with different outlier datasets (TI-300K, TI-80M) and compares against relevant baselines", "Strong empirical results: DOS achieves significant improvements, with up to 25.79% reduction in FPR95 on CIFAR-100 with TI-300K", "Good ablation studies: The paper provides ablations on key components like the number of clusters and the impact of diversity"], "weaknesses": ["Limited novelty: The core idea of combining clustering with uncertainty-based selection is relatively straightforward and incremental. The technical contribution is limited", "Lack of theoretical justification: The paper does not provide theoretical analysis of why diversity helps or how the proposed method guarantees diverse sampling", "Clustering method not well justified: The choice of k-means clustering on normalized features is not well motivated. Why is this the right way to measure diversity? Other diversity metrics are not explored", "Computational cost not discussed: The paper does not analyze the computational overhead of performing clustering at each iteration, which could be significant for large outlier datasets", "Limited analysis of what diversity means: The paper does not provide clear analysis or visualization of what types of diversity are captured by the clustering approach", "Missing comparisons: Some recent OOD detection methods that also consider diversity or coverage are not compared", "Hyperparameter sensitivity: The method introduces additional hyperparameters (number of clusters K) but sensitivity analysis is limited"], "suggestions": ["Provide theoretical analysis or intuition about why diversity in outlier sampling improves OOD detection, beyond empirical observations", "Compare different diversity measures (e.g., determinantal point processes, core-set selection) and justify why clustering is the best choice", "Include computational cost analysis and wall-clock time comparisons with baselines", "Provide visualizations (e.g., t-SNE) showing how DOS samples differ from uncertainty-only sampling in terms of coverage of the outlier space", "Analyze what types of outliers are selected by DOS vs baselines - are they semantically diverse?", "Include more recent OOD detection baselines, especially those considering diversity or coverage", "Provide more extensive hyperparameter sensitivity analysis, especially for the number of clusters K across different settings", "Discuss failure cases or limitations of the diversity-based approach"], "summary": "This paper proposes DOS, a sampling strategy for selecting diverse outliers to improve OOD detection. The method combines clustering with uncertainty-based selection to ensure both diversity and informativeness. While the empirical results are strong and the motivation is clear, the technical novelty is limited and the paper lacks theoretical justification. The clustering-based diversity measure is not well justified, and important aspects like computational cost and what diversity actually captures are not thoroughly analyzed. The paper makes a solid empirical contribution but needs stronger theoretical grounding and more thorough analysis to be truly compelling."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Clear motivation: The paper identifies an important limitation of uncertainty-based outlier sampling - that it can be biased towards certain types of outliers - and proposes diversity as a complementary criterion\",\n\"Simple and practical method: DOS is straightforward to implement, combining clustering with uncertainty-based selection within each cluster\",\n\"Comprehensive experiments: The paper includes extensive experiments across multiple benchmarks (CIFAR-10, CIFAR-100) with different outlier datasets (TI-300K, TI-80M) and compares against relevant baselines\",\n\"Strong empirical results: DOS achieves significant improvements, with up to 25.79% reduction in FPR95 on CIFAR-100 with TI-300K\",\n\"Good ablation studies: The paper provides ablations on key components like the number of clusters and the impact of diversity\"\n],\n\"weaknesses\": [\n\"Limited novelty: The core idea of combining clustering with uncertainty-based selection is relatively straightforward and incremental. The technical contribution is limited\",\n\"Lack of theoretical justification: The paper does not provide theoretical analysis of why diversity helps or how the proposed method guarantees diverse sampling\",\n\"Clustering method not well justified: The choice of k-means clustering on normalized features is not well motivated. Why is this the right way to measure diversity? Other diversity metrics are not explored\",\n\"Computational cost not discussed: The paper does not analyze the computational overhead of performing clustering at each iteration, which could be significant for large outlier datasets\",\n\"Limited analysis of what diversity means: The paper does not provide clear analysis or visualization of what types of diversity are captured by the clustering approach\",\n\"Missing comparisons: Some recent OOD detection methods that also consider diversity or coverage are not compared\",\n\"Hyperparameter sensitivity: The method introduces additional hyperparameters (number of clusters K) but sensitivity analysis is limited\"\n],\n\"suggestions\": [\n\"Provide theoretical analysis or intuition about why diversity in outlier sampling improves OOD detection, beyond empirical observations\",\n\"Compare different diversity measures (e.g., determinantal point processes, core-set selection) and justify why clustering is the best choice\",\n\"Include computational cost analysis and wall-clock time comparisons with baselines\",\n\"Provide visualizations (e.g., t-SNE) showing how DOS samples differ from uncertainty-only sampling in terms of coverage of the outlier space\",\n\"Analyze what types of outliers are selected by DOS vs baselines - are they semantically diverse?\",\n\"Include more recent OOD detection baselines, especially those considering diversity or coverage\",\n\"Provide more extensive hyperparameter sensitivity analysis, especially for the number of clusters K across different settings\",\n\"Discuss failure cases or limitations of the diversity-based approach\"\n],\n\"summary\": \"This paper proposes DOS, a sampling strategy for selecting diverse outliers to improve OOD detection. The method combines clustering with uncertainty-based selection to ensure both diversity and informativeness. While the empirical results are strong and the motivation is clear, the technical novelty is limited and the paper lacks theoretical justification. The clustering-based diversity measure is not well justified, and important aspects like computational cost and what diversity actually captures are not thoroughly analyzed. The paper makes a solid empirical contribution but needs stronger theoretical grounding and more thorough analysis to be truly compelling.\"\n}\n```", "usage": {"prompt_tokens": 335, "completion_tokens": 759, "total_tokens": 1094}, "timestamp": "2025-12-29T06:13:35.859306"}
{"paper_id": "iriEqxFB4y", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["The paper addresses a well-motivated and important"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"The paper addresses a well-motivated and important\"\n  ]\n}\n```", "usage": {"completion_tokens": 1091, "prompt_tokens": 340, "total_tokens": 1431}, "timestamp": "2025-12-29T06:13:48.066553"}
{"paper_id": "yatNm6A6sR", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses a real practical need by providing a large-scale, structured road network database from OpenStreetMap data that can serve as a valuable resource for the research community", "Comprehensive scale with 1 billion nodes covering world-wide road networks, which is significantly larger than existing datasets", "Demonstrates practical utility through three diverse use cases: traffic prediction, city boundary detection, and traffic policy control", "Provides 31 datasets for traffic prediction benchmark, which is more comprehensive than previously available resources", "Includes data converters to facilitate integration of multimodal spatial-temporal data, improving accessibility and usability"], "weaknesses": ["The paper reads more as a dataset/system description rather than a research contribution with novel methodological insights", "Limited technical novelty - processing OSM data and creating a database, while useful, does not present significant algorithmic or theoretical contributions", "Insufficient details on data processing methodology, quality control measures, and validation procedures for the processed road network data", "The three use cases appear to be demonstrations rather than in-depth evaluations that validate the dataset's quality or reveal new insights", "Missing comparison with existing road network databases and benchmarks to clearly establish the advantages of OSM+", "Lack of discussion on data maintenance, update frequency, and long-term sustainability of the database", "No clear evaluation metrics or baseline results provided for the released benchmarks, making it difficult to assess their quality", "Limited discussion of potential biases or limitations in OpenStreetMap data that might affect downstream applications"], "suggestions": ["Reframe the paper to emphasize novel contributions beyond dataset creation, such as new processing techniques, quality assurance methods, or insights from large-scale analysis", "Provide detailed technical documentation on data processing pipeline, including data cleaning, validation, and quality control procedures", "Include comprehensive baseline experiments on the released benchmarks with multiple state-of-the-art methods to establish performance standards", "Add quantitative comparisons with existing road network databases (e.g., coverage, accuracy, completeness) to demonstrate clear advantages", "Discuss data quality issues, potential biases in OSM data, and how they were addressed or should be considered by users", "Provide a sustainability plan including update frequency, version control, and community contribution mechanisms", "Expand the use case evaluations to provide deeper insights and validation of the dataset quality", "Consider submitting to a datasets and benchmarks track or workshop rather than the main conference track, as the contribution is primarily a resource rather than a methodological advance"], "summary": "This paper presents OSM+, a large-scale structured road network database derived from OpenStreetMap data, along with benchmarks for traffic prediction and policy control tasks. While the resource could be valuable for the research community due to its comprehensive scale and accessibility, the paper lacks sufficient technical novelty and methodological contributions for a top-tier venue. The work is primarily a dataset/system contribution without significant algorithmic innovations or deep analytical insights. The paper would benefit from more rigorous evaluation, clearer positioning relative to existing resources, and either reframing to highlight novel technical contributions or submission to a more appropriate venue focused on datasets and benchmarks."}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"Addresses a real practical need by providing a large-scale, structured road network database from OpenStreetMap data that can serve as a valuable resource for the research community\",\n    \"Comprehensive scale with 1 billion nodes covering world-wide road networks, which is significantly larger than existing datasets\",\n    \"Demonstrates practical utility through three diverse use cases: traffic prediction, city boundary detection, and traffic policy control\",\n    \"Provides 31 datasets for traffic prediction benchmark, which is more comprehensive than previously available resources\",\n    \"Includes data converters to facilitate integration of multimodal spatial-temporal data, improving accessibility and usability\"\n  ],\n  \"weaknesses\": [\n    \"The paper reads more as a dataset/system description rather than a research contribution with novel methodological insights\",\n    \"Limited technical novelty - processing OSM data and creating a database, while useful, does not present significant algorithmic or theoretical contributions\",\n    \"Insufficient details on data processing methodology, quality control measures, and validation procedures for the processed road network data\",\n    \"The three use cases appear to be demonstrations rather than in-depth evaluations that validate the dataset's quality or reveal new insights\",\n    \"Missing comparison with existing road network databases and benchmarks to clearly establish the advantages of OSM+\",\n    \"Lack of discussion on data maintenance, update frequency, and long-term sustainability of the database\",\n    \"No clear evaluation metrics or baseline results provided for the released benchmarks, making it difficult to assess their quality\",\n    \"Limited discussion of potential biases or limitations in OpenStreetMap data that might affect downstream applications\"\n  ],\n  \"suggestions\": [\n    \"Reframe the paper to emphasize novel contributions beyond dataset creation, such as new processing techniques, quality assurance methods, or insights from large-scale analysis\",\n    \"Provide detailed technical documentation on data processing pipeline, including data cleaning, validation, and quality control procedures\",\n    \"Include comprehensive baseline experiments on the released benchmarks with multiple state-of-the-art methods to establish performance standards\",\n    \"Add quantitative comparisons with existing road network databases (e.g., coverage, accuracy, completeness) to demonstrate clear advantages\",\n    \"Discuss data quality issues, potential biases in OSM data, and how they were addressed or should be considered by users\",\n    \"Provide a sustainability plan including update frequency, version control, and community contribution mechanisms\",\n    \"Expand the use case evaluations to provide deeper insights and validation of the dataset quality\",\n    \"Consider submitting to a datasets and benchmarks track or workshop rather than the main conference track, as the contribution is primarily a resource rather than a methodological advance\"\n  ],\n  \"summary\": \"This paper presents OSM+, a large-scale structured road network database derived from OpenStreetMap data, along with benchmarks for traffic prediction and policy control tasks. While the resource could be valuable for the research community due to its comprehensive scale and accessibility, the paper lacks sufficient technical novelty and methodological contributions for a top-tier venue. The work is primarily a dataset/system contribution without significant algorithmic innovations or deep analytical insights. The paper would benefit from more rigorous evaluation, clearer positioning relative to existing resources, and either reframing to highlight novel technical contributions or submission to a more appropriate venue focused on datasets and benchmarks.\"\n}\n```", "usage": {"prompt_tokens": 309, "completion_tokens": 750, "total_tokens": 1059}, "timestamp": "2025-12-29T06:14:25.534821"}
{"paper_id": "yatNm6A6sR", "model": "google/gemini-2.5-pro", "response": {"score": 5, "strengths": ["Addresses a significant practical challenge: processing and standard"]}, "raw": "```json\n{\n  \"score\": 5,\n  \"strengths\": [\n    \"Addresses a significant practical challenge: processing and standard\"\n  ]\n}\n```", "usage": {"completion_tokens": 1073, "prompt_tokens": 328, "total_tokens": 1401}, "timestamp": "2025-12-29T06:14:38.203465"}
{"paper_id": "Fb0q2uI4Ha", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important and practical problem domain (traffic accident understanding) that has clear real-world applications", "Introduces a large-scale dataset (TAU-106K) with 106K traffic accident videos and images, which could be valuable for the research community", "Proposes a systematic video-to-image annotation pipeline to ensure comprehensive and high-quality labels", "Employs a multi-granularity training approach that integrates multiple tasks (accident recognition, spatial-temporal grounding, and description)", "Demonstrates superior performance through extensive experiments"], "weaknesses": ["Limited technical novelty - the approach appears to be a straightforward application of existing MLLM techniques to a new domain without significant methodological innovations", "Insufficient details about the video-to-image annotation pipeline and quality control measures", "Lack of comparison with domain-specific baselines or traditional computer vision methods for accident detection", "Missing analysis of failure cases and limitations of the proposed approach", "Ethical considerations regarding the use of real accident footage are not adequately addressed", "No discussion of potential biases in the dataset (geographic, temporal, accident type distribution)", "Limited ablation studies to justify design choices in the two-step training approach", "Unclear how the model handles edge cases or rare accident types"], "suggestions": ["Provide more detailed technical contributions beyond dataset construction and standard MLLM fine-tuning", "Include comprehensive ablation studies on the two-step training approach and the impact of different task combinations", "Add comparisons with traditional computer vision methods and domain-specific baselines", "Discuss ethical considerations, data privacy, and potential misuse of the technology", "Analyze dataset biases and their potential impact on model performance", "Include failure case analysis and discuss model limitations", "Provide more details on the annotation pipeline, inter-annotator agreement, and quality control", "Discuss generalization capabilities to unseen accident types or different geographic regions", "Consider adding temporal reasoning evaluation metrics specific to accident progression understanding"], "summary": "This paper presents TAU-106K, a large-scale multimodal dataset for traffic accident understanding, and TABot, an MLLM specialized for accident-related tasks. While the dataset contribution is valuable and the problem domain is important, the technical novelty is limited as it primarily applies existing MLLM techniques to a new domain. The paper would benefit from more rigorous ablation studies, ethical considerations, bias analysis, and comparisons with domain-specific baselines. The work makes a solid contribution to the field but needs strengthening in methodology and analysis to meet the standards of a top-tier venue."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important and practical problem domain (traffic accident understanding) that has clear real-world applications\",\n\"Introduces a large-scale dataset (TAU-106K) with 106K traffic accident videos and images, which could be valuable for the research community\",\n\"Proposes a systematic video-to-image annotation pipeline to ensure comprehensive and high-quality labels\",\n\"Employs a multi-granularity training approach that integrates multiple tasks (accident recognition, spatial-temporal grounding, and description)\",\n\"Demonstrates superior performance through extensive experiments\"\n],\n\"weaknesses\": [\n\"Limited technical novelty - the approach appears to be a straightforward application of existing MLLM techniques to a new domain without significant methodological innovations\",\n\"Insufficient details about the video-to-image annotation pipeline and quality control measures\",\n\"Lack of comparison with domain-specific baselines or traditional computer vision methods for accident detection\",\n\"Missing analysis of failure cases and limitations of the proposed approach\",\n\"Ethical considerations regarding the use of real accident footage are not adequately addressed\",\n\"No discussion of potential biases in the dataset (geographic, temporal, accident type distribution)\",\n\"Limited ablation studies to justify design choices in the two-step training approach\",\n\"Unclear how the model handles edge cases or rare accident types\"\n],\n\"suggestions\": [\n\"Provide more detailed technical contributions beyond dataset construction and standard MLLM fine-tuning\",\n\"Include comprehensive ablation studies on the two-step training approach and the impact of different task combinations\",\n\"Add comparisons with traditional computer vision methods and domain-specific baselines\",\n\"Discuss ethical considerations, data privacy, and potential misuse of the technology\",\n\"Analyze dataset biases and their potential impact on model performance\",\n\"Include failure case analysis and discuss model limitations\",\n\"Provide more details on the annotation pipeline, inter-annotator agreement, and quality control\",\n\"Discuss generalization capabilities to unseen accident types or different geographic regions\",\n\"Consider adding temporal reasoning evaluation metrics specific to accident progression understanding\"\n],\n\"summary\": \"This paper presents TAU-106K, a large-scale multimodal dataset for traffic accident understanding, and TABot, an MLLM specialized for accident-related tasks. While the dataset contribution is valuable and the problem domain is important, the technical novelty is limited as it primarily applies existing MLLM techniques to a new domain. The paper would benefit from more rigorous ablation studies, ethical considerations, bias analysis, and comparisons with domain-specific baselines. The work makes a solid contribution to the field but needs strengthening in methodology and analysis to meet the standards of a top-tier venue.\"\n}\n```", "usage": {"prompt_tokens": 362, "completion_tokens": 611, "total_tokens": 973}, "timestamp": "2025-12-29T06:15:14.507129"}
{"paper_id": "Fb0q2uI4Ha", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["The paper introduces TAU-106K"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"The paper introduces TAU-106K\"\n  ]\n}\n```", "usage": {"completion_tokens": 1543, "prompt_tokens": 376, "total_tokens": 1919}, "timestamp": "2025-12-29T06:15:31.375462"}
{"paper_id": "QkDUdPRcma", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important but understudied problem of direct spike encoding in SNNs, which is crucial for energy-efficient deep learning", "Introduces a biologically-inspired homeostasis concept to improve encoding efficiency and training performance", "Proposes three concrete components (dynamic feature encoding loss, adaptive threshold, feature diversity loss) that work together systematically", "Demonstrates consistent improvements across multiple datasets and architectures, showing generalizability", "Provides thorough analysis of conventional direct encoding limitations, motivating the proposed approach"], "weaknesses": ["The biological motivation for homeostasis in spike encoding could be explained more rigorously - the connection between biological homeostasis and the proposed method needs stronger justification", "Limited comparison with other recent encoding methods beyond conventional direct encoding - broader baseline comparisons would strengthen claims", "Computational overhead of the proposed method (three additional loss terms) is not thoroughly analyzed - training time and memory costs should be reported", "The adaptive threshold mechanism lacks theoretical analysis - why this particular adaptation strategy is optimal is unclear", "Ablation studies could be more comprehensive - individual contribution of each component and their interactions need deeper investigation", "Energy efficiency claims are not directly validated with actual energy measurements on neuromorphic hardware"], "suggestions": ["Provide theoretical analysis or stronger biological justification for why homeostasis principles apply to direct spike encoding", "Include comparisons with other recent encoding methods (e.g., rate coding variants, temporal coding schemes)", "Report detailed computational costs including training time, memory usage, and convergence speed comparisons", "Add ablation studies showing the effect of each loss component individually and in combination", "Validate energy efficiency claims with actual measurements on neuromorphic hardware (e.g., Loihi, TrueNorth) if possible", "Provide sensitivity analysis for hyperparameters introduced by the three proposed components", "Include visualization of learned encodings to provide intuition about how H-Direct differs from conventional encoding"], "summary": "This paper proposes H-Direct, a homeostasis-aware direct spike encoding method for deep SNNs that addresses limitations in conventional direct encoding. The approach introduces three components inspired by biological homeostasis: dynamic feature encoding loss, adaptive threshold, and feature diversity loss. While the paper demonstrates consistent performance improvements across multiple benchmarks and the biological motivation is interesting, the work would benefit from stronger theoretical justification, more comprehensive comparisons with alternative encoding methods, detailed computational cost analysis, and actual energy efficiency validation on neuromorphic hardware. The core idea is promising but needs more rigorous evaluation."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important but understudied problem of direct spike encoding in SNNs, which is crucial for energy-efficient deep learning\",\n\"Introduces a biologically-inspired homeostasis concept to improve encoding efficiency and training performance\",\n\"Proposes three concrete components (dynamic feature encoding loss, adaptive threshold, feature diversity loss) that work together systematically\",\n\"Demonstrates consistent improvements across multiple datasets and architectures, showing generalizability\",\n\"Provides thorough analysis of conventional direct encoding limitations, motivating the proposed approach\"\n],\n\"weaknesses\": [\n\"The biological motivation for homeostasis in spike encoding could be explained more rigorously - the connection between biological homeostasis and the proposed method needs stronger justification\",\n\"Limited comparison with other recent encoding methods beyond conventional direct encoding - broader baseline comparisons would strengthen claims\",\n\"Computational overhead of the proposed method (three additional loss terms) is not thoroughly analyzed - training time and memory costs should be reported\",\n\"The adaptive threshold mechanism lacks theoretical analysis - why this particular adaptation strategy is optimal is unclear\",\n\"Ablation studies could be more comprehensive - individual contribution of each component and their interactions need deeper investigation\",\n\"Energy efficiency claims are not directly validated with actual energy measurements on neuromorphic hardware\"\n],\n\"suggestions\": [\n\"Provide theoretical analysis or stronger biological justification for why homeostasis principles apply to direct spike encoding\",\n\"Include comparisons with other recent encoding methods (e.g., rate coding variants, temporal coding schemes)\",\n\"Report detailed computational costs including training time, memory usage, and convergence speed comparisons\",\n\"Add ablation studies showing the effect of each loss component individually and in combination\",\n\"Validate energy efficiency claims with actual measurements on neuromorphic hardware (e.g., Loihi, TrueNorth) if possible\",\n\"Provide sensitivity analysis for hyperparameters introduced by the three proposed components\",\n\"Include visualization of learned encodings to provide intuition about how H-Direct differs from conventional encoding\"\n],\n\"summary\": \"This paper proposes H-Direct, a homeostasis-aware direct spike encoding method for deep SNNs that addresses limitations in conventional direct encoding. The approach introduces three components inspired by biological homeostasis: dynamic feature encoding loss, adaptive threshold, and feature diversity loss. While the paper demonstrates consistent performance improvements across multiple benchmarks and the biological motivation is interesting, the work would benefit from stronger theoretical justification, more comprehensive comparisons with alternative encoding methods, detailed computational cost analysis, and actual energy efficiency validation on neuromorphic hardware. The core idea is promising but needs more rigorous evaluation.\"\n}\n```", "usage": {"prompt_tokens": 355, "completion_tokens": 589, "total_tokens": 944}, "timestamp": "2025-12-29T06:16:02.685516"}
{"paper_id": "QkDUdPRcma", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["The paper addresses a critical and often under-"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"The paper addresses a critical and often under-\"\n  ]\n}\n```", "usage": {"completion_tokens": 1547, "prompt_tokens": 367, "total_tokens": 1914}, "timestamp": "2025-12-29T06:16:18.513411"}
{"paper_id": "r0kY4SS7ts", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important gap in the literature by studying Reward-Potential Markov Games (RPMGs), which are more general and easier to verify than Markov Potential Games (MPGs)", "Provides comprehensive complexity-theoretic characterization with both hardness results (PPAD-hardness for stationary NE in infinite-horizon and nonstationary NE in finite-horizon settings) and positive algorithmic results", "The positive result for RPMGs with additive transitions is significant, showing polynomial-time computation of \u03b5-approximate NE with runtime polynomial in 1/\u03b5, breaking the curse of multiagents", "Introduces the novel concept of Adversarial Reward-Potential Markov Games, extending the framework in an interesting direction", "Well-motivated problem with clear practical relevance given that reward potentials are easier to verify than value function potentials"], "weaknesses": ["The abstract lacks clarity on what 'additive transitions' means - this is a crucial assumption for the positive result but is not defined or explained", "No experimental validation or empirical evaluation of the proposed algorithm is mentioned", "The relationship between RPMGs and MPGs could be more precisely characterized - when does an RPMG become an MPG?", "Missing details on the algorithm itself - what is the computational approach? Is it value iteration, policy gradient, or something else?", "The adversarial extension seems somewhat disconnected from the main narrative and its motivation is unclear", "No discussion of sample complexity or learning-theoretic aspects - only computational complexity is addressed", "Limited discussion of applications despite mentioning them in the title"], "suggestions": ["Provide a clear definition of 'additive transitions' in the abstract or introduction", "Include at least one concrete application domain where RPMGs naturally arise and demonstrate the algorithm's effectiveness", "Add a formal theorem statement or proposition clarifying the relationship between RPMGs and MPGs", "Provide more algorithmic details: what is the high-level approach? How does it differ from existing methods for MPGs?", "Better motivate the adversarial extension - what practical scenarios does this model capture?", "Discuss sample complexity: if learning from samples, how many are needed to achieve \u03b5-approximate NE?", "Include experimental results comparing your algorithm to baselines on benchmark problems", "Clarify whether the PPAD-hardness results are novel or follow from existing reductions", "Discuss the tightness of your results - are there matching lower bounds for the positive algorithmic result?"], "summary": "This paper studies Nash equilibria in Reward-Potential Markov Games (RPMGs), a class of games that is more general than Markov Potential Games. The main contributions include PPAD-hardness results for computing stationary NE in infinite-horizon settings and nonstationary NE in finite-horizon settings, along with a positive algorithmic result for RPMGs with additive transitions. While the theoretical contributions appear solid and address an important gap in the literature, the paper would benefit from clearer exposition of key concepts (especially 'additive transitions'), empirical validation, and better motivation for some extensions. The work makes meaningful theoretical progress but needs strengthening in presentation and experimental validation."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important gap in the literature by studying Reward-Potential Markov Games (RPMGs), which are more general and easier to verify than Markov Potential Games (MPGs)\",\n\"Provides comprehensive complexity-theoretic characterization with both hardness results (PPAD-hardness for stationary NE in infinite-horizon and nonstationary NE in finite-horizon settings) and positive algorithmic results\",\n\"The positive result for RPMGs with additive transitions is significant, showing polynomial-time computation of \u03b5-approximate NE with runtime polynomial in 1/\u03b5, breaking the curse of multiagents\",\n\"Introduces the novel concept of Adversarial Reward-Potential Markov Games, extending the framework in an interesting direction\",\n\"Well-motivated problem with clear practical relevance given that reward potentials are easier to verify than value function potentials\"\n],\n\"weaknesses\": [\n\"The abstract lacks clarity on what 'additive transitions' means - this is a crucial assumption for the positive result but is not defined or explained\",\n\"No experimental validation or empirical evaluation of the proposed algorithm is mentioned\",\n\"The relationship between RPMGs and MPGs could be more precisely characterized - when does an RPMG become an MPG?\",\n\"Missing details on the algorithm itself - what is the computational approach? Is it value iteration, policy gradient, or something else?\",\n\"The adversarial extension seems somewhat disconnected from the main narrative and its motivation is unclear\",\n\"No discussion of sample complexity or learning-theoretic aspects - only computational complexity is addressed\",\n\"Limited discussion of applications despite mentioning them in the title\"\n],\n\"suggestions\": [\n\"Provide a clear definition of 'additive transitions' in the abstract or introduction\",\n\"Include at least one concrete application domain where RPMGs naturally arise and demonstrate the algorithm's effectiveness\",\n\"Add a formal theorem statement or proposition clarifying the relationship between RPMGs and MPGs\",\n\"Provide more algorithmic details: what is the high-level approach? How does it differ from existing methods for MPGs?\",\n\"Better motivate the adversarial extension - what practical scenarios does this model capture?\",\n\"Discuss sample complexity: if learning from samples, how many are needed to achieve \u03b5-approximate NE?\",\n\"Include experimental results comparing your algorithm to baselines on benchmark problems\",\n\"Clarify whether the PPAD-hardness results are novel or follow from existing reductions\",\n\"Discuss the tightness of your results - are there matching lower bounds for the positive algorithmic result?\"\n],\n\"summary\": \"This paper studies Nash equilibria in Reward-Potential Markov Games (RPMGs), a class of games that is more general than Markov Potential Games. The main contributions include PPAD-hardness results for computing stationary NE in infinite-horizon settings and nonstationary NE in finite-horizon settings, along with a positive algorithmic result for RPMGs with additive transitions. While the theoretical contributions appear solid and address an important gap in the literature, the paper would benefit from clearer exposition of key concepts (especially 'additive transitions'), empirical validation, and better motivation for some extensions. The work makes meaningful theoretical progress but needs strengthening in presentation and experimental validation.\"\n}\n```", "usage": {"prompt_tokens": 394, "completion_tokens": 752, "total_tokens": 1146}, "timestamp": "2025-12-29T06:16:56.737511"}
{"paper_id": "r0kY4SS7ts", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Introduces and motivates a novel and well"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Introduces and motivates a novel and well\"\n  ]\n}\n```", "usage": {"completion_tokens": 1469, "prompt_tokens": 362, "total_tokens": 1831}, "timestamp": "2025-12-29T06:17:12.280978"}
{"paper_id": "0yTf37PXcH", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important problem of improving visual understanding in vision-language models", "Proposes two concrete technical contributions: MM-LoRA for decoder enhancement and QLadder for visual encoder improvement", "The disentangled parameter design in MM-LoRA is intuitive and allows specialized learning for each modality", "QLadder's hierarchical aggregation of intermediate features from frozen encoders is a reasonable approach to preserve pretrained capabilities while learning new features", "Claims to provide extensive experiments and ablation studies"], "weaknesses": ["The abstract lacks specific quantitative results or performance metrics to support effectiveness claims", "No comparison with existing methods or baselines mentioned in the abstract", "The novelty of using LoRA variants and adapter modules is limited given extensive prior work in parameter-efficient fine-tuning", "Unclear how MM-LoRA differs substantially from existing multi-modal adaptation techniques", "The 'ladder' structure in QLadder is not clearly explained - what makes it a 'ladder' and how does it differ from standard feature aggregation?", "Missing details on computational overhead and efficiency of the proposed modules", "No discussion of limitations or failure cases", "The name 'Arcana' appears unmotivated and doesn't clearly relate to the technical contributions"], "suggestions": ["Include specific quantitative results in the abstract (e.g., performance improvements on standard benchmarks)", "Provide clearer technical distinction between MM-LoRA and existing multi-modal LoRA approaches", "Explain the 'ladder' metaphor more concretely - what is the architectural structure?", "Add computational cost analysis comparing to baseline methods", "Include comparisons with recent state-of-the-art vision-language models", "Discuss what types of visual understanding improvements are achieved (e.g., fine-grained recognition, spatial reasoning, etc.)", "Provide ablation studies showing individual contributions of MM-LoRA vs QLadder", "Consider renaming to better reflect the technical approach or keep the name but provide motivation"], "summary": "This paper proposes Arcana, a multimodal language model with two main components: MM-LoRA for decoder enhancement using disentangled vision and language parameters, and QLadder for visual encoder improvement through hierarchical feature aggregation. While the problem is important and the approach is reasonable, the abstract lacks sufficient technical detail and quantitative evidence to assess novelty and effectiveness. The contributions appear incremental over existing parameter-efficient fine-tuning methods, and key architectural details (especially the 'ladder' structure) need clarification. The paper would benefit from concrete performance metrics, clearer technical distinctions from prior work, and efficiency analysis."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important problem of improving visual understanding in vision-language models\",\n\"Proposes two concrete technical contributions: MM-LoRA for decoder enhancement and QLadder for visual encoder improvement\",\n\"The disentangled parameter design in MM-LoRA is intuitive and allows specialized learning for each modality\",\n\"QLadder's hierarchical aggregation of intermediate features from frozen encoders is a reasonable approach to preserve pretrained capabilities while learning new features\",\n\"Claims to provide extensive experiments and ablation studies\"\n],\n\"weaknesses\": [\n\"The abstract lacks specific quantitative results or performance metrics to support effectiveness claims\",\n\"No comparison with existing methods or baselines mentioned in the abstract\",\n\"The novelty of using LoRA variants and adapter modules is limited given extensive prior work in parameter-efficient fine-tuning\",\n\"Unclear how MM-LoRA differs substantially from existing multi-modal adaptation techniques\",\n\"The 'ladder' structure in QLadder is not clearly explained - what makes it a 'ladder' and how does it differ from standard feature aggregation?\",\n\"Missing details on computational overhead and efficiency of the proposed modules\",\n\"No discussion of limitations or failure cases\",\n\"The name 'Arcana' appears unmotivated and doesn't clearly relate to the technical contributions\"\n],\n\"suggestions\": [\n\"Include specific quantitative results in the abstract (e.g., performance improvements on standard benchmarks)\",\n\"Provide clearer technical distinction between MM-LoRA and existing multi-modal LoRA approaches\",\n\"Explain the 'ladder' metaphor more concretely - what is the architectural structure?\",\n\"Add computational cost analysis comparing to baseline methods\",\n\"Include comparisons with recent state-of-the-art vision-language models\",\n\"Discuss what types of visual understanding improvements are achieved (e.g., fine-grained recognition, spatial reasoning, etc.)\",\n\"Provide ablation studies showing individual contributions of MM-LoRA vs QLadder\",\n\"Consider renaming to better reflect the technical approach or keep the name but provide motivation\"\n],\n\"summary\": \"This paper proposes Arcana, a multimodal language model with two main components: MM-LoRA for decoder enhancement using disentangled vision and language parameters, and QLadder for visual encoder improvement through hierarchical feature aggregation. While the problem is important and the approach is reasonable, the abstract lacks sufficient technical detail and quantitative evidence to assess novelty and effectiveness. The contributions appear incremental over existing parameter-efficient fine-tuning methods, and key architectural details (especially the 'ladder' structure) need clarification. The paper would benefit from concrete performance metrics, clearer technical distinctions from prior work, and efficiency analysis.\"\n}\n```", "usage": {"prompt_tokens": 356, "completion_tokens": 635, "total_tokens": 991}, "timestamp": "2025-12-29T06:17:49.275503"}
{"paper_id": "0yTf37PXcH", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["The paper addresses"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"The paper addresses\"\n  ]\n}\n```", "usage": {"completion_tokens": 1295, "prompt_tokens": 345, "total_tokens": 1640}, "timestamp": "2025-12-29T06:18:05.344226"}
{"paper_id": "yIKjkRZBrX", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important limitation of fixed-length skills by proposing variable-length skills based on decision point identification", "Novel use of state-action novelty as a proxy for critical decision points, which is computationally efficient and leverages offline data", "Comprehensive experimental evaluation across multiple domains (robotic manipulation and maze navigation)", "Clear motivation with concrete examples (e.g., crossroad scenario) that illustrate the problem with fixed-length skills", "Demonstrates improved sample efficiency and transfer learning capabilities compared to baselines"], "weaknesses": ["The novelty module design and its theoretical justification for identifying decision points could be more rigorously explained", "Limited analysis on how the method performs with different quality offline datasets or when offline data is scarce", "Computational overhead of the novelty module during training is not thoroughly discussed", "Ablation studies could be more comprehensive, particularly regarding the impact of different components of the novelty detection mechanism", "The relationship between novelty and true decision points may not always hold - some novel states may not be critical decision points and vice versa", "Comparison with other variable-length skill learning methods beyond fixed-length baselines would strengthen the evaluation"], "suggestions": ["Provide theoretical analysis or empirical evidence showing when novelty correlates with critical decision points", "Include experiments with varying quality and quantity of offline data to understand robustness", "Add computational cost analysis comparing NBDI to baseline methods", "Conduct more detailed ablation studies on novelty threshold selection and different novelty metrics", "Discuss failure cases where novelty-based identification might miss important decision points or identify false positives", "Compare with other adaptive or variable-length skill learning approaches if available", "Provide visualization of identified decision points to give intuition about what the method learns", "Include sensitivity analysis for key hyperparameters"], "summary": "This paper presents NBDI, a method for learning variable-length skills by identifying decision points through state-action novelty detection using offline data. The core idea is intuitive and addresses a real limitation of fixed-length skills. The experimental results show promising improvements in sample efficiency and transfer learning. However, the paper would benefit from stronger theoretical justification for why novelty serves as a good proxy for decision points, more comprehensive ablations, and analysis of robustness to offline data quality. The work makes a solid contribution to hierarchical RL, but some aspects need further development for publication at a top venue."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important limitation of fixed-length skills by proposing variable-length skills based on decision point identification\",\n\"Novel use of state-action novelty as a proxy for critical decision points, which is computationally efficient and leverages offline data\",\n\"Comprehensive experimental evaluation across multiple domains (robotic manipulation and maze navigation)\",\n\"Clear motivation with concrete examples (e.g., crossroad scenario) that illustrate the problem with fixed-length skills\",\n\"Demonstrates improved sample efficiency and transfer learning capabilities compared to baselines\"\n],\n\"weaknesses\": [\n\"The novelty module design and its theoretical justification for identifying decision points could be more rigorously explained\",\n\"Limited analysis on how the method performs with different quality offline datasets or when offline data is scarce\",\n\"Computational overhead of the novelty module during training is not thoroughly discussed\",\n\"Ablation studies could be more comprehensive, particularly regarding the impact of different components of the novelty detection mechanism\",\n\"The relationship between novelty and true decision points may not always hold - some novel states may not be critical decision points and vice versa\",\n\"Comparison with other variable-length skill learning methods beyond fixed-length baselines would strengthen the evaluation\"\n],\n\"suggestions\": [\n\"Provide theoretical analysis or empirical evidence showing when novelty correlates with critical decision points\",\n\"Include experiments with varying quality and quantity of offline data to understand robustness\",\n\"Add computational cost analysis comparing NBDI to baseline methods\",\n\"Conduct more detailed ablation studies on novelty threshold selection and different novelty metrics\",\n\"Discuss failure cases where novelty-based identification might miss important decision points or identify false positives\",\n\"Compare with other adaptive or variable-length skill learning approaches if available\",\n\"Provide visualization of identified decision points to give intuition about what the method learns\",\n\"Include sensitivity analysis for key hyperparameters\"\n],\n\"summary\": \"This paper presents NBDI, a method for learning variable-length skills by identifying decision points through state-action novelty detection using offline data. The core idea is intuitive and addresses a real limitation of fixed-length skills. The experimental results show promising improvements in sample efficiency and transfer learning. However, the paper would benefit from stronger theoretical justification for why novelty serves as a good proxy for decision points, more comprehensive ablations, and analysis of robustness to offline data quality. The work makes a solid contribution to hierarchical RL, but some aspects need further development for publication at a top venue.\"\n}\n```", "usage": {"prompt_tokens": 338, "completion_tokens": 570, "total_tokens": 908}, "timestamp": "2025-12-29T06:18:35.266090"}
{"paper_id": "yIKjkRZBrX", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["The paper addresses a well-motivated and significant"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"The paper addresses a well-motivated and significant\"\n  ]\n}\n```", "usage": {"completion_tokens": 1158, "prompt_tokens": 352, "total_tokens": 1510}, "timestamp": "2025-12-29T06:18:47.944533"}
{"paper_id": "GkWA6NjePN", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 7, "strengths": ["Novel theoretical contribution with the first unbiased, higher-derivative-free policy gradient algorithm for learning-aware reinforcement learning", "Strong empirical results on standard social dilemmas demonstrating cooperative behavior", "Addresses the important problem of multi-agent cooperation among self-interested agents", "Combines learning-aware modeling with efficient sequence models to handle long observation histories", "Provides theoretical analysis through the iterated prisoner's dilemma to explain emergence of cooperation"], "weaknesses": ["Limited discussion of computational complexity and scalability to larger numbers of agents", "Unclear how the approach generalizes beyond the tested social dilemmas to other multi-agent settings", "Insufficient comparison with other recent learning-aware methods and meta-learning approaches", "The assumption that agents can observe learning dynamics of others may be restrictive in practice", "Missing ablation studies to isolate the contribution of different components (learning-aware gradients vs. long-context modeling)"], "suggestions": ["Include computational complexity analysis and wall-clock time comparisons", "Provide more extensive ablation studies separating the effects of learning-aware gradients and sequence modeling", "Compare against a broader range of baselines including recent meta-learning and opponent modeling methods", "Discuss limitations and failure cases where the approach may not lead to cooperation", "Extend experiments to settings with more than 2 agents to demonstrate scalability", "Clarify the observability assumptions and how they might be relaxed in practical scenarios", "Provide more details on hyperparameter sensitivity and training stability"], "summary": "This paper presents a novel policy gradient algorithm for learning-aware multi-agent reinforcement learning that enables cooperation among self-interested agents. The key contributions are an unbiased, higher-derivative-free gradient estimator and the use of sequence models to capture learning dynamics. While the theoretical contribution is solid and empirical results on social dilemmas are promising, the paper would benefit from more thorough empirical analysis including ablations, broader comparisons, and scalability studies. The work addresses an important problem and shows promise, but needs strengthening in experimental validation and discussion of limitations."}, "raw": "```json\n{\n\"score\": 7,\n\"strengths\": [\n\"Novel theoretical contribution with the first unbiased, higher-derivative-free policy gradient algorithm for learning-aware reinforcement learning\",\n\"Strong empirical results on standard social dilemmas demonstrating cooperative behavior\",\n\"Addresses the important problem of multi-agent cooperation among self-interested agents\",\n\"Combines learning-aware modeling with efficient sequence models to handle long observation histories\",\n\"Provides theoretical analysis through the iterated prisoner's dilemma to explain emergence of cooperation\"\n],\n\"weaknesses\": [\n\"Limited discussion of computational complexity and scalability to larger numbers of agents\",\n\"Unclear how the approach generalizes beyond the tested social dilemmas to other multi-agent settings\",\n\"Insufficient comparison with other recent learning-aware methods and meta-learning approaches\",\n\"The assumption that agents can observe learning dynamics of others may be restrictive in practice\",\n\"Missing ablation studies to isolate the contribution of different components (learning-aware gradients vs. long-context modeling)\"\n],\n\"suggestions\": [\n\"Include computational complexity analysis and wall-clock time comparisons\",\n\"Provide more extensive ablation studies separating the effects of learning-aware gradients and sequence modeling\",\n\"Compare against a broader range of baselines including recent meta-learning and opponent modeling methods\",\n\"Discuss limitations and failure cases where the approach may not lead to cooperation\",\n\"Extend experiments to settings with more than 2 agents to demonstrate scalability\",\n\"Clarify the observability assumptions and how they might be relaxed in practical scenarios\",\n\"Provide more details on hyperparameter sensitivity and training stability\"\n],\n\"summary\": \"This paper presents a novel policy gradient algorithm for learning-aware multi-agent reinforcement learning that enables cooperation among self-interested agents. The key contributions are an unbiased, higher-derivative-free gradient estimator and the use of sequence models to capture learning dynamics. While the theoretical contribution is solid and empirical results on social dilemmas are promising, the paper would benefit from more thorough empirical analysis including ablations, broader comparisons, and scalability studies. The work addresses an important problem and shows promise, but needs strengthening in experimental validation and discussion of limitations.\"\n}\n```", "usage": {"prompt_tokens": 284, "completion_tokens": 498, "total_tokens": 782}, "timestamp": "2025-12-29T06:19:20.022916"}
{"paper_id": "GkWA6NjePN", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Addresses the fundamental and important problem of emergent cooperation in"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses the fundamental and important problem of emergent cooperation in\"\n  ]\n}\n```", "usage": {"completion_tokens": 1093, "prompt_tokens": 305, "total_tokens": 1398}, "timestamp": "2025-12-29T06:19:32.983286"}
{"paper_id": "b42wmsdwmB", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important problem of modality flexibility in multimodal human sensing, allowing dynamic addition/removal of modalities without retraining", "Proposes a novel X-fusion mechanism that preserves modality-specific features while enabling cross-modal learning", "Comprehensive experimental validation across multiple datasets (MM-Fi, XRF55) and six different modalities", "Demonstrates state-of-the-art performance on both human pose estimation (HPE) and human activity recognition (HAR) tasks", "The modality-invariant design is practically valuable for real-world deployment scenarios"], "weaknesses": ["The paper lacks sufficient technical details about the X-fusion mechanism - the core contribution needs more rigorous mathematical formulation and architectural specifics", "Limited ablation studies to validate individual components of the proposed approach (e.g., impact of X-fusion vs. standard fusion methods)", "The claim of being a 'foundation model' may be overstated - unclear if the model demonstrates true foundation model characteristics like zero-shot transfer or broad generalization", "Missing comparisons with recent multimodal fusion methods and transformer-based approaches in human sensing", "Computational complexity and efficiency analysis is absent - important for practical deployment", "The scalability claims need more rigorous validation - experiments with varying numbers of modalities and systematic analysis of performance degradation"], "suggestions": ["Provide detailed mathematical formulation of the X-fusion mechanism with clear architectural diagrams", "Include comprehensive ablation studies isolating the contribution of each component", "Add comparisons with recent state-of-the-art multimodal fusion methods (e.g., attention-based fusion, late fusion strategies)", "Provide computational cost analysis including FLOPs, parameters, and inference time comparisons", "Demonstrate zero-shot or few-shot capabilities if claiming foundation model status", "Include experiments systematically varying the number and types of modalities to validate scalability claims", "Discuss failure cases and limitations more thoroughly", "Add visualization of learned features to demonstrate modality-specific preservation"], "summary": "This paper presents X-Fi, a modality-invariant model for multimodal human sensing that aims to flexibly handle different sensor modalities without retraining. While the problem is well-motivated and the experimental results show promise, the paper suffers from insufficient technical depth in describing the core X-fusion mechanism and lacks rigorous ablation studies. The foundation model claim needs better justification, and comparisons with recent methods are missing. The work would benefit from more detailed technical exposition, comprehensive ablations, and efficiency analysis before it can be considered a strong contribution to the field."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important problem of modality flexibility in multimodal human sensing, allowing dynamic addition/removal of modalities without retraining\",\n\"Proposes a novel X-fusion mechanism that preserves modality-specific features while enabling cross-modal learning\",\n\"Comprehensive experimental validation across multiple datasets (MM-Fi, XRF55) and six different modalities\",\n\"Demonstrates state-of-the-art performance on both human pose estimation (HPE) and human activity recognition (HAR) tasks\",\n\"The modality-invariant design is practically valuable for real-world deployment scenarios\"\n],\n\"weaknesses\": [\n\"The paper lacks sufficient technical details about the X-fusion mechanism - the core contribution needs more rigorous mathematical formulation and architectural specifics\",\n\"Limited ablation studies to validate individual components of the proposed approach (e.g., impact of X-fusion vs. standard fusion methods)\",\n\"The claim of being a 'foundation model' may be overstated - unclear if the model demonstrates true foundation model characteristics like zero-shot transfer or broad generalization\",\n\"Missing comparisons with recent multimodal fusion methods and transformer-based approaches in human sensing\",\n\"Computational complexity and efficiency analysis is absent - important for practical deployment\",\n\"The scalability claims need more rigorous validation - experiments with varying numbers of modalities and systematic analysis of performance degradation\"\n],\n\"suggestions\": [\n\"Provide detailed mathematical formulation of the X-fusion mechanism with clear architectural diagrams\",\n\"Include comprehensive ablation studies isolating the contribution of each component\",\n\"Add comparisons with recent state-of-the-art multimodal fusion methods (e.g., attention-based fusion, late fusion strategies)\",\n\"Provide computational cost analysis including FLOPs, parameters, and inference time comparisons\",\n\"Demonstrate zero-shot or few-shot capabilities if claiming foundation model status\",\n\"Include experiments systematically varying the number and types of modalities to validate scalability claims\",\n\"Discuss failure cases and limitations more thoroughly\",\n\"Add visualization of learned features to demonstrate modality-specific preservation\"\n],\n\"summary\": \"This paper presents X-Fi, a modality-invariant model for multimodal human sensing that aims to flexibly handle different sensor modalities without retraining. While the problem is well-motivated and the experimental results show promise, the paper suffers from insufficient technical depth in describing the core X-fusion mechanism and lacks rigorous ablation studies. The foundation model claim needs better justification, and comparisons with recent methods are missing. The work would benefit from more detailed technical exposition, comprehensive ablations, and efficiency analysis before it can be considered a strong contribution to the field.\"\n}\n```", "usage": {"prompt_tokens": 378, "completion_tokens": 612, "total_tokens": 990}, "timestamp": "2025-12-29T06:20:06.501010"}
{"paper_id": "b42wmsdwmB", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["Addresses a well-defined and significant problem in multimodal"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"Addresses a well-defined and significant problem in multimodal\"\n  ]\n}\n```", "usage": {"completion_tokens": 1081, "prompt_tokens": 371, "total_tokens": 1452}, "timestamp": "2025-12-29T06:20:19.176589"}
{"paper_id": "3wde105NL2", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 1, "strengths": [], "weaknesses": ["No actual content provided - title and abstract both simply say 'test'", "Impossible to evaluate scientific merit, novelty, or contribution", "Does not meet minimum standards for a conference submission", "No methodology, experiments, results, or discussion present", "Cannot assess technical quality, clarity, or significance"], "suggestions": ["Submit a complete paper with proper title, abstract, introduction, methodology, experiments, and conclusions", "Ensure the submission includes novel research contributions", "Follow ICLR formatting guidelines and page requirements", "Provide sufficient technical detail for reproducibility", "Include proper literature review and comparison with related work"], "summary": "This submission contains no actual content and appears to be a placeholder or test submission. It cannot be evaluated as a scientific contribution and should be rejected. A complete paper addressing a research problem in machine learning is required for consideration."}, "raw": "```json\n{\n\"score\": 1,\n\"strengths\": [],\n\"weaknesses\": [\n\"No actual content provided - title and abstract both simply say 'test'\",\n\"Impossible to evaluate scientific merit, novelty, or contribution\",\n\"Does not meet minimum standards for a conference submission\",\n\"No methodology, experiments, results, or discussion present\",\n\"Cannot assess technical quality, clarity, or significance\"\n],\n\"suggestions\": [\n\"Submit a complete paper with proper title, abstract, introduction, methodology, experiments, and conclusions\",\n\"Ensure the submission includes novel research contributions\",\n\"Follow ICLR formatting guidelines and page requirements\",\n\"Provide sufficient technical detail for reproducibility\",\n\"Include proper literature review and comparison with related work\"\n],\n\"summary\": \"This submission contains no actual content and appears to be a placeholder or test submission. It cannot be evaluated as a scientific contribution and should be rejected. A complete paper addressing a research problem in machine learning is required for consideration.\"\n}\n```", "usage": {"prompt_tokens": 71, "completion_tokens": 226, "total_tokens": 297}, "timestamp": "2025-12-29T06:20:44.735235"}
{"paper_id": "3wde105NL2", "model": "google/gemini-2.5-pro", "response": {"parse_error": true, "raw": "```json\n{\n  \"score\": 1,\n  \"strengths\": [],\n  \"weaknesses\": [\n    \"The submission"}, "raw": "```json\n{\n  \"score\": 1,\n  \"strengths\": [],\n  \"weaknesses\": [\n    \"The submission", "usage": {"completion_tokens": 1592, "prompt_tokens": 269, "total_tokens": 1861}, "timestamp": "2025-12-29T06:21:00.120725"}
{"paper_id": "4EjdYiNRzE", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 7, "strengths": ["Achieves O(d/T) convergence rate for DDPM under minimal assumptions (only finite first-order moment), which is a significant theoretical improvement over existing results", "Removes stringent assumptions commonly required in prior work such as log-Sobolev inequalities, strong log-concavity, or bounded support", "Provides a novel analytical framework for fine-grained error propagation analysis in the reverse diffusion process", "The result applies to general target distributions, making it broadly applicable", "Clear theoretical contribution with well-motivated problem setup"], "weaknesses": ["The paper appears to be theory-focused with no empirical validation or experiments to support the theoretical findings", "The assumption of \u2113\u2082-accurate score estimates may be strong in practice - no discussion of how realistic this assumption is or how to verify it", "Missing comparison with other samplers (e.g., probability flow ODE, DDIM) - unclear if similar rates can be achieved with different sampling methods", "The 'ignoring logarithmic factors' caveat could hide important dependencies - these factors should be made explicit", "No discussion of the constants hidden in the O(\u00b7) notation, which could be large and affect practical implications", "Limited discussion on the tightness of the bound or whether O(d/T) is optimal for this problem setting"], "suggestions": ["Include numerical experiments to validate the theoretical convergence rate, even on simple synthetic distributions", "Provide explicit dependence on all problem parameters including logarithmic factors and constants", "Discuss the practical implications of the \u2113\u2082-accurate score assumption and how it relates to neural network approximation", "Compare with convergence rates of other diffusion samplers (DDIM, probability flow ODE) under similar assumptions", "Add lower bounds or discuss optimality of the O(d/T) rate", "Provide concrete examples of distributions satisfying your assumptions and discuss failure cases", "Include a more detailed proof sketch in the main paper to highlight the key novel technical contributions"], "summary": "This paper establishes an improved O(d/T) convergence rate for DDPM under minimal assumptions (finite first-order moment), which is a significant theoretical advancement. The main contribution is removing stringent assumptions required by prior work while achieving better rates. However, the paper would benefit from empirical validation, explicit treatment of all constants and logarithmic factors, and discussion of optimality. The theoretical contribution is solid but the practical implications remain unclear."}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"Achieves O(d/T) convergence rate for DDPM under minimal assumptions (only finite first-order moment), which is a significant theoretical improvement over existing results\",\n    \"Removes stringent assumptions commonly required in prior work such as log-Sobolev inequalities, strong log-concavity, or bounded support\",\n    \"Provides a novel analytical framework for fine-grained error propagation analysis in the reverse diffusion process\",\n    \"The result applies to general target distributions, making it broadly applicable\",\n    \"Clear theoretical contribution with well-motivated problem setup\"\n  ],\n  \"weaknesses\": [\n    \"The paper appears to be theory-focused with no empirical validation or experiments to support the theoretical findings\",\n    \"The assumption of \u2113\u2082-accurate score estimates may be strong in practice - no discussion of how realistic this assumption is or how to verify it\",\n    \"Missing comparison with other samplers (e.g., probability flow ODE, DDIM) - unclear if similar rates can be achieved with different sampling methods\",\n    \"The 'ignoring logarithmic factors' caveat could hide important dependencies - these factors should be made explicit\",\n    \"No discussion of the constants hidden in the O(\u00b7) notation, which could be large and affect practical implications\",\n    \"Limited discussion on the tightness of the bound or whether O(d/T) is optimal for this problem setting\"\n  ],\n  \"suggestions\": [\n    \"Include numerical experiments to validate the theoretical convergence rate, even on simple synthetic distributions\",\n    \"Provide explicit dependence on all problem parameters including logarithmic factors and constants\",\n    \"Discuss the practical implications of the \u2113\u2082-accurate score assumption and how it relates to neural network approximation\",\n    \"Compare with convergence rates of other diffusion samplers (DDIM, probability flow ODE) under similar assumptions\",\n    \"Add lower bounds or discuss optimality of the O(d/T) rate\",\n    \"Provide concrete examples of distributions satisfying your assumptions and discuss failure cases\",\n    \"Include a more detailed proof sketch in the main paper to highlight the key novel technical contributions\"\n  ],\n  \"summary\": \"This paper establishes an improved O(d/T) convergence rate for DDPM under minimal assumptions (finite first-order moment), which is a significant theoretical advancement. The main contribution is removing stringent assumptions required by prior work while achieving better rates. However, the paper would benefit from empirical validation, explicit treatment of all constants and logarithmic factors, and discussion of optimality. The theoretical contribution is solid but the practical implications remain unclear.\"\n}\n```", "usage": {"prompt_tokens": 353, "completion_tokens": 610, "total_tokens": 963}, "timestamp": "2025-12-29T06:21:25.262783"}
{"paper_id": "4EjdYiNRzE", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["The paper addresses a fundamental and highly"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"The paper addresses a fundamental and highly\"\n  ]\n}\n```", "usage": {"completion_tokens": 1048, "prompt_tokens": 344, "total_tokens": 1392}, "timestamp": "2025-12-29T06:21:37.949777"}
{"paper_id": "eJFBMqCE4X", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Provides a unifying theoretical framework connecting discriminative self-supervised methods through a graphical model perspective", "Bridges the gap between discriminative and generative approaches to self-supervised learning, offering theoretical insights into what discriminative methods implicitly learn", "Demonstrates empirical improvements over previous VAE-based methods on multiple standard benchmarks", "The theoretical analysis provides principled understanding of self-supervised learning methods that have previously lacked comprehensive theoretical grounding"], "weaknesses": ["The gap to discriminative methods is only 'narrowed' but not closed - the proposed method still underperforms state-of-the-art discriminative approaches like SimCLR, CLIP, and DINO", "Limited experimental evaluation - only tested on relatively simple datasets (MNIST, FashionMNIST, CIFAR10, Celeb-A) without evaluation on larger-scale or more challenging benchmarks", "The claim that 'generatively learned representations offer the promise of preserving more information' is not sufficiently validated empirically", "Unclear computational cost comparison with discriminative methods - VAE-based approaches may be more expensive to train", "The paper does not provide sufficient ablation studies to understand which components of the proposed framework contribute most to performance gains"], "suggestions": ["Include experiments on larger-scale datasets (e.g., ImageNet) to demonstrate scalability and competitiveness with modern discriminative methods", "Provide quantitative evidence for the claim about information preservation in generative representations, perhaps through mutual information estimation or downstream task performance analysis", "Add computational cost analysis comparing training time and memory requirements with discriminative baselines", "Include more comprehensive ablation studies to isolate the contribution of different theoretical components", "Discuss practical scenarios where the generative approach would be preferred over discriminative methods despite potential performance gaps", "Provide more detailed analysis of failure cases and limitations of the proposed framework"], "summary": "This paper presents a theoretical framework unifying discriminative self-supervised learning methods through a graphical model perspective and proposes a VAE-based approach (SimVAE) that narrows the performance gap with discriminative methods. While the theoretical contributions are valuable and the empirical results show improvements over previous VAE methods, the work still falls short of matching state-of-the-art discriminative approaches. The experimental evaluation is limited to relatively simple datasets, and key claims about information preservation lack sufficient empirical validation. The paper makes solid theoretical contributions but needs stronger empirical validation and broader experimental scope to be truly compelling."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Provides a unifying theoretical framework connecting discriminative self-supervised methods through a graphical model perspective\",\n\"Bridges the gap between discriminative and generative approaches to self-supervised learning, offering theoretical insights into what discriminative methods implicitly learn\",\n\"Demonstrates empirical improvements over previous VAE-based methods on multiple standard benchmarks\",\n\"The theoretical analysis provides principled understanding of self-supervised learning methods that have previously lacked comprehensive theoretical grounding\"\n],\n\"weaknesses\": [\n\"The gap to discriminative methods is only 'narrowed' but not closed - the proposed method still underperforms state-of-the-art discriminative approaches like SimCLR, CLIP, and DINO\",\n\"Limited experimental evaluation - only tested on relatively simple datasets (MNIST, FashionMNIST, CIFAR10, Celeb-A) without evaluation on larger-scale or more challenging benchmarks\",\n\"The claim that 'generatively learned representations offer the promise of preserving more information' is not sufficiently validated empirically\",\n\"Unclear computational cost comparison with discriminative methods - VAE-based approaches may be more expensive to train\",\n\"The paper does not provide sufficient ablation studies to understand which components of the proposed framework contribute most to performance gains\"\n],\n\"suggestions\": [\n\"Include experiments on larger-scale datasets (e.g., ImageNet) to demonstrate scalability and competitiveness with modern discriminative methods\",\n\"Provide quantitative evidence for the claim about information preservation in generative representations, perhaps through mutual information estimation or downstream task performance analysis\",\n\"Add computational cost analysis comparing training time and memory requirements with discriminative baselines\",\n\"Include more comprehensive ablation studies to isolate the contribution of different theoretical components\",\n\"Discuss practical scenarios where the generative approach would be preferred over discriminative methods despite potential performance gaps\",\n\"Provide more detailed analysis of failure cases and limitations of the proposed framework\"\n],\n\"summary\": \"This paper presents a theoretical framework unifying discriminative self-supervised learning methods through a graphical model perspective and proposes a VAE-based approach (SimVAE) that narrows the performance gap with discriminative methods. While the theoretical contributions are valuable and the empirical results show improvements over previous VAE methods, the work still falls short of matching state-of-the-art discriminative approaches. The experimental evaluation is limited to relatively simple datasets, and key claims about information preservation lack sufficient empirical validation. The paper makes solid theoretical contributions but needs stronger empirical validation and broader experimental scope to be truly compelling.\"\n}\n```", "usage": {"prompt_tokens": 381, "completion_tokens": 582, "total_tokens": 963}, "timestamp": "2025-12-29T06:22:06.795193"}
{"paper_id": "eJFBMqCE4X", "model": "google/gemini-2.5-pro", "response": {"score": 4, "strengths": ["The paper addresses the important and well"]}, "raw": "```json\n{\n  \"score\": 4,\n  \"strengths\": [\n    \"The paper addresses the important and well\"\n  ]\n}\n```", "usage": {"completion_tokens": 1441, "prompt_tokens": 372, "total_tokens": 1813}, "timestamp": "2025-12-29T06:22:22.742019"}
{"paper_id": "7qMrDf9zFU", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important and timely problem in instruction tuning - data quality vs quantity trade-off", "Novel use of noise injection as a quality metric for instruction data selection", "Considers both inter-class and intra-class diversity in data selection, which is a comprehensive approach", "Strong empirical results showing 12% of data achieving better performance than full dataset", "Commits to releasing the selected high-quality instruction dataset, which benefits the community"], "weaknesses": ["The abstract lacks clarity on the specific noise injection mechanism and how it quantifies data quality", "No comparison with recent state-of-the-art instruction data selection methods (e.g., IFD, LESS, Alpagasus)", "Missing details on computational cost of the selection process compared to baselines", "Unclear how the method balances quality and diversity - the weighting mechanism is not explained", "Limited discussion on what constitutes 'inter-class' and 'intra-class' diversity in instruction tuning context", "No ablation studies mentioned to validate individual components (noise injection vs diversity strategies)", "Generalizability concerns - unclear if results hold across different model sizes and architectures"], "suggestions": ["Provide mathematical formulation of the noise injection quality metric in the abstract or introduction", "Include comprehensive comparisons with recent methods like IFD, LESS, and Alpagasus", "Add ablation studies separating the contributions of noise injection and diversity strategies", "Clarify the definition of inter-class and intra-class diversity with concrete examples", "Report computational overhead and selection time compared to baseline methods", "Test the approach on multiple model families (e.g., LLaMA, Mistral, Phi) and sizes", "Analyze failure cases where the selected subset underperforms the full dataset", "Provide theoretical justification for why noise injection correlates with instruction quality"], "summary": "This paper proposes a novel instruction data selection method using noise injection to assess quality and combining inter/intra-class diversity. While the core idea is interesting and results are promising (12% data matching full dataset performance), the paper lacks sufficient technical detail, comprehensive baselines, and ablation studies. The noise injection mechanism needs clearer explanation, and comparisons with recent state-of-the-art selection methods are essential. The diversity strategy requires better formalization. With revisions addressing these concerns, this could be a solid contribution to efficient instruction tuning."}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"Addresses an important and timely problem in instruction tuning - data quality vs quantity trade-off\",\n    \"Novel use of noise injection as a quality metric for instruction data selection\",\n    \"Considers both inter-class and intra-class diversity in data selection, which is a comprehensive approach\",\n    \"Strong empirical results showing 12% of data achieving better performance than full dataset\",\n    \"Commits to releasing the selected high-quality instruction dataset, which benefits the community\"\n  ],\n  \"weaknesses\": [\n    \"The abstract lacks clarity on the specific noise injection mechanism and how it quantifies data quality\",\n    \"No comparison with recent state-of-the-art instruction data selection methods (e.g., IFD, LESS, Alpagasus)\",\n    \"Missing details on computational cost of the selection process compared to baselines\",\n    \"Unclear how the method balances quality and diversity - the weighting mechanism is not explained\",\n    \"Limited discussion on what constitutes 'inter-class' and 'intra-class' diversity in instruction tuning context\",\n    \"No ablation studies mentioned to validate individual components (noise injection vs diversity strategies)\",\n    \"Generalizability concerns - unclear if results hold across different model sizes and architectures\"\n  ],\n  \"suggestions\": [\n    \"Provide mathematical formulation of the noise injection quality metric in the abstract or introduction\",\n    \"Include comprehensive comparisons with recent methods like IFD, LESS, and Alpagasus\",\n    \"Add ablation studies separating the contributions of noise injection and diversity strategies\",\n    \"Clarify the definition of inter-class and intra-class diversity with concrete examples\",\n    \"Report computational overhead and selection time compared to baseline methods\",\n    \"Test the approach on multiple model families (e.g., LLaMA, Mistral, Phi) and sizes\",\n    \"Analyze failure cases where the selected subset underperforms the full dataset\",\n    \"Provide theoretical justification for why noise injection correlates with instruction quality\"\n  ],\n  \"summary\": \"This paper proposes a novel instruction data selection method using noise injection to assess quality and combining inter/intra-class diversity. While the core idea is interesting and results are promising (12% data matching full dataset performance), the paper lacks sufficient technical detail, comprehensive baselines, and ablation studies. The noise injection mechanism needs clearer explanation, and comparisons with recent state-of-the-art selection methods are essential. The diversity strategy requires better formalization. With revisions addressing these concerns, this could be a solid contribution to efficient instruction tuning.\"\n}\n```", "usage": {"prompt_tokens": 321, "completion_tokens": 603, "total_tokens": 924}, "timestamp": "2025-12-29T06:22:52.381482"}
{"paper_id": "7qMrDf9zFU", "model": "google/gemini-2.5-pro", "response": {"score": 4, "strengths": ["Addresses the timely and important problem of instruction data"]}, "raw": "```json\n{\n  \"score\": 4,\n  \"strengths\": [\n    \"Addresses the timely and important problem of instruction data\"\n  ]\n}\n```", "usage": {"completion_tokens": 1159, "prompt_tokens": 337, "total_tokens": 1496}, "timestamp": "2025-12-29T06:23:06.050719"}
{"paper_id": "Yk87CwhBDx", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel evaluation paradigm that tests LLMs' ability to reason about visual content from symbolic programs without requiring vision encoders", "Well-motivated benchmark that addresses the challenge of finding tasks LLMs haven't encountered during training", "Procedurally generated benchmark requiring minimal human effort, enabling scalability", "Comprehensive evaluation across multiple commercial and open-source LLMs", "Introduction of Symbolic Instruction Tuning (SIT) that shows improvements both on the target task and general reasoning benchmarks", "Focus on program transformations that preserve semantic meaning while changing syntax is methodologically interesting"], "weaknesses": ["Limited technical details about the benchmark construction process and the specific types of symbolic graphics programs used", "Unclear how the semantic questions are generated and validated for correctness", "Missing analysis of what specific reasoning capabilities are being tested (e.g., spatial reasoning, compositional understanding, geometric transformations)", "No discussion of potential data contamination - symbolic graphics programs may appear in training data", "Insufficient details about the SIT method - what instruction data is collected, how much, and the training procedure", "Lack of error analysis to understand failure modes and what types of reasoning are most challenging", "No comparison with vision-language models that could actually see the rendered output", "Missing ablation studies on key design choices in benchmark construction and SIT"], "suggestions": ["Provide concrete examples of symbolic graphics programs and corresponding questions in the main paper", "Include detailed statistics about benchmark composition (number of programs, question types, difficulty levels)", "Conduct thorough analysis of which reasoning skills correlate with performance", "Add ablation studies on SIT components (data size, instruction format, training duration)", "Compare with vision-language models to establish whether symbolic reasoning provides advantages", "Analyze failure cases to identify specific reasoning gaps in current LLMs", "Discuss potential training data contamination and mitigation strategies", "Provide more details on the invariant transformations and their semantic preservation properties", "Include human performance baselines on the benchmark", "Release code and data to enable reproducibility and future research"], "summary": "This paper introduces an interesting benchmark for evaluating LLMs' ability to understand symbolic graphics programs and reason about their visual output without vision encoders. The core idea is novel and addresses a real need for evaluation tasks less likely to be in training data. The proposed Symbolic Instruction Tuning method shows promise. However, the paper lacks sufficient technical depth in describing the benchmark construction, question generation, and the SIT method. More rigorous analysis of what reasoning capabilities are being tested, failure modes, and comparisons with vision-language baselines would strengthen the work. The claim that this tests 'imagination' needs more careful justification."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel evaluation paradigm that tests LLMs' ability to reason about visual content from symbolic programs without requiring vision encoders\",\n\"Well-motivated benchmark that addresses the challenge of finding tasks LLMs haven't encountered during training\",\n\"Procedurally generated benchmark requiring minimal human effort, enabling scalability\",\n\"Comprehensive evaluation across multiple commercial and open-source LLMs\",\n\"Introduction of Symbolic Instruction Tuning (SIT) that shows improvements both on the target task and general reasoning benchmarks\",\n\"Focus on program transformations that preserve semantic meaning while changing syntax is methodologically interesting\"\n],\n\"weaknesses\": [\n\"Limited technical details about the benchmark construction process and the specific types of symbolic graphics programs used\",\n\"Unclear how the semantic questions are generated and validated for correctness\",\n\"Missing analysis of what specific reasoning capabilities are being tested (e.g., spatial reasoning, compositional understanding, geometric transformations)\",\n\"No discussion of potential data contamination - symbolic graphics programs may appear in training data\",\n\"Insufficient details about the SIT method - what instruction data is collected, how much, and the training procedure\",\n\"Lack of error analysis to understand failure modes and what types of reasoning are most challenging\",\n\"No comparison with vision-language models that could actually see the rendered output\",\n\"Missing ablation studies on key design choices in benchmark construction and SIT\"\n],\n\"suggestions\": [\n\"Provide concrete examples of symbolic graphics programs and corresponding questions in the main paper\",\n\"Include detailed statistics about benchmark composition (number of programs, question types, difficulty levels)\",\n\"Conduct thorough analysis of which reasoning skills correlate with performance\",\n\"Add ablation studies on SIT components (data size, instruction format, training duration)\",\n\"Compare with vision-language models to establish whether symbolic reasoning provides advantages\",\n\"Analyze failure cases to identify specific reasoning gaps in current LLMs\",\n\"Discuss potential training data contamination and mitigation strategies\",\n\"Provide more details on the invariant transformations and their semantic preservation properties\",\n\"Include human performance baselines on the benchmark\",\n\"Release code and data to enable reproducibility and future research\"\n],\n\"summary\": \"This paper introduces an interesting benchmark for evaluating LLMs' ability to understand symbolic graphics programs and reason about their visual output without vision encoders. The core idea is novel and addresses a real need for evaluation tasks less likely to be in training data. The proposed Symbolic Instruction Tuning method shows promise. However, the paper lacks sufficient technical depth in describing the benchmark construction, question generation, and the SIT method. More rigorous analysis of what reasoning capabilities are being tested, failure modes, and comparisons with vision-language baselines would strengthen the work. The claim that this tests 'imagination' needs more careful justification.\"\n}\n```", "usage": {"prompt_tokens": 456, "completion_tokens": 632, "total_tokens": 1088}, "timestamp": "2025-12-29T06:23:45.135137"}
{"paper_id": "Yk87CwhBDx", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["The paper introduces a novel and well-"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"The paper introduces a novel and well-\"\n  ]\n}\n```", "usage": {"completion_tokens": 1128, "prompt_tokens": 446, "total_tokens": 1574}, "timestamp": "2025-12-29T06:23:57.868096"}
{"paper_id": "kSBIEkHzon", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel conceptual framework introducing task-trees as a fundamental unit for learning generalities across graphs, addressing a key challenge in graph foundation models", "Strong theoretical foundation with formal analysis of stability, transferability, and generalization properties of task-trees", "Comprehensive experimental validation across 30 graphs from 5 different domains, demonstrating broad applicability", "Evaluation covers multiple learning paradigms (fine-tuning, in-context learning, zero-shot learning), showing versatility", "The approach shows practical promise by matching or surpassing domain-specific expert models"], "weaknesses": ["The definition and construction of task-trees lacks clarity - the paper needs to better explain what constitutes a task-tree and how they are extracted from arbitrary graphs", "Limited discussion of computational complexity and scalability - pretraining on diverse task-trees could be expensive", "The assumption that generalities are preserved in task-trees is stated but not sufficiently justified before the theoretical analysis", "Comparison with recent graph foundation model baselines (e.g., GraphGPT, OFA) appears limited or missing", "The reconstruction task used for pretraining may be too simple - unclear if it captures sufficient complexity for diverse downstream tasks", "Ablation studies on key design choices (e.g., task-tree size, diversity requirements) are insufficient"], "suggestions": ["Provide concrete examples and visualizations of task-trees extracted from different graph types to improve clarity", "Include detailed complexity analysis and runtime comparisons with baseline methods", "Expand related work section to better position against recent graph foundation model literature", "Add ablation studies examining: (1) impact of task-tree diversity, (2) sensitivity to task-tree construction parameters, (3) alternative pretraining objectives", "Discuss failure cases or domains where the task-tree assumption may not hold", "Provide more implementation details to ensure reproducibility, including hyperparameters and training procedures", "Consider adding experiments on very large-scale graphs to demonstrate true foundation model capabilities"], "summary": "This paper proposes an interesting approach to graph foundation models by introducing task-trees as fundamental learning units that preserve generalities across graphs. The theoretical analysis and multi-domain experiments are valuable contributions. However, the paper suffers from clarity issues in defining task-trees, limited comparison with recent baselines, and insufficient ablation studies. The core assumption needs better justification, and computational considerations require more attention. With revisions addressing these concerns, this could be a solid contribution to the emerging field of graph foundation models."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel conceptual framework introducing task-trees as a fundamental unit for learning generalities across graphs, addressing a key challenge in graph foundation models\",\n\"Strong theoretical foundation with formal analysis of stability, transferability, and generalization properties of task-trees\",\n\"Comprehensive experimental validation across 30 graphs from 5 different domains, demonstrating broad applicability\",\n\"Evaluation covers multiple learning paradigms (fine-tuning, in-context learning, zero-shot learning), showing versatility\",\n\"The approach shows practical promise by matching or surpassing domain-specific expert models\"\n],\n\"weaknesses\": [\n\"The definition and construction of task-trees lacks clarity - the paper needs to better explain what constitutes a task-tree and how they are extracted from arbitrary graphs\",\n\"Limited discussion of computational complexity and scalability - pretraining on diverse task-trees could be expensive\",\n\"The assumption that generalities are preserved in task-trees is stated but not sufficiently justified before the theoretical analysis\",\n\"Comparison with recent graph foundation model baselines (e.g., GraphGPT, OFA) appears limited or missing\",\n\"The reconstruction task used for pretraining may be too simple - unclear if it captures sufficient complexity for diverse downstream tasks\",\n\"Ablation studies on key design choices (e.g., task-tree size, diversity requirements) are insufficient\"\n],\n\"suggestions\": [\n\"Provide concrete examples and visualizations of task-trees extracted from different graph types to improve clarity\",\n\"Include detailed complexity analysis and runtime comparisons with baseline methods\",\n\"Expand related work section to better position against recent graph foundation model literature\",\n\"Add ablation studies examining: (1) impact of task-tree diversity, (2) sensitivity to task-tree construction parameters, (3) alternative pretraining objectives\",\n\"Discuss failure cases or domains where the task-tree assumption may not hold\",\n\"Provide more implementation details to ensure reproducibility, including hyperparameters and training procedures\",\n\"Consider adding experiments on very large-scale graphs to demonstrate true foundation model capabilities\"\n],\n\"summary\": \"This paper proposes an interesting approach to graph foundation models by introducing task-trees as fundamental learning units that preserve generalities across graphs. The theoretical analysis and multi-domain experiments are valuable contributions. However, the paper suffers from clarity issues in defining task-trees, limited comparison with recent baselines, and insufficient ablation studies. The core assumption needs better justification, and computational considerations require more attention. With revisions addressing these concerns, this could be a solid contribution to the emerging field of graph foundation models.\"\n}\n```", "usage": {"prompt_tokens": 423, "completion_tokens": 587, "total_tokens": 1010}, "timestamp": "2025-12-29T06:24:28.603096"}
{"paper_id": "kSBIEkHzon", "model": "google/gemini-2.5-pro", "response": {"score": 7, "strengths": ["Addresses the highly significant and challenging problem of creating graph"]}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses the highly significant and challenging problem of creating graph\"\n  ]\n}\n```", "usage": {"completion_tokens": 1329, "prompt_tokens": 431, "total_tokens": 1760}, "timestamp": "2025-12-29T06:24:43.075026"}
{"paper_id": "aNuQyV30Yw", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses an important and practical problem of learning multiple object-level concepts simultaneously from single images, extending beyond single-concept textual inversion", "Proposes three well-motivated regularization techniques (AttnMask, PromptCL, Bind adj.) that are intuitive and technically sound", "Introduces a new dataset and evaluation protocol specifically designed for multi-concept learning, which could benefit future research", "Comprehensive experimental evaluation including image generation, editing, and attention visualization", "Clear presentation with good visual examples demonstrating the method's capabilities"], "weaknesses": ["Limited technical novelty - the proposed regularization techniques are relatively straightforward extensions of existing ideas (attention masking, contrastive learning, binding)", "Insufficient comparison with recent multi-concept personalization methods (e.g., Custom Diffusion, SVDiff, Mix-of-Show) - only compares against single-concept baselines", "Lack of ablation studies showing the individual contribution of each regularization component", "The paper does not adequately discuss failure cases or limitations of the approach", "Evaluation metrics could be more comprehensive - missing user studies or perceptual quality metrics", "The scalability to more than 2-3 concepts per image is not thoroughly explored", "Missing details on computational costs and training time compared to baselines"], "suggestions": ["Include comprehensive comparisons with recent multi-concept personalization methods published in 2023-2024", "Add detailed ablation studies showing the contribution of each regularization technique (AttnMask, PromptCL, Bind adj.) individually and in combination", "Conduct user studies to evaluate the semantic disentanglement and generation quality more rigorously", "Provide analysis of failure cases and discuss when the method struggles (e.g., overlapping objects, similar concepts)", "Explore scalability by testing with scenes containing 4+ concepts", "Include computational cost analysis (training time, memory requirements) compared to baselines", "Provide more implementation details such as hyperparameter sensitivity analysis", "Discuss potential negative societal impacts and limitations more thoroughly"], "summary": "This paper presents a framework for learning multiple object-level concepts simultaneously from single images using multi-concept prompt learning. While the problem is well-motivated and the proposed regularization techniques are reasonable, the technical contribution is somewhat incremental. The main concerns are limited novelty of the individual components, insufficient comparison with recent multi-concept methods, and lack of thorough ablation studies. The introduction of a new dataset is valuable, but the evaluation could be more comprehensive. The paper would benefit from stronger baselines, more detailed analysis, and user studies to validate the claimed improvements in semantic disentanglement."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses an important and practical problem of learning multiple object-level concepts simultaneously from single images, extending beyond single-concept textual inversion\",\n\"Proposes three well-motivated regularization techniques (AttnMask, PromptCL, Bind adj.) that are intuitive and technically sound\",\n\"Introduces a new dataset and evaluation protocol specifically designed for multi-concept learning, which could benefit future research\",\n\"Comprehensive experimental evaluation including image generation, editing, and attention visualization\",\n\"Clear presentation with good visual examples demonstrating the method's capabilities\"\n],\n\"weaknesses\": [\n\"Limited technical novelty - the proposed regularization techniques are relatively straightforward extensions of existing ideas (attention masking, contrastive learning, binding)\",\n\"Insufficient comparison with recent multi-concept personalization methods (e.g., Custom Diffusion, SVDiff, Mix-of-Show) - only compares against single-concept baselines\",\n\"Lack of ablation studies showing the individual contribution of each regularization component\",\n\"The paper does not adequately discuss failure cases or limitations of the approach\",\n\"Evaluation metrics could be more comprehensive - missing user studies or perceptual quality metrics\",\n\"The scalability to more than 2-3 concepts per image is not thoroughly explored\",\n\"Missing details on computational costs and training time compared to baselines\"\n],\n\"suggestions\": [\n\"Include comprehensive comparisons with recent multi-concept personalization methods published in 2023-2024\",\n\"Add detailed ablation studies showing the contribution of each regularization technique (AttnMask, PromptCL, Bind adj.) individually and in combination\",\n\"Conduct user studies to evaluate the semantic disentanglement and generation quality more rigorously\",\n\"Provide analysis of failure cases and discuss when the method struggles (e.g., overlapping objects, similar concepts)\",\n\"Explore scalability by testing with scenes containing 4+ concepts\",\n\"Include computational cost analysis (training time, memory requirements) compared to baselines\",\n\"Provide more implementation details such as hyperparameter sensitivity analysis\",\n\"Discuss potential negative societal impacts and limitations more thoroughly\"\n],\n\"summary\": \"This paper presents a framework for learning multiple object-level concepts simultaneously from single images using multi-concept prompt learning. While the problem is well-motivated and the proposed regularization techniques are reasonable, the technical contribution is somewhat incremental. The main concerns are limited novelty of the individual components, insufficient comparison with recent multi-concept methods, and lack of thorough ablation studies. The introduction of a new dataset is valuable, but the evaluation could be more comprehensive. The paper would benefit from stronger baselines, more detailed analysis, and user studies to validate the claimed improvements in semantic disentanglement.\"\n}\n```", "usage": {"prompt_tokens": 358, "completion_tokens": 627, "total_tokens": 985}, "timestamp": "2025-12-29T06:25:13.187757"}
{"paper_id": "aNuQyV30Yw", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["The paper addresses a well-motivated and"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"The paper addresses a well-motivated and\"\n  ]\n}\n```", "usage": {"completion_tokens": 1034, "prompt_tokens": 357, "total_tokens": 1391}, "timestamp": "2025-12-29T06:25:26.068081"}
{"paper_id": "pxI5IPeWgW", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel approach to treatment effect estimation using ODEs instead of neural networks, which is a refreshing departure from the dominant paradigm", "Framework is general and can incorporate different ODE discovery methods, making it flexible and extensible", "Handles irregular sampling naturally, which is an important practical consideration in longitudinal studies", "Provides interpretability advantages over black-box neural network approaches", "Addresses an important problem in causal inference with longitudinal data"], "weaknesses": ["The abstract lacks concrete details about the methodology - how exactly are ODEs used for treatment effect inference?", "No mention of experimental validation or empirical results comparing to existing methods", "The claim about 'different set of identification assumptions' is vague - what are these assumptions and how do they compare to existing methods?", "Missing discussion of computational complexity and scalability compared to neural network approaches", "Unclear how the method handles high-dimensional covariates or complex treatment regimes", "No discussion of limitations or failure modes of the ODE-based approach", "The connection between ODE discovery and unbiased treatment effect estimation needs clearer explanation"], "suggestions": ["Provide a concrete example or toy problem illustrating how the ODE framework works for treatment effect estimation", "Include preliminary experimental results or at least a clear experimental design in the abstract", "Explicitly state the identification assumptions required by the ODE approach and compare them to standard assumptions (e.g., unconfoundedness, overlap)", "Discuss computational trade-offs: while ODEs may be interpretable, are they more expensive to fit than neural networks?", "Clarify how the method handles time-varying confounders and treatment-confounder feedback", "Address potential limitations such as model misspecification when the true data-generating process is not well-approximated by an ODE", "Provide more detail on how heterogeneous treatment effects are modeled within the ODE framework"], "summary": "This paper proposes using ordinary differential equations (ODEs) instead of neural networks for inferring longitudinal heterogeneous treatment effects. While the core idea of bringing ODE discovery methods to treatment effect estimation is novel and potentially impactful, the abstract lacks sufficient technical detail and empirical validation. The claimed advantages (interpretability, handling irregular sampling, different identification assumptions) are appealing but need more rigorous justification. The paper would benefit from clearer exposition of the methodology, explicit statement of assumptions, and preliminary experimental evidence demonstrating the approach's effectiveness compared to existing neural network-based methods."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel approach to treatment effect estimation using ODEs instead of neural networks, which is a refreshing departure from the dominant paradigm\",\n\"Framework is general and can incorporate different ODE discovery methods, making it flexible and extensible\",\n\"Handles irregular sampling naturally, which is an important practical consideration in longitudinal studies\",\n\"Provides interpretability advantages over black-box neural network approaches\",\n\"Addresses an important problem in causal inference with longitudinal data\"\n],\n\"weaknesses\": [\n\"The abstract lacks concrete details about the methodology - how exactly are ODEs used for treatment effect inference?\",\n\"No mention of experimental validation or empirical results comparing to existing methods\",\n\"The claim about 'different set of identification assumptions' is vague - what are these assumptions and how do they compare to existing methods?\",\n\"Missing discussion of computational complexity and scalability compared to neural network approaches\",\n\"Unclear how the method handles high-dimensional covariates or complex treatment regimes\",\n\"No discussion of limitations or failure modes of the ODE-based approach\",\n\"The connection between ODE discovery and unbiased treatment effect estimation needs clearer explanation\"\n],\n\"suggestions\": [\n\"Provide a concrete example or toy problem illustrating how the ODE framework works for treatment effect estimation\",\n\"Include preliminary experimental results or at least a clear experimental design in the abstract\",\n\"Explicitly state the identification assumptions required by the ODE approach and compare them to standard assumptions (e.g., unconfoundedness, overlap)\",\n\"Discuss computational trade-offs: while ODEs may be interpretable, are they more expensive to fit than neural networks?\",\n\"Clarify how the method handles time-varying confounders and treatment-confounder feedback\",\n\"Address potential limitations such as model misspecification when the true data-generating process is not well-approximated by an ODE\",\n\"Provide more detail on how heterogeneous treatment effects are modeled within the ODE framework\"\n],\n\"summary\": \"This paper proposes using ordinary differential equations (ODEs) instead of neural networks for inferring longitudinal heterogeneous treatment effects. While the core idea of bringing ODE discovery methods to treatment effect estimation is novel and potentially impactful, the abstract lacks sufficient technical detail and empirical validation. The claimed advantages (interpretability, handling irregular sampling, different identification assumptions) are appealing but need more rigorous justification. The paper would benefit from clearer exposition of the methodology, explicit statement of assumptions, and preliminary experimental evidence demonstrating the approach's effectiveness compared to existing neural network-based methods.\"\n}\n```", "usage": {"prompt_tokens": 326, "completion_tokens": 584, "total_tokens": 910}, "timestamp": "2025-12-29T06:26:00.332480"}
{"paper_id": "pxI5IPeWgW", "model": "google/gemini-2.5-pro", "response": {"score": 7, "strengths": ["The core proposal to use ODE discovery for"]}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"The core proposal to use ODE discovery for\"\n  ]\n}\n```", "usage": {"completion_tokens": 1080, "prompt_tokens": 337, "total_tokens": 1417}, "timestamp": "2025-12-29T06:26:13.825961"}
{"paper_id": "LPfLsSqrQJ", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel approach to few-shot action recognition by decomposing action names into spatio-temporal attributes using large language models", "Well-motivated framework that addresses the limitation of using only coarse category names as context", "Comprehensive experimental validation on four standard FSAR benchmarks (Kinetics, UCF101, HMDB51, SSv2-small) showing state-of-the-art results", "Clear methodology with distinct decomposition and incorporation stages, making the approach interpretable", "Multi-granularity prototype learning (object-level and frame-level) provides fine-grained understanding of actions"], "weaknesses": ["The paper lacks clarity on how exactly the LLM generates spatio-temporal attributes - what prompts are used and how consistent/reliable are these generations", "Computational overhead of using LLMs for knowledge generation is not discussed or analyzed", "Ablation studies could be more comprehensive - missing analysis on the impact of different LLM choices, prompt designs, and number of generated attributes", "The notation 'PKC' appears in abstract but should be 'TKC' (Temporal Knowledge Compensator) based on context - inconsistency in terminology", "Limited discussion on failure cases or scenarios where the decomposed knowledge might not help or could be misleading", "The relationship between spatial and temporal knowledge compensators could be better explained - are they independent or do they interact?"], "suggestions": ["Provide detailed examples of LLM prompts and generated spatio-temporal attributes for different action categories", "Include computational cost analysis comparing DiST with baseline methods", "Add ablation studies on: (1) different LLM choices, (2) varying numbers of generated attributes, (3) quality/diversity of generated knowledge", "Fix the notation inconsistency (PKC vs TKC) throughout the paper", "Include qualitative analysis showing when the method works well vs. fails, particularly for ambiguous actions", "Provide visualization of learned prototypes at different granularities to demonstrate interpretability", "Discuss potential biases or limitations in LLM-generated knowledge and how they might affect performance"], "summary": "This paper presents DiST, a decomposition-incorporation framework for few-shot action recognition that leverages LLM-generated spatio-temporal knowledge to learn multi-granularity prototypes. The approach is well-motivated and shows strong empirical results across multiple benchmarks. However, the paper would benefit from more detailed analysis of the LLM knowledge generation process, computational costs, and more comprehensive ablation studies. The core idea is sound but execution details and analysis need strengthening."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel approach to few-shot action recognition by decomposing action names into spatio-temporal attributes using large language models\",\n\"Well-motivated framework that addresses the limitation of using only coarse category names as context\",\n\"Comprehensive experimental validation on four standard FSAR benchmarks (Kinetics, UCF101, HMDB51, SSv2-small) showing state-of-the-art results\",\n\"Clear methodology with distinct decomposition and incorporation stages, making the approach interpretable\",\n\"Multi-granularity prototype learning (object-level and frame-level) provides fine-grained understanding of actions\"\n],\n\"weaknesses\": [\n\"The paper lacks clarity on how exactly the LLM generates spatio-temporal attributes - what prompts are used and how consistent/reliable are these generations\",\n\"Computational overhead of using LLMs for knowledge generation is not discussed or analyzed\",\n\"Ablation studies could be more comprehensive - missing analysis on the impact of different LLM choices, prompt designs, and number of generated attributes\",\n\"The notation 'PKC' appears in abstract but should be 'TKC' (Temporal Knowledge Compensator) based on context - inconsistency in terminology\",\n\"Limited discussion on failure cases or scenarios where the decomposed knowledge might not help or could be misleading\",\n\"The relationship between spatial and temporal knowledge compensators could be better explained - are they independent or do they interact?\"\n],\n\"suggestions\": [\n\"Provide detailed examples of LLM prompts and generated spatio-temporal attributes for different action categories\",\n\"Include computational cost analysis comparing DiST with baseline methods\",\n\"Add ablation studies on: (1) different LLM choices, (2) varying numbers of generated attributes, (3) quality/diversity of generated knowledge\",\n\"Fix the notation inconsistency (PKC vs TKC) throughout the paper\",\n\"Include qualitative analysis showing when the method works well vs. fails, particularly for ambiguous actions\",\n\"Provide visualization of learned prototypes at different granularities to demonstrate interpretability\",\n\"Discuss potential biases or limitations in LLM-generated knowledge and how they might affect performance\"\n],\n\"summary\": \"This paper presents DiST, a decomposition-incorporation framework for few-shot action recognition that leverages LLM-generated spatio-temporal knowledge to learn multi-granularity prototypes. The approach is well-motivated and shows strong empirical results across multiple benchmarks. However, the paper would benefit from more detailed analysis of the LLM knowledge generation process, computational costs, and more comprehensive ablation studies. The core idea is sound but execution details and analysis need strengthening.\"\n}\n```", "usage": {"prompt_tokens": 499, "completion_tokens": 613, "total_tokens": 1112}, "timestamp": "2025-12-29T06:26:56.633257"}
{"paper_id": "LPfLsSqrQJ", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["The core idea of using LLMs to decompose coarse"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"The core idea of using LLMs to decompose coarse\"\n  ]\n}\n```", "usage": {"completion_tokens": 1083, "prompt_tokens": 472, "total_tokens": 1555}, "timestamp": "2025-12-29T06:27:08.709610"}
{"paper_id": "gHLWTzKiZV", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 7, "strengths": ["Novel theoretical contribution introducing Unbalanced Flow Matching (UFM) that generalizes standard Flow Matching by allowing for mass creation/destruction during transport", "Strong empirical results on PDBBind benchmark, improving energetically favorable poses from 30% to 73% while maintaining competitive docking performance", "Addresses two important limitations in molecular docking: protein flexibility and generation of physically realistic poses", "Clear mathematical framework with well-defined objectives and theoretical grounding", "Comprehensive experimental evaluation including ablation studies and comparison with multiple baselines"], "weaknesses": ["Limited theoretical analysis of when and why UFM provides advantages over standard FM - the trade-off between sample efficiency and approximation accuracy needs more rigorous characterization", "Computational cost comparison with baselines is missing - UFM appears to require additional neural network evaluations which may impact practical applicability", "The choice of unbalanced transport formulation (specific divergence terms, hyperparameters) seems somewhat arbitrary and lacks principled justification", "Evaluation is limited to one primary benchmark (PDBBind) - broader evaluation on diverse protein families and binding scenarios would strengthen claims", "The paper does not adequately discuss failure cases or limitations of the approach", "Protein flexibility modeling is demonstrated but not thoroughly validated against experimental data or MD simulations"], "suggestions": ["Provide theoretical analysis or empirical evidence characterizing when UFM outperforms standard FM (e.g., properties of source/target distributions)", "Include detailed computational cost analysis and wall-clock time comparisons with baselines", "Justify the choice of divergence terms in the unbalanced formulation, possibly through ablation studies or theoretical arguments", "Expand evaluation to additional benchmarks and diverse protein-ligand systems", "Add analysis of failure modes and discuss when the method may not be appropriate", "Validate predicted protein conformational changes against experimental structures or MD simulations", "Provide more implementation details for reproducibility (network architectures, training procedures, hyperparameter selection)", "Discuss the relationship between the unbalanced parameter and physical interpretability of intermediate states"], "summary": "This paper presents Unbalanced Flow Matching (UFM) for flexible molecular docking, extending Flow Matching to allow mass creation/destruction during transport. The method shows promising empirical results on generating energetically favorable poses while modeling protein flexibility. The theoretical contribution is interesting and the experimental results are strong, but the paper would benefit from deeper theoretical analysis, more comprehensive evaluation, and better justification of design choices. The work makes a solid contribution to an important problem, though some aspects need strengthening for publication at a top venue."}, "raw": "```json\n{\n\"score\": 7,\n\"strengths\": [\n\"Novel theoretical contribution introducing Unbalanced Flow Matching (UFM) that generalizes standard Flow Matching by allowing for mass creation/destruction during transport\",\n\"Strong empirical results on PDBBind benchmark, improving energetically favorable poses from 30% to 73% while maintaining competitive docking performance\",\n\"Addresses two important limitations in molecular docking: protein flexibility and generation of physically realistic poses\",\n\"Clear mathematical framework with well-defined objectives and theoretical grounding\",\n\"Comprehensive experimental evaluation including ablation studies and comparison with multiple baselines\"\n],\n\"weaknesses\": [\n\"Limited theoretical analysis of when and why UFM provides advantages over standard FM - the trade-off between sample efficiency and approximation accuracy needs more rigorous characterization\",\n\"Computational cost comparison with baselines is missing - UFM appears to require additional neural network evaluations which may impact practical applicability\",\n\"The choice of unbalanced transport formulation (specific divergence terms, hyperparameters) seems somewhat arbitrary and lacks principled justification\",\n\"Evaluation is limited to one primary benchmark (PDBBind) - broader evaluation on diverse protein families and binding scenarios would strengthen claims\",\n\"The paper does not adequately discuss failure cases or limitations of the approach\",\n\"Protein flexibility modeling is demonstrated but not thoroughly validated against experimental data or MD simulations\"\n],\n\"suggestions\": [\n\"Provide theoretical analysis or empirical evidence characterizing when UFM outperforms standard FM (e.g., properties of source/target distributions)\",\n\"Include detailed computational cost analysis and wall-clock time comparisons with baselines\",\n\"Justify the choice of divergence terms in the unbalanced formulation, possibly through ablation studies or theoretical arguments\",\n\"Expand evaluation to additional benchmarks and diverse protein-ligand systems\",\n\"Add analysis of failure modes and discuss when the method may not be appropriate\",\n\"Validate predicted protein conformational changes against experimental structures or MD simulations\",\n\"Provide more implementation details for reproducibility (network architectures, training procedures, hyperparameter selection)\",\n\"Discuss the relationship between the unbalanced parameter and physical interpretability of intermediate states\"\n],\n\"summary\": \"This paper presents Unbalanced Flow Matching (UFM) for flexible molecular docking, extending Flow Matching to allow mass creation/destruction during transport. The method shows promising empirical results on generating energetically favorable poses while modeling protein flexibility. The theoretical contribution is interesting and the experimental results are strong, but the paper would benefit from deeper theoretical analysis, more comprehensive evaluation, and better justification of design choices. The work makes a solid contribution to an important problem, though some aspects need strengthening for publication at a top venue.\"\n}\n```", "usage": {"prompt_tokens": 271, "completion_tokens": 609, "total_tokens": 880}, "timestamp": "2025-12-29T06:27:43.834277"}
{"paper_id": "gHLWTzKiZV", "model": "google/gemini-2.5-pro", "response": {"score": 7, "strengths": ["Addresses a critical and challenging problem in computational drug"]}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses a critical and challenging problem in computational drug\"\n  ]\n}\n```", "usage": {"completion_tokens": 1161, "prompt_tokens": 271, "total_tokens": 1432}, "timestamp": "2025-12-29T06:27:57.425656"}
{"paper_id": "nzvoDKEvU1", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 4, "strengths": ["Addresses a practical problem of reducing data requirements for personalized hand mesh generation, making the approach more accessible", "Proposes a novel joint pose approximation algorithm and silhouette-based loss function", "Achieves real-time performance for mesh generation", "Clear motivation for reducing reliance on specialized equipment and extensive data"], "weaknesses": ["Limited technical novelty - primarily applies existing DHM model with reduced data rather than fundamental algorithmic innovations", "Insufficient experimental validation - lacks comprehensive comparisons with DHM and other baselines on standard metrics", "Acknowledged artifacts in extreme poses suggest the method may not generalize well beyond the limited training poses", "Missing important details: network architecture, training procedures, quantitative evaluation metrics (e.g., vertex error, chamfer distance)", "No ablation studies to validate the contribution of individual components (joint pose approximation, silhouette loss)", "Unclear how the 24 poses were selected and whether this selection generalizes across different users", "Limited discussion of failure cases and method limitations beyond brief mention of artifacts"], "suggestions": ["Provide comprehensive quantitative comparisons with DHM using full data and other hand mesh methods on standard benchmarks", "Include ablation studies isolating the contributions of the joint pose approximation algorithm and silhouette loss", "Add detailed technical descriptions of the network architecture, training procedure, and hyperparameters", "Evaluate generalization across diverse hand shapes and poses not in the training set", "Provide analysis of which poses are most critical for reconstruction quality", "Include user studies or perceptual evaluations for VR applications", "Discuss computational requirements and compare efficiency gains from reduced data", "Analyze failure modes more systematically and propose potential solutions"], "summary": "This paper proposes reducing data requirements for personalized hand mesh generation by training on only 24 poses instead of 1070. While the practical motivation is clear, the work suffers from limited technical novelty and insufficient experimental validation. The paper lacks quantitative comparisons with baselines, ablation studies, and detailed technical descriptions. The acknowledged artifacts in extreme poses raise concerns about generalization. To be acceptable, the work needs substantial improvements in experimental rigor, technical depth, and demonstration that the reduced data approach achieves comparable quality to methods using more data."}, "raw": "```json\n{\n\"score\": 4,\n\"strengths\": [\n\"Addresses a practical problem of reducing data requirements for personalized hand mesh generation, making the approach more accessible\",\n\"Proposes a novel joint pose approximation algorithm and silhouette-based loss function\",\n\"Achieves real-time performance for mesh generation\",\n\"Clear motivation for reducing reliance on specialized equipment and extensive data\"\n],\n\"weaknesses\": [\n\"Limited technical novelty - primarily applies existing DHM model with reduced data rather than fundamental algorithmic innovations\",\n\"Insufficient experimental validation - lacks comprehensive comparisons with DHM and other baselines on standard metrics\",\n\"Acknowledged artifacts in extreme poses suggest the method may not generalize well beyond the limited training poses\",\n\"Missing important details: network architecture, training procedures, quantitative evaluation metrics (e.g., vertex error, chamfer distance)\",\n\"No ablation studies to validate the contribution of individual components (joint pose approximation, silhouette loss)\",\n\"Unclear how the 24 poses were selected and whether this selection generalizes across different users\",\n\"Limited discussion of failure cases and method limitations beyond brief mention of artifacts\"\n],\n\"suggestions\": [\n\"Provide comprehensive quantitative comparisons with DHM using full data and other hand mesh methods on standard benchmarks\",\n\"Include ablation studies isolating the contributions of the joint pose approximation algorithm and silhouette loss\",\n\"Add detailed technical descriptions of the network architecture, training procedure, and hyperparameters\",\n\"Evaluate generalization across diverse hand shapes and poses not in the training set\",\n\"Provide analysis of which poses are most critical for reconstruction quality\",\n\"Include user studies or perceptual evaluations for VR applications\",\n\"Discuss computational requirements and compare efficiency gains from reduced data\",\n\"Analyze failure modes more systematically and propose potential solutions\"\n],\n\"summary\": \"This paper proposes reducing data requirements for personalized hand mesh generation by training on only 24 poses instead of 1070. While the practical motivation is clear, the work suffers from limited technical novelty and insufficient experimental validation. The paper lacks quantitative comparisons with baselines, ablation studies, and detailed technical descriptions. The acknowledged artifacts in extreme poses raise concerns about generalization. To be acceptable, the work needs substantial improvements in experimental rigor, technical depth, and demonstration that the reduced data approach achieves comparable quality to methods using more data.\"\n}\n```", "usage": {"prompt_tokens": 318, "completion_tokens": 535, "total_tokens": 853}, "timestamp": "2025-12-29T06:28:29.495297"}
{"paper_id": "nzvoDKEvU1", "model": "google/gemini-2.5-pro", "response": {"score": 5, "strengths": ["Addresses the relevant and practical problem of personalized"]}, "raw": "```json\n{\n  \"score\": 5,\n  \"strengths\": [\n    \"Addresses the relevant and practical problem of personalized\"\n  ]\n}\n```", "usage": {"completion_tokens": 1416, "prompt_tokens": 326, "total_tokens": 1742}, "timestamp": "2025-12-29T06:28:45.374761"}
{"paper_id": "LvuSFvGShf", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel investigation into the origin of massive activations, tracing them to intermediate states in FFN modules rather than just hidden states", "Clear identification and definition of 'massive weights' with empirical evidence showing their disproportionate importance", "Simple and practical method (MacDrop) that is plug-and-play and applicable to parameter-efficient fine-tuning", "Comprehensive experiments across multiple models (LLaMA, OPT) and tasks demonstrating consistent improvements", "Good ablation studies examining different curriculum schedules and dropout probabilities"], "weaknesses": ["Limited theoretical justification for why curriculum dropout on massive weights specifically helps - the connection between massive activations causing bias and the proposed solution needs stronger motivation", "The performance improvements from MacDrop are relatively modest (often <1-2% across tasks)", "Lacks comparison with other regularization techniques beyond standard dropout - unclear if the benefit is specific to massive weights or just from better regularization", "The paper doesn't adequately address why massive weights emerge during pre-training or whether they represent a fundamental architectural issue", "Missing analysis on computational overhead and whether the method scales to larger models", "Limited discussion of potential negative effects - are there cases where massive weights are actually beneficial?"], "suggestions": ["Provide theoretical analysis or intuition for why curriculum dropout specifically on massive weights reduces over-reliance better than uniform dropout", "Compare with other regularization methods (L2, weight decay variations, other structured dropout) to isolate the contribution of targeting massive weights", "Analyze what linguistic or semantic patterns the massive weights capture to better understand their role", "Include experiments on larger models (70B+) to demonstrate scalability", "Investigate whether massive weights can be pruned entirely or if they serve essential functions", "Add failure case analysis showing when MacDrop might hurt performance", "Provide more detailed analysis of the curriculum schedule - is the specific schedule critical or do other schedules work similarly?"], "summary": "This paper identifies that massive activations in LLMs originate from specific 'massive weights' in FFN modules and proposes MacDrop, a curriculum dropout method applied to these weights during fine-tuning. While the investigation into massive weights is interesting and the method shows consistent but modest improvements, the paper would benefit from stronger theoretical motivation, more comprehensive comparisons with alternative approaches, and deeper analysis of why this phenomenon occurs and what these weights represent."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel investigation into the origin of massive activations, tracing them to intermediate states in FFN modules rather than just hidden states\",\n\"Clear identification and definition of 'massive weights' with empirical evidence showing their disproportionate importance\",\n\"Simple and practical method (MacDrop) that is plug-and-play and applicable to parameter-efficient fine-tuning\",\n\"Comprehensive experiments across multiple models (LLaMA, OPT) and tasks demonstrating consistent improvements\",\n\"Good ablation studies examining different curriculum schedules and dropout probabilities\"\n],\n\"weaknesses\": [\n\"Limited theoretical justification for why curriculum dropout on massive weights specifically helps - the connection between massive activations causing bias and the proposed solution needs stronger motivation\",\n\"The performance improvements from MacDrop are relatively modest (often <1-2% across tasks)\",\n\"Lacks comparison with other regularization techniques beyond standard dropout - unclear if the benefit is specific to massive weights or just from better regularization\",\n\"The paper doesn't adequately address why massive weights emerge during pre-training or whether they represent a fundamental architectural issue\",\n\"Missing analysis on computational overhead and whether the method scales to larger models\",\n\"Limited discussion of potential negative effects - are there cases where massive weights are actually beneficial?\"\n],\n\"suggestions\": [\n\"Provide theoretical analysis or intuition for why curriculum dropout specifically on massive weights reduces over-reliance better than uniform dropout\",\n\"Compare with other regularization methods (L2, weight decay variations, other structured dropout) to isolate the contribution of targeting massive weights\",\n\"Analyze what linguistic or semantic patterns the massive weights capture to better understand their role\",\n\"Include experiments on larger models (70B+) to demonstrate scalability\",\n\"Investigate whether massive weights can be pruned entirely or if they serve essential functions\",\n\"Add failure case analysis showing when MacDrop might hurt performance\",\n\"Provide more detailed analysis of the curriculum schedule - is the specific schedule critical or do other schedules work similarly?\"\n],\n\"summary\": \"This paper identifies that massive activations in LLMs originate from specific 'massive weights' in FFN modules and proposes MacDrop, a curriculum dropout method applied to these weights during fine-tuning. While the investigation into massive weights is interesting and the method shows consistent but modest improvements, the paper would benefit from stronger theoretical motivation, more comprehensive comparisons with alternative approaches, and deeper analysis of why this phenomenon occurs and what these weights represent.\"\n}\n```", "usage": {"prompt_tokens": 387, "completion_tokens": 554, "total_tokens": 941}, "timestamp": "2025-12-29T06:29:18.189583"}
{"paper_id": "LvuSFvGShf", "model": "google/gemini-2.5-pro", "response": {"score": 7, "strengths": ["The paper presents a compelling and surprising finding:"]}, "raw": "```json\n{\n  \"score\": 7,\n  \"strengths\": [\n    \"The paper presents a compelling and surprising finding:\"\n  ]\n}\n```", "usage": {"completion_tokens": 1511, "prompt_tokens": 396, "total_tokens": 1907}, "timestamp": "2025-12-29T06:29:35.158205"}
{"paper_id": "WRxCuhTMB2", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 5, "strengths": ["Addresses an important and practical problem of evaluating uncertainty disentanglement in ML models, which is crucial for deployment in real-world applications", "Proposes a novel methodology with epistemic variability injection mechanism to evaluate UQ approaches without ground truth", "Evaluates multiple state-of-the-art UQ approaches (4 methods) on both synthetic and real-world datasets", "Provides code and data for reproducibility", "Tackles the challenging problem of assessing uncertainty decomposition quality in the absence of ground truth"], "weaknesses": ["The abstract lacks clarity on what specific indicators are proposed and how they work", "Limited description of the four UQ approaches being compared - unclear which methods are evaluated", "No clear explanation of the 'epistemic variability injection mechanism' - this core contribution needs better description", "Evaluation appears limited to gas demand datasets for real-world experiments, which may limit generalizability claims", "Missing details on how the proposed indicators relate to existing evaluation metrics in the UQ literature", "No discussion of computational costs or scalability of the proposed methodology", "Unclear what constitutes 'effectiveness' of the methodology beyond comparing different UQ approaches"], "suggestions": ["Provide more specific details about the proposed indicators in the abstract and their theoretical justification", "Clearly name and briefly describe the four UQ approaches being benchmarked", "Add a detailed explanation of the epistemic variability injection mechanism with mathematical formulation", "Expand real-world experiments to multiple domains beyond gas demand to demonstrate broader applicability", "Include comparison with existing UQ evaluation metrics and explain advantages of proposed indicators", "Add computational complexity analysis and runtime comparisons", "Provide clearer success criteria for when uncertainty disentanglement is considered 'effective'", "Include ablation studies to validate the necessity of each component in the proposed methodology", "Add statistical significance tests for the comparative results"], "summary": "This paper proposes a methodology to evaluate uncertainty disentanglement in regression models by introducing indicators and an epistemic variability injection mechanism. While the problem is important and the approach of evaluating UQ without ground truth is valuable, the paper lacks sufficient detail in key areas. The abstract does not clearly describe the proposed indicators or the injection mechanism, and the evaluation is limited in scope. The work would benefit from more thorough explanation of the methodology, broader experimental validation, and clearer articulation of what makes the disentanglement 'effective'."}, "raw": "```json\n{\n  \"score\": 5,\n  \"strengths\": [\n    \"Addresses an important and practical problem of evaluating uncertainty disentanglement in ML models, which is crucial for deployment in real-world applications\",\n    \"Proposes a novel methodology with epistemic variability injection mechanism to evaluate UQ approaches without ground truth\",\n    \"Evaluates multiple state-of-the-art UQ approaches (4 methods) on both synthetic and real-world datasets\",\n    \"Provides code and data for reproducibility\",\n    \"Tackles the challenging problem of assessing uncertainty decomposition quality in the absence of ground truth\"\n  ],\n  \"weaknesses\": [\n    \"The abstract lacks clarity on what specific indicators are proposed and how they work\",\n    \"Limited description of the four UQ approaches being compared - unclear which methods are evaluated\",\n    \"No clear explanation of the 'epistemic variability injection mechanism' - this core contribution needs better description\",\n    \"Evaluation appears limited to gas demand datasets for real-world experiments, which may limit generalizability claims\",\n    \"Missing details on how the proposed indicators relate to existing evaluation metrics in the UQ literature\",\n    \"No discussion of computational costs or scalability of the proposed methodology\",\n    \"Unclear what constitutes 'effectiveness' of the methodology beyond comparing different UQ approaches\"\n  ],\n  \"suggestions\": [\n    \"Provide more specific details about the proposed indicators in the abstract and their theoretical justification\",\n    \"Clearly name and briefly describe the four UQ approaches being benchmarked\",\n    \"Add a detailed explanation of the epistemic variability injection mechanism with mathematical formulation\",\n    \"Expand real-world experiments to multiple domains beyond gas demand to demonstrate broader applicability\",\n    \"Include comparison with existing UQ evaluation metrics and explain advantages of proposed indicators\",\n    \"Add computational complexity analysis and runtime comparisons\",\n    \"Provide clearer success criteria for when uncertainty disentanglement is considered 'effective'\",\n    \"Include ablation studies to validate the necessity of each component in the proposed methodology\",\n    \"Add statistical significance tests for the comparative results\"\n  ],\n  \"summary\": \"This paper proposes a methodology to evaluate uncertainty disentanglement in regression models by introducing indicators and an epistemic variability injection mechanism. While the problem is important and the approach of evaluating UQ without ground truth is valuable, the paper lacks sufficient detail in key areas. The abstract does not clearly describe the proposed indicators or the injection mechanism, and the evaluation is limited in scope. The work would benefit from more thorough explanation of the methodology, broader experimental validation, and clearer articulation of what makes the disentanglement 'effective'.\"\n}\n```", "usage": {"prompt_tokens": 385, "completion_tokens": 602, "total_tokens": 987}, "timestamp": "2025-12-29T06:30:17.062009"}
{"paper_id": "WRxCuhTMB2", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["Addresses a critical and often-overlooked problem"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"Addresses a critical and often-overlooked problem\"\n  ]\n}\n```", "usage": {"completion_tokens": 1074, "prompt_tokens": 383, "total_tokens": 1457}, "timestamp": "2025-12-29T06:30:29.684502"}
{"paper_id": "PacBhLzeGO", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel pre-training approach that leverages degradation type classification as weak supervision, which is naturally available in image restoration datasets", "Comprehensive experimental validation across multiple degradation types (10D all-in-one) and mixed degradation scenarios, showing significant improvements (up to 2.55 dB and 6.53 dB respectively)", "The method is applicable to both CNNs and Transformers, demonstrating generalizability across architectures", "Unlike masked image modeling approaches, the decoder (degradation classifier) remains useful after pre-training for transfer learning", "Well-motivated approach that addresses a practical problem in universal image restoration"], "weaknesses": ["The paper lacks sufficient ablation studies on key design choices, such as the choice of decoder architecture (why ResNet18 specifically?), the impact of different degradation classification granularities, and the effect of varying numbers of degradation types", "Limited theoretical justification for why degradation classification should lead to better feature representations for restoration tasks", "The comparison with other pre-training methods appears limited - only masked image modeling is discussed, but other self-supervised approaches (contrastive learning, etc.) are not compared", "Computational cost analysis is missing - no discussion of training time, memory requirements, or efficiency compared to training from scratch or other pre-training methods", "The writing could be improved for clarity, particularly in explaining the two-stage process and how the degradation classifier enables transfer learning", "Missing details on how the method handles unknown or mixed degradation types during inference"], "suggestions": ["Include comprehensive ablation studies examining: (1) different decoder architectures, (2) varying numbers of degradation classes, (3) impact of classification accuracy on downstream performance", "Provide theoretical analysis or intuition for why learning to classify degradations improves restoration features", "Expand comparisons to include other self-supervised pre-training methods beyond masked image modeling", "Add computational cost analysis including training time, memory usage, and FLOPs comparison", "Clarify the transfer learning mechanism - provide concrete examples and possibly visualizations of how the degradation classifier facilitates knowledge transfer", "Discuss limitations and failure cases, particularly for handling unknown degradation types", "Include more implementation details such as hyperparameters, training schedules, and data augmentation strategies"], "summary": "This paper introduces DCPT, a pre-training method for universal image restoration that uses degradation type classification as weak supervision. The approach shows promising empirical results with significant performance gains across multiple benchmarks. However, the paper would benefit from deeper analysis through ablation studies, theoretical justification, broader comparisons with other pre-training methods, and computational cost analysis. The core idea is interesting and practical, but the execution and presentation need strengthening to meet the standards of a top-tier venue."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel pre-training approach that leverages degradation type classification as weak supervision, which is naturally available in image restoration datasets\",\n\"Comprehensive experimental validation across multiple degradation types (10D all-in-one) and mixed degradation scenarios, showing significant improvements (up to 2.55 dB and 6.53 dB respectively)\",\n\"The method is applicable to both CNNs and Transformers, demonstrating generalizability across architectures\",\n\"Unlike masked image modeling approaches, the decoder (degradation classifier) remains useful after pre-training for transfer learning\",\n\"Well-motivated approach that addresses a practical problem in universal image restoration\"\n],\n\"weaknesses\": [\n\"The paper lacks sufficient ablation studies on key design choices, such as the choice of decoder architecture (why ResNet18 specifically?), the impact of different degradation classification granularities, and the effect of varying numbers of degradation types\",\n\"Limited theoretical justification for why degradation classification should lead to better feature representations for restoration tasks\",\n\"The comparison with other pre-training methods appears limited - only masked image modeling is discussed, but other self-supervised approaches (contrastive learning, etc.) are not compared\",\n\"Computational cost analysis is missing - no discussion of training time, memory requirements, or efficiency compared to training from scratch or other pre-training methods\",\n\"The writing could be improved for clarity, particularly in explaining the two-stage process and how the degradation classifier enables transfer learning\",\n\"Missing details on how the method handles unknown or mixed degradation types during inference\"\n],\n\"suggestions\": [\n\"Include comprehensive ablation studies examining: (1) different decoder architectures, (2) varying numbers of degradation classes, (3) impact of classification accuracy on downstream performance\",\n\"Provide theoretical analysis or intuition for why learning to classify degradations improves restoration features\",\n\"Expand comparisons to include other self-supervised pre-training methods beyond masked image modeling\",\n\"Add computational cost analysis including training time, memory usage, and FLOPs comparison\",\n\"Clarify the transfer learning mechanism - provide concrete examples and possibly visualizations of how the degradation classifier facilitates knowledge transfer\",\n\"Discuss limitations and failure cases, particularly for handling unknown degradation types\",\n\"Include more implementation details such as hyperparameters, training schedules, and data augmentation strategies\"\n],\n\"summary\": \"This paper introduces DCPT, a pre-training method for universal image restoration that uses degradation type classification as weak supervision. The approach shows promising empirical results with significant performance gains across multiple benchmarks. However, the paper would benefit from deeper analysis through ablation studies, theoretical justification, broader comparisons with other pre-training methods, and computational cost analysis. The core idea is interesting and practical, but the execution and presentation need strengthening to meet the standards of a top-tier venue.\"\n}\n```", "usage": {"prompt_tokens": 422, "completion_tokens": 647, "total_tokens": 1069}, "timestamp": "2025-12-29T06:30:59.646743"}
{"paper_id": "PacBhLzeGO", "model": "google/gemini-2.5-pro", "response": {"score": 6, "strengths": ["The proposed pre-training task,"]}, "raw": "```json\n{\n  \"score\": 6,\n  \"strengths\": [\n    \"The proposed pre-training task,\"\n  ]\n}\n```", "usage": {"completion_tokens": 1295, "prompt_tokens": 417, "total_tokens": 1712}, "timestamp": "2025-12-29T06:31:13.104940"}
{"paper_id": "QfyZ28FpVY", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Addresses two important and practical challenges in DEL screening: distribution noise and systematic shifts between read counts and binding affinities", "Novel dual-perspective ranking strategy (PSR and LGR) that preserves both local and global relationships in the data", "Comprehensive evaluation on five diverse DEL datasets with state-of-the-art performance across multiple correlation metrics", "Significant contribution through curation and release of three new comprehensive DEL datasets with 2D sequences, 3D structures, and activity labels", "Demonstrates interpretability by identifying key functional groups associated with binding affinity, providing actionable insights for drug discovery", "Strong generalization ability across different protein targets"], "weaknesses": ["Abstract mentions 'five diverse DEL datasets' and 'three new comprehensive datasets' but the relationship and overlap between these is unclear", "Limited technical details in the abstract about the Activity-Referenced Correction (ARC) module and how iterative refinement works", "No comparison with specific baseline methods mentioned in the abstract", "Unclear what 'biological consistency enforcement' means concretely in the ARC module", "Missing information about computational complexity and scalability of the approach", "No discussion of limitations or failure cases", "The novelty of the ranking-based denoising approach compared to existing ranking methods in other domains is not clearly articulated"], "suggestions": ["Clarify the dataset description - specify which datasets are new contributions versus existing benchmarks used for validation", "Provide more technical details about the ARC module, including the iterative refinement process and convergence criteria", "Include specific baseline methods in the abstract for context on performance improvements", "Define 'biological consistency enforcement' more concretely with examples", "Add computational complexity analysis and discuss scalability to larger DEL libraries", "Include ablation studies to demonstrate the contribution of each component (PSR, LGR, ARC)", "Discuss limitations and potential failure modes of the approach", "Provide more details on how the method identifies functional groups and validates these insights", "Compare the ranking-based approach with other denoising methods beyond DEL domain"], "summary": "This paper presents DEL-Ranking, a framework for improving molecular affinity prediction in DNA-encoded libraries through ranking-based denoising and activity-referenced correction. The work addresses important practical challenges and makes valuable dataset contributions. However, the abstract lacks clarity in dataset descriptions, technical details about key components, and baseline comparisons. The approach shows promise with state-of-the-art results and interpretability, but would benefit from more rigorous technical exposition and discussion of limitations."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Addresses two important and practical challenges in DEL screening: distribution noise and systematic shifts between read counts and binding affinities\",\n\"Novel dual-perspective ranking strategy (PSR and LGR) that preserves both local and global relationships in the data\",\n\"Comprehensive evaluation on five diverse DEL datasets with state-of-the-art performance across multiple correlation metrics\",\n\"Significant contribution through curation and release of three new comprehensive DEL datasets with 2D sequences, 3D structures, and activity labels\",\n\"Demonstrates interpretability by identifying key functional groups associated with binding affinity, providing actionable insights for drug discovery\",\n\"Strong generalization ability across different protein targets\"\n],\n\"weaknesses\": [\n\"Abstract mentions 'five diverse DEL datasets' and 'three new comprehensive datasets' but the relationship and overlap between these is unclear\",\n\"Limited technical details in the abstract about the Activity-Referenced Correction (ARC) module and how iterative refinement works\",\n\"No comparison with specific baseline methods mentioned in the abstract\",\n\"Unclear what 'biological consistency enforcement' means concretely in the ARC module\",\n\"Missing information about computational complexity and scalability of the approach\",\n\"No discussion of limitations or failure cases\",\n\"The novelty of the ranking-based denoising approach compared to existing ranking methods in other domains is not clearly articulated\"\n],\n\"suggestions\": [\n\"Clarify the dataset description - specify which datasets are new contributions versus existing benchmarks used for validation\",\n\"Provide more technical details about the ARC module, including the iterative refinement process and convergence criteria\",\n\"Include specific baseline methods in the abstract for context on performance improvements\",\n\"Define 'biological consistency enforcement' more concretely with examples\",\n\"Add computational complexity analysis and discuss scalability to larger DEL libraries\",\n\"Include ablation studies to demonstrate the contribution of each component (PSR, LGR, ARC)\",\n\"Discuss limitations and potential failure modes of the approach\",\n\"Provide more details on how the method identifies functional groups and validates these insights\",\n\"Compare the ranking-based approach with other denoising methods beyond DEL domain\"\n],\n\"summary\": \"This paper presents DEL-Ranking, a framework for improving molecular affinity prediction in DNA-encoded libraries through ranking-based denoising and activity-referenced correction. The work addresses important practical challenges and makes valuable dataset contributions. However, the abstract lacks clarity in dataset descriptions, technical details about key components, and baseline comparisons. The approach shows promise with state-of-the-art results and interpretability, but would benefit from more rigorous technical exposition and discussion of limitations.\"\n}\n```", "usage": {"prompt_tokens": 428, "completion_tokens": 607, "total_tokens": 1035}, "timestamp": "2025-12-29T06:31:47.700429"}
{"paper_id": "QfyZ28FpVY", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Addresses a well-defined and critical problem in DEL"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses a well-defined and critical problem in DEL\"\n  ]\n}\n```", "usage": {"completion_tokens": 1310, "prompt_tokens": 416, "total_tokens": 1726}, "timestamp": "2025-12-29T06:32:01.836839"}
{"paper_id": "9poxbngJzR", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 6, "strengths": ["Novel application of conformal prediction to LLM decision-making with a creative connection to the Monty Hall problem", "CP-OPT framework provides a principled approach to learning score functions that minimize prediction set sizes while maintaining coverage guarantees", "CROQ method cleverly leverages CP prediction sets to improve accuracy by revising questions with reduced answer choices", "Comprehensive experimental evaluation across multiple datasets (MMLU, ToolAlpaca, TruthfulQA) and models (Llama-3, Phi-3)", "Addresses an important problem of overconfidence in LLMs for high-stakes applications"], "weaknesses": ["The Monty Hall analogy, while creative, may be misleading - the Monty Hall problem relies on the host having knowledge of the correct answer, which doesn't directly translate to the CP setting", "Limited theoretical analysis of why CROQ improves accuracy beyond the intuitive argument about fewer choices", "Computational cost of CP-OPT optimization not thoroughly discussed - training score functions may be expensive", "Unclear how the method scales to settings beyond multiple choice questions (e.g., open-ended generation)", "Missing comparison with other uncertainty quantification methods for LLMs beyond basic CP with logits", "The coverage guarantee only ensures the correct answer is in the set with high probability, but CROQ's accuracy improvement depends on the LLM's ability to select from the reduced set"], "suggestions": ["Provide more rigorous theoretical analysis of CROQ's accuracy improvement, potentially with bounds or guarantees", "Include ablation studies on the score function architecture and optimization procedure in CP-OPT", "Compare computational costs (time, memory) of CP-OPT versus baseline methods", "Discuss failure modes and limitations more explicitly - when does CROQ not help or potentially hurt performance?", "Extend experiments to include comparison with other uncertainty quantification methods (e.g., ensemble methods, Bayesian approaches)", "Clarify the Monty Hall analogy or consider removing it if the connection is not sufficiently rigorous", "Provide more details on how calibration data is selected and its impact on performance", "Include experiments on how performance varies with different coverage levels (1-\u03b1)"], "summary": "This paper presents an interesting approach combining optimized conformal prediction with a novel question revision strategy (CROQ) to improve LLM decision-making. The CP-OPT framework for learning score functions and the creative application to accuracy improvement are notable contributions. However, the paper would benefit from stronger theoretical justification for CROQ, more thorough computational analysis, and clearer discussion of the method's limitations. The Monty Hall framing, while attention-grabbing, needs more rigorous connection to the actual method. Overall, this is solid work with practical value, but needs refinement in presentation and analysis."}, "raw": "```json\n{\n\"score\": 6,\n\"strengths\": [\n\"Novel application of conformal prediction to LLM decision-making with a creative connection to the Monty Hall problem\",\n\"CP-OPT framework provides a principled approach to learning score functions that minimize prediction set sizes while maintaining coverage guarantees\",\n\"CROQ method cleverly leverages CP prediction sets to improve accuracy by revising questions with reduced answer choices\",\n\"Comprehensive experimental evaluation across multiple datasets (MMLU, ToolAlpaca, TruthfulQA) and models (Llama-3, Phi-3)\",\n\"Addresses an important problem of overconfidence in LLMs for high-stakes applications\"\n],\n\"weaknesses\": [\n\"The Monty Hall analogy, while creative, may be misleading - the Monty Hall problem relies on the host having knowledge of the correct answer, which doesn't directly translate to the CP setting\",\n\"Limited theoretical analysis of why CROQ improves accuracy beyond the intuitive argument about fewer choices\",\n\"Computational cost of CP-OPT optimization not thoroughly discussed - training score functions may be expensive\",\n\"Unclear how the method scales to settings beyond multiple choice questions (e.g., open-ended generation)\",\n\"Missing comparison with other uncertainty quantification methods for LLMs beyond basic CP with logits\",\n\"The coverage guarantee only ensures the correct answer is in the set with high probability, but CROQ's accuracy improvement depends on the LLM's ability to select from the reduced set\"\n],\n\"suggestions\": [\n\"Provide more rigorous theoretical analysis of CROQ's accuracy improvement, potentially with bounds or guarantees\",\n\"Include ablation studies on the score function architecture and optimization procedure in CP-OPT\",\n\"Compare computational costs (time, memory) of CP-OPT versus baseline methods\",\n\"Discuss failure modes and limitations more explicitly - when does CROQ not help or potentially hurt performance?\",\n\"Extend experiments to include comparison with other uncertainty quantification methods (e.g., ensemble methods, Bayesian approaches)\",\n\"Clarify the Monty Hall analogy or consider removing it if the connection is not sufficiently rigorous\",\n\"Provide more details on how calibration data is selected and its impact on performance\",\n\"Include experiments on how performance varies with different coverage levels (1-\u03b1)\"\n],\n\"summary\": \"This paper presents an interesting approach combining optimized conformal prediction with a novel question revision strategy (CROQ) to improve LLM decision-making. The CP-OPT framework for learning score functions and the creative application to accuracy improvement are notable contributions. However, the paper would benefit from stronger theoretical justification for CROQ, more thorough computational analysis, and clearer discussion of the method's limitations. The Monty Hall framing, while attention-grabbing, needs more rigorous connection to the actual method. Overall, this is solid work with practical value, but needs refinement in presentation and analysis.\"\n}\n```", "usage": {"prompt_tokens": 448, "completion_tokens": 670, "total_tokens": 1118}, "timestamp": "2025-12-29T06:33:02.277214"}
{"paper_id": "9poxbngJzR", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["The paper addresses the critical and timely problem of"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"The paper addresses the critical and timely problem of\"\n  ]\n}\n```", "usage": {"total_tokens": 1771, "prompt_tokens": 435, "completion_tokens": 1336}, "timestamp": "2025-12-29T06:33:15.802095"}
{"paper_id": "cPmLjxedbD", "model": "anthropic/claude-sonnet-4.5", "response": {"score": 2, "strengths": ["Addresses an interesting and underexplored problem domain (chemosensing/olfaction)", "Emphasizes interpretability, which is valuable for scientific understanding", "Proposes analytical approaches that could provide theoretical insights"], "weaknesses": ["Abstract is extremely vague with no concrete technical contributions specified", "No methodology, experimental design, or results presented", "Claims about 'primitive machine intelligence' and 'cognitive development' are poorly defined and unsupported", "The comparison between LLMs and chemosensing appears superficial and unmotivated", "No related work discussed or positioning within existing literature", "The phrase 'creative mathematical models' provides no actual information about the approach", "Missing any validation, datasets, baselines, or evaluation metrics", "The connection between linear mixture models and 'statistical mechanics of learning' is unclear and unexplained", "Title uses clickbait-style phrasing ('LMM not LLM') without justification"], "suggestions": ["Provide concrete mathematical formulations of the proposed linear mixture models", "Include experimental validation on real chemosensing datasets with quantitative results", "Clearly define what 'primitive machine intelligence' means in this context", "Explain the specific advantages of LMMs over deep learning approaches with empirical evidence", "Add comprehensive related work section covering both chemosensing and interpretable ML", "Demonstrate the claimed analytical expressions for limits of chemosensing", "Compare against relevant baselines (both classical and deep learning methods)", "Clarify the connection to statistical mechanics with rigorous theoretical development", "Provide a clear problem formulation and technical approach in the abstract"], "summary": "This submission presents an interesting idea about using interpretable linear mixture models for chemosensing, but lacks the technical depth, rigor, and empirical validation expected for a venue like ICLR. The abstract is too vague, providing no concrete methodology, results, or theoretical contributions. The work needs substantial development including: precise problem formulation, detailed technical approach, theoretical analysis, experimental validation, and proper positioning within existing literature before it can be considered for publication."}, "raw": "```json\n{\n\"score\": 2,\n\"strengths\": [\n\"Addresses an interesting and underexplored problem domain (chemosensing/olfaction)\",\n\"Emphasizes interpretability, which is valuable for scientific understanding\",\n\"Proposes analytical approaches that could provide theoretical insights\"\n],\n\"weaknesses\": [\n\"Abstract is extremely vague with no concrete technical contributions specified\",\n\"No methodology, experimental design, or results presented\",\n\"Claims about 'primitive machine intelligence' and 'cognitive development' are poorly defined and unsupported\",\n\"The comparison between LLMs and chemosensing appears superficial and unmotivated\",\n\"No related work discussed or positioning within existing literature\",\n\"The phrase 'creative mathematical models' provides no actual information about the approach\",\n\"Missing any validation, datasets, baselines, or evaluation metrics\",\n\"The connection between linear mixture models and 'statistical mechanics of learning' is unclear and unexplained\",\n\"Title uses clickbait-style phrasing ('LMM not LLM') without justification\"\n],\n\"suggestions\": [\n\"Provide concrete mathematical formulations of the proposed linear mixture models\",\n\"Include experimental validation on real chemosensing datasets with quantitative results\",\n\"Clearly define what 'primitive machine intelligence' means in this context\",\n\"Explain the specific advantages of LMMs over deep learning approaches with empirical evidence\",\n\"Add comprehensive related work section covering both chemosensing and interpretable ML\",\n\"Demonstrate the claimed analytical expressions for limits of chemosensing\",\n\"Compare against relevant baselines (both classical and deep learning methods)\",\n\"Clarify the connection to statistical mechanics with rigorous theoretical development\",\n\"Provide a clear problem formulation and technical approach in the abstract\"\n],\n\"summary\": \"This submission presents an interesting idea about using interpretable linear mixture models for chemosensing, but lacks the technical depth, rigor, and empirical validation expected for a venue like ICLR. The abstract is too vague, providing no concrete methodology, results, or theoretical contributions. The work needs substantial development including: precise problem formulation, detailed technical approach, theoretical analysis, experimental validation, and proper positioning within existing literature before it can be considered for publication.\"\n}\n```", "usage": {"prompt_tokens": 260, "completion_tokens": 508, "total_tokens": 768}, "timestamp": "2025-12-29T06:33:46.830410"}
{"paper_id": "cPmLjxedbD", "model": "google/gemini-2.5-pro", "response": {"score": 2, "strengths": ["The paper addresses an interesting and important problem: machine"]}, "raw": "```json\n{\n  \"score\": 2,\n  \"strengths\": [\n    \"The paper addresses an interesting and important problem: machine\"\n  ]\n}\n```", "usage": {"total_tokens": 1561, "prompt_tokens": 279, "completion_tokens": 1282}, "timestamp": "2025-12-29T06:34:01.099690"}
{"paper_id": "o9YC0B6P2m", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Proposes a novel and more expressive scaling law that"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Proposes a novel and more expressive scaling law that\"\n  ]\n}\n```", "usage": {"completion_tokens": 972, "total_tokens": 1402, "prompt_tokens": 430}, "timestamp": "2025-12-29T06:36:30.170190"}
{"paper_id": "708lti8yfI", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Addresses a significant problem: overcoming the curse"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses a significant problem: overcoming the curse\"\n  ]\n}\n```", "usage": {"completion_tokens": 1085, "total_tokens": 1410, "prompt_tokens": 325}, "timestamp": "2025-12-29T06:36:56.237864"}
{"paper_id": "X5qi6fnnw7", "model": "google/gemini-2.5-pro", "response": {"error": "No JSON provided to fix. Please provide the invalid JSON.", "example": {"key": "value", "number": 123, "boolean": true, "array": [1, 2, 3], "nestedObject": {"anotherKey": "anotherValue"}}}, "raw": "```json\n{\n  \"error\": \"No JSON provided to fix. Please provide the invalid JSON.\",\n  \"example\": {\n    \"key\": \"value\",\n    \"number\": 123,\n    \"boolean\": true,\n    \"array\": [1, 2, 3],\n    \"nestedObject\": {\n      \"anotherKey\": \"anotherValue\"\n    }\n  }\n}\n```", "usage": {"completion_tokens": 765, "total_tokens": 782, "prompt_tokens": 17}, "timestamp": "2025-12-29T06:37:20.589130"}
{"paper_id": "4dHyH42ha7", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Addresses a novel and highly relevant problem: text"]}, "raw": "```json\n{\n  \"score\": 9,\n  \"strengths\": [\n    \"Addresses a novel and highly relevant problem: text\"\n  ]\n}\n```", "usage": {"completion_tokens": 1082, "total_tokens": 1483, "prompt_tokens": 401}, "timestamp": "2025-12-29T06:37:47.812346"}
{"paper_id": "qnGir4dyu9", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Proposes a novel end-to"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Proposes a novel end-to\"\n  ]\n}\n```", "usage": {"completion_tokens": 1035, "total_tokens": 1520, "prompt_tokens": 485}, "timestamp": "2025-12-29T06:38:15.680579"}
{"paper_id": "wVmShpwtY0", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Addresses a critical limitation of prior methods"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses a critical limitation of prior methods\"\n  ]\n}\n```", "usage": {"completion_tokens": 1183, "total_tokens": 1475, "prompt_tokens": 292}, "timestamp": "2025-12-29T06:38:47.655012"}
{"paper_id": "nS2DBNydCC", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Addresses two critical and well-known problems in vector"]}, "raw": "```json\n{\n  \"score\": 9,\n  \"strengths\": [\n    \"Addresses two critical and well-known problems in vector\"\n  ]\n}\n```", "usage": {"completion_tokens": 1215, "total_tokens": 1491, "prompt_tokens": 276}, "timestamp": "2025-12-29T06:39:13.467675"}
{"paper_id": "DiWRG9JTWZ", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Addresses a well-defined and important problem"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses a well-defined and important problem\"\n  ]\n}\n```", "usage": {"completion_tokens": 1352, "total_tokens": 1809, "prompt_tokens": 457}, "timestamp": "2025-12-29T06:39:39.795093"}
{"paper_id": "ZtTgoomrT1", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Clearly identifies a significant and well-"]}, "raw": "```json\n{\n  \"score\": 9,\n  \"strengths\": [\n    \"Clearly identifies a significant and well-\"\n  ]\n}\n```", "usage": {"completion_tokens": 1454, "total_tokens": 1794, "prompt_tokens": 340}, "timestamp": "2025-12-29T06:40:12.288996"}
{"paper_id": "sOte83GogU", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Addresses a fundamental and necessary component (down"]}, "raw": "```json\n{\n  \"score\": 9,\n  \"strengths\": [\n    \"Addresses a fundamental and necessary component (down\"\n  ]\n}\n```", "usage": {"completion_tokens": 1116, "total_tokens": 1452, "prompt_tokens": 336}, "timestamp": "2025-12-29T06:40:38.759385"}
{"paper_id": "OUkZXbbwQr", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Addresses the critical and timely problem"]}, "raw": "```json\n{\n  \"score\": 9,\n  \"strengths\": [\n    \"Addresses the critical and timely problem\"\n  ]\n}\n```", "usage": {"completion_tokens": 1053, "total_tokens": 1475, "prompt_tokens": 422}, "timestamp": "2025-12-29T06:41:07.216606"}
{"paper_id": "cNi2EJ8OCh", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Addresses a clear and important gap in the literature"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses a clear and important gap in the literature\"\n  ]\n}\n```", "usage": {"completion_tokens": 1329, "total_tokens": 1673, "prompt_tokens": 344}, "timestamp": "2025-12-29T06:41:33.954831"}
{"paper_id": "55oi1LCdDL", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["The paper addresses a significant"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"The paper addresses a significant\"\n  ]\n}\n```", "usage": {"completion_tokens": 1053, "total_tokens": 1396, "prompt_tokens": 343}, "timestamp": "2025-12-29T06:42:02.209332"}
{"paper_id": "WX9cd9iII4", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Addresses a critical and relatively underexplored"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses a critical and relatively underexplored\"\n  ]\n}\n```", "usage": {"completion_tokens": 1319, "total_tokens": 1617, "prompt_tokens": 298}, "timestamp": "2025-12-29T06:42:31.199731"}
{"paper_id": "iOy2pITOoH", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Proposes a novel and integrated approach to"]}, "raw": "```json\n{\n  \"score\": 9,\n  \"strengths\": [\n    \"Proposes a novel and integrated approach to\"\n  ]\n}\n```", "usage": {"completion_tokens": 1178, "total_tokens": 1574, "prompt_tokens": 396}, "timestamp": "2025-12-29T06:42:55.582271"}
{"paper_id": "EXnDAXyVxw", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["The core idea of repurposing quantization-aware"]}, "raw": "```json\n{\n  \"score\": 9,\n  \"strengths\": [\n    \"The core idea of repurposing quantization-aware\"\n  ]\n}\n```", "usage": {"completion_tokens": 1071, "total_tokens": 1417, "prompt_tokens": 346}, "timestamp": "2025-12-29T06:43:22.278744"}
{"paper_id": "aYx7JR20sI", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Introduces a highly novel and powerful theoretical"]}, "raw": "```json\n{\n  \"score\": 9,\n  \"strengths\": [\n    \"Introduces a highly novel and powerful theoretical\"\n  ]\n}\n```", "usage": {"completion_tokens": 1198, "total_tokens": 1552, "prompt_tokens": 354}, "timestamp": "2025-12-29T06:43:48.441235"}
{"paper_id": "xoBPfUyLWj", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Proposes a novel unsupervised approach for time-series"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Proposes a novel unsupervised approach for time-series\"\n  ]\n}\n```", "usage": {"completion_tokens": 1094, "total_tokens": 1398, "prompt_tokens": 304}, "timestamp": "2025-12-29T06:44:12.818806"}
{"paper_id": "0t1O8ziRZp", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Addresses a significant and practical problem in chip design"]}, "raw": "```json\n{\n  \"score\": 9,\n  \"strengths\": [\n    \"Addresses a significant and practical problem in chip design\"\n  ]\n}\n```", "usage": {"completion_tokens": 1495, "total_tokens": 1892, "prompt_tokens": 397}, "timestamp": "2025-12-29T06:44:41.394161"}
{"paper_id": "ukidfml68f", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Identifies a specific and critical technical problem:"]}, "raw": "```json\n{\n  \"score\": 8,\n  \"strengths\": [\n    \"Identifies a specific and critical technical problem:\"\n  ]\n}\n```", "usage": {"completion_tokens": 1515, "total_tokens": 1920, "prompt_tokens": 405}, "timestamp": "2025-12-29T06:45:09.832432"}
{"paper_id": "0uFTqvQhML", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Proposes a novel and counter-intuitive pipeline ('"]}, "raw": "```json\n{\n  \"score\": 9,\n  \"strengths\": [\n    \"Proposes a novel and counter-intuitive pipeline ('\"\n  ]\n}\n```", "usage": {"completion_tokens": 1474, "total_tokens": 1805, "prompt_tokens": 331}, "timestamp": "2025-12-29T06:45:40.461537"}
{"paper_id": "o9YC0B6P2m", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Pro"]}, "raw": "{\n  \"score\": 9,\n  \"strengths\": [\n    \"Pro\"\n  ]\n}", "usage": {"total_tokens": 1472, "prompt_tokens": 418, "completion_tokens": 1054}, "timestamp": "2025-12-29T06:47:16.054207"}
{"paper_id": "708lti8yfI", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": []}, "raw": "{\n  \"score\": 9,\n  \"strengths\": []\n}", "usage": {"total_tokens": 1660, "prompt_tokens": 314, "completion_tokens": 1346}, "timestamp": "2025-12-29T06:47:44.444465"}
{"paper_id": "X5qi6fnnw7", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": [""]}, "raw": "{\n  \"score\": 9,\n  \"strengths\": [\n    \"\"\n  ]\n}", "usage": {"total_tokens": 1687, "prompt_tokens": 334, "completion_tokens": 1353}, "timestamp": "2025-12-29T06:48:10.804924"}
{"paper_id": "4dHyH42ha7", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": []}, "raw": "{\n  \"score\": 9,\n  \"strengths\": []\n}", "usage": {"total_tokens": 1676, "prompt_tokens": 389, "completion_tokens": 1287}, "timestamp": "2025-12-29T06:48:36.590784"}
{"paper_id": "qnGir4dyu9", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Proposes"]}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Proposes\"\n  ]\n}", "usage": {"total_tokens": 1476, "prompt_tokens": 476, "completion_tokens": 1000}, "timestamp": "2025-12-29T06:49:01.191360"}
{"paper_id": "wVmShpwtY0", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": []}, "raw": "{\n  \"score\": 8,\n  \"strengths\": []\n}", "usage": {"total_tokens": 1378, "prompt_tokens": 279, "completion_tokens": 1099}, "timestamp": "2025-12-29T06:49:26.462454"}
{"paper_id": "nS2DBNydCC", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Clearly identifies and addresses two critical issues in vector quantization:"]}, "raw": "{\n  \"score\": 9,\n  \"strengths\": [\n    \"Clearly identifies and addresses two critical issues in vector quantization:\"\n  ]\n}", "usage": {"total_tokens": 1418, "prompt_tokens": 274, "completion_tokens": 1144}, "timestamp": "2025-12-29T06:49:50.179899"}
{"paper_id": "DiWRG9JTWZ", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": []}, "raw": "{\n  \"score\": 9,\n  \"strengths\": []\n}", "usage": {"total_tokens": 1416, "prompt_tokens": 443, "completion_tokens": 973}, "timestamp": "2025-12-29T06:50:16.330151"}
{"paper_id": "ZtTgoomrT1", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Clearly"]}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Clearly\"\n  ]\n}", "usage": {"total_tokens": 1320, "prompt_tokens": 331, "completion_tokens": 989}, "timestamp": "2025-12-29T06:50:39.744736"}
{"paper_id": "sOte83GogU", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Addresses"]}, "raw": "{\n  \"score\": 9,\n  \"strengths\": [\n    \"Addresses\"\n  ]\n}", "usage": {"total_tokens": 1320, "prompt_tokens": 326, "completion_tokens": 994}, "timestamp": "2025-12-29T06:51:06.260330"}
{"paper_id": "OUkZXbbwQr", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": []}, "raw": "{\n  \"score\": 9,\n  \"strengths\": []\n}", "usage": {"total_tokens": 1760, "prompt_tokens": 413, "completion_tokens": 1347}, "timestamp": "2025-12-29T06:51:36.817283"}
{"paper_id": "cNi2EJ8OCh", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Addresses"]}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses\"\n  ]\n}", "usage": {"total_tokens": 1328, "prompt_tokens": 333, "completion_tokens": 995}, "timestamp": "2025-12-29T06:52:02.004113"}
{"paper_id": "55oi1LCdDL", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Addresses"]}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses\"\n  ]\n}", "usage": {"total_tokens": 1330, "prompt_tokens": 336, "completion_tokens": 994}, "timestamp": "2025-12-29T06:52:31.473734"}
{"paper_id": "WX9cd9iII4", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Addresses a critical and timely problem: ensuring fairness in decision-making learned from potentially biased observational"]}, "raw": "{\n  \"score\": 9,\n  \"strengths\": [\n    \"Addresses a critical and timely problem: ensuring fairness in decision-making learned from potentially biased observational\"\n  ]\n}", "usage": {"total_tokens": 1380, "prompt_tokens": 304, "completion_tokens": 1076}, "timestamp": "2025-12-29T06:52:55.500524"}
{"paper_id": "iOy2pITOoH", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": []}, "raw": "{\n  \"score\": 8,\n  \"strengths\": []\n}", "usage": {"total_tokens": 1343, "prompt_tokens": 382, "completion_tokens": 961}, "timestamp": "2025-12-29T06:53:18.307812"}
{"paper_id": "EXnDAXyVxw", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": []}, "raw": "{\n  \"score\": 9,\n  \"strengths\": []\n}", "usage": {"total_tokens": 1482, "prompt_tokens": 331, "completion_tokens": 1151}, "timestamp": "2025-12-29T06:53:42.961649"}
{"paper_id": "aYx7JR20sI", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Proposes"]}, "raw": "{\n  \"score\": 9,\n  \"strengths\": [\n    \"Proposes\"\n  ]\n}", "usage": {"total_tokens": 1354, "prompt_tokens": 345, "completion_tokens": 1009}, "timestamp": "2025-12-29T06:54:06.291520"}
{"paper_id": "xoBPfUyLWj", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Problem-solving"]}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Problem-solving\"\n  ]\n}", "usage": {"total_tokens": 1311, "prompt_tokens": 292, "completion_tokens": 1019}, "timestamp": "2025-12-29T06:54:29.314038"}
{"paper_id": "0t1O8ziRZp", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Addresses"]}, "raw": "{\n  \"score\": 9,\n  \"strengths\": [\n    \"Addresses\"\n  ]\n}", "usage": {"total_tokens": 1380, "prompt_tokens": 385, "completion_tokens": 995}, "timestamp": "2025-12-29T06:54:55.272493"}
{"paper_id": "ukidfml68f", "model": "google/gemini-2.5-pro", "response": {"score": 8, "strengths": ["Ident"]}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Ident\"\n  ]\n}", "usage": {"total_tokens": 1385, "prompt_tokens": 394, "completion_tokens": 991}, "timestamp": "2025-12-29T06:55:19.490204"}
{"paper_id": "0uFTqvQhML", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": []}, "raw": "{\n  \"score\": 9,\n  \"strengths\": []\n}", "usage": {"total_tokens": 1303, "prompt_tokens": 315, "completion_tokens": 988}, "timestamp": "2025-12-29T06:55:44.095346"}
{"paper_id": "GkJCgUmIqA", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": []}, "raw": "{\n  \"score\": 9,\n  \"strengths\": []\n}", "usage": {"total_tokens": 1383, "prompt_tokens": 410, "completion_tokens": 973}, "timestamp": "2025-12-29T06:56:09.186982"}
{"paper_id": "4X9RpKH4Ls", "model": "google/gemini-2.5-pro", "response": {"score": 9, "strengths": ["Addresses"]}, "raw": "{\n  \"score\": 9,\n  \"strengths\": [\n    \"Addresses\"\n  ]\n}", "usage": {"total_tokens": 1385, "prompt_tokens": 391, "completion_tokens": 994}, "timestamp": "2025-12-29T06:56:38.141294"}
{"paper_id": "o9YC0B6P2m", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Novel extension of scaling law to full training curves with LR annealing", "Formulation is general, encompassing varying LRS and arbitrary training steps", "Reduces compute for scaling law formulation by predicting from few curves", "Demonstrates practical value for efficient exploration of LLM training regimes", "Empirical validation is reported to be thorough across architectures and hyperparameters"], "weaknesses": ["Details of empirical validation (datasets, models, numbers) are missing from abstract", "No discussion of limitations or failure cases", "Formulation parameters (L0, A, C, \u03b1) lack theoretical justification\u2014why this form?", "Unclear if method generalizes to non-language domains or other loss functions", "Potential overfitting risk when fitting to single or few curves"], "suggestions": ["Provide more details in the main text on experimental setup and key quantitative results", "Discuss theoretical basis for the chosen law beyond empirical fit", "Analyze failure cases or discuss boundaries of applicability (e.g., other domains)", "Assess sensitivity to initial parameter selection and curve fitting procedure", "Compare with more baseline methods beyond final-step fitting\u2014e.g., time-series models"], "summary": "This paper proposes a new empirical scaling law for neural language models that incorporates learning rate annealing and enables accurate predictions of training curves from minimal data. The work is practically valuable and potentially impactful for efficient large model training. However, further evidence, deeper theoretical grounding, and clear presentation of limitations would strengthen the paper."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Novel extension of scaling law to full training curves with LR annealing\",\n    \"Formulation is general, encompassing varying LRS and arbitrary training steps\",\n    \"Reduces compute for scaling law formulation by predicting from few curves\",\n    \"Demonstrates practical value for efficient exploration of LLM training regimes\",\n    \"Empirical validation is reported to be thorough across architectures and hyperparameters\"\n  ],\n  \"weaknesses\": [\n    \"Details of empirical validation (datasets, models, numbers) are missing from abstract\",\n    \"No discussion of limitations or failure cases\",\n    \"Formulation parameters (L0, A, C, \u03b1) lack theoretical justification\u2014why this form?\",\n    \"Unclear if method generalizes to non-language domains or other loss functions\",\n    \"Potential overfitting risk when fitting to single or few curves\"\n  ],\n  \"suggestions\": [\n    \"Provide more details in the main text on experimental setup and key quantitative results\",\n    \"Discuss theoretical basis for the chosen law beyond empirical fit\",\n    \"Analyze failure cases or discuss boundaries of applicability (e.g., other domains)\",\n    \"Assess sensitivity to initial parameter selection and curve fitting procedure\",\n    \"Compare with more baseline methods beyond final-step fitting\u2014e.g., time-series models\"\n  ],\n  \"summary\": \"This paper proposes a new empirical scaling law for neural language models that incorporates learning rate annealing and enables accurate predictions of training curves from minimal data. The work is practically valuable and potentially impactful for efficient large model training. However, further evidence, deeper theoretical grounding, and clear presentation of limitations would strengthen the paper.\"\n}", "usage": {"prompt_tokens": 405, "completion_tokens": 343, "total_tokens": 748}, "timestamp": "2025-12-29T06:58:00.669622"}
{"paper_id": "708lti8yfI", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Addresses high-dimensional PDEs, a significant challenge in scientific computing.", "Provides explicit Barron norm complexity estimates with dimension dependence.", "Utilizes Green's function representations, building on strong mathematical foundations.", "Extends and generalizes prior work (Chen et al., 2021) to broader classes of PDEs.", "Demonstrates improved theoretical guarantees for Barron space membership."], "weaknesses": ["Limited discussion of practical implications: No empirical validation or numerical experiments.", "Focuses primarily on theoretical results; real-world application impact is not demonstrated.", "Sufficient conditions provided may be restrictive; scope for more general conditions.", "Complexity bounds\u2014while explicit\u2014might still be loose for certain PDE types."], "suggestions": ["Include numerical experiments to illustrate theoretical findings and practical relevance.", "Discuss the tightness of complexity bounds more thoroughly.", "Clarify the extent to which the sufficient conditions are necessary or can be further relaxed.", "Expand discussion to potential limitations or extension to nonlinear equations."], "summary": "The paper makes a strong theoretical contribution to the understanding of solution representations for high-dimensional linear PDEs in Barron space, using Green's function methods to rigorously characterize Barron norms. While the mathematical extensions are significant and address important open questions, further empirical validation and discussion of practical consequences would enhance the impact and accessibility."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses high-dimensional PDEs, a significant challenge in scientific computing.\",\n    \"Provides explicit Barron norm complexity estimates with dimension dependence.\",\n    \"Utilizes Green's function representations, building on strong mathematical foundations.\",\n    \"Extends and generalizes prior work (Chen et al., 2021) to broader classes of PDEs.\",\n    \"Demonstrates improved theoretical guarantees for Barron space membership.\"\n  ],\n  \"weaknesses\": [\n    \"Limited discussion of practical implications: No empirical validation or numerical experiments.\",\n    \"Focuses primarily on theoretical results; real-world application impact is not demonstrated.\",\n    \"Sufficient conditions provided may be restrictive; scope for more general conditions.\",\n    \"Complexity bounds\u2014while explicit\u2014might still be loose for certain PDE types.\"\n  ],\n  \"suggestions\": [\n    \"Include numerical experiments to illustrate theoretical findings and practical relevance.\",\n    \"Discuss the tightness of complexity bounds more thoroughly.\",\n    \"Clarify the extent to which the sufficient conditions are necessary or can be further relaxed.\",\n    \"Expand discussion to potential limitations or extension to nonlinear equations.\"\n  ],\n  \"summary\": \"The paper makes a strong theoretical contribution to the understanding of solution representations for high-dimensional linear PDEs in Barron space, using Green's function methods to rigorously characterize Barron norms. While the mathematical extensions are significant and address important open questions, further empirical validation and discussion of practical consequences would enhance the impact and accessibility.\"\n}", "usage": {"prompt_tokens": 307, "completion_tokens": 309, "total_tokens": 616}, "timestamp": "2025-12-29T06:58:15.761499"}
{"paper_id": "X5qi6fnnw7", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Addresses an important limitation of FB representations in zero-shot RL under realistic data scarcity.", "Clearly demonstrates through experiments how conservatism can mitigate performance degradation on low-diversity datasets.", "The method consistently outperforms vanilla FB and, impressively, can surpass task-specific baselines despite not using reward labels.", "Strong empirical evaluation across multiple domains, datasets, and metrics.", "No observed performance loss in the large data regime, suggesting safety of adoption."], "weaknesses": ["Lack of ablation studies dissecting which aspects of conservatism most drive the benefits.", "Limited theoretical analysis to support why conservatism generalizes well in the FB framework.", "Insufficient discussion of potential weaknesses in specific tasks or negative transfer scenarios.", "The method is heavily benchmarked against FB; more baselines could be considered."], "suggestions": ["Provide a deeper empirical or theoretical analysis of why and when conservatism yields gains in FB.", "Include ablation studies on different conservatism design choices.", "Evaluate against more modern or alternative world model and offline RL baselines, if possible.", "Discuss scenarios where conservatism could introduce undue bias or hinder policy learning.", "Make the limitations and failure cases more prominent in the discussion."], "summary": "This work tackles a practical weakness of forward-backward (FB) representations in zero-shot RL by introducing conservative objectives that maintain strong performance, especially under data scarcity. The approach is well-validated, even surpassing task-specific baselines in some settings. Despite the strong results, the paper would benefit from deeper analyses of the conservative mechanism's effects, richer baselines, and more discussion of possible downsides."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses an important limitation of FB representations in zero-shot RL under realistic data scarcity.\",\n    \"Clearly demonstrates through experiments how conservatism can mitigate performance degradation on low-diversity datasets.\",\n    \"The method consistently outperforms vanilla FB and, impressively, can surpass task-specific baselines despite not using reward labels.\",\n    \"Strong empirical evaluation across multiple domains, datasets, and metrics.\",\n    \"No observed performance loss in the large data regime, suggesting safety of adoption.\"\n  ],\n  \"weaknesses\": [\n    \"Lack of ablation studies dissecting which aspects of conservatism most drive the benefits.\",\n    \"Limited theoretical analysis to support why conservatism generalizes well in the FB framework.\",\n    \"Insufficient discussion of potential weaknesses in specific tasks or negative transfer scenarios.\",\n    \"The method is heavily benchmarked against FB; more baselines could be considered.\"\n  ],\n  \"suggestions\": [\n    \"Provide a deeper empirical or theoretical analysis of why and when conservatism yields gains in FB.\",\n    \"Include ablation studies on different conservatism design choices.\",\n    \"Evaluate against more modern or alternative world model and offline RL baselines, if possible.\",\n    \"Discuss scenarios where conservatism could introduce undue bias or hinder policy learning.\",\n    \"Make the limitations and failure cases more prominent in the discussion.\"\n  ],\n  \"summary\": \"This work tackles a practical weakness of forward-backward (FB) representations in zero-shot RL by introducing conservative objectives that maintain strong performance, especially under data scarcity. The approach is well-validated, even surpassing task-specific baselines in some settings. Despite the strong results, the paper would benefit from deeper analyses of the conservative mechanism's effects, richer baselines, and more discussion of possible downsides.\"\n}", "usage": {"prompt_tokens": 308, "completion_tokens": 377, "total_tokens": 685}, "timestamp": "2025-12-29T06:58:28.222436"}
{"paper_id": "4dHyH42ha7", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Addresses a novel and practical problem: consistent 4D scene editing from monocular videos with text prompts.", "Proposes innovative modules (TPE and SPE) for temporal and spatial consistency.", "Eliminates reliance on external SfM tools by adopting a pose-free 4DGS reconstruction approach.", "Presents strong empirical results, including both quantitative metrics and user studies.", "Progressive, end-to-end framework adaptable for casual, real-world monocular video input."], "weaknesses": ["Details of the TPE and SPE modules are not fully clear (e.g., mechanism and ablations missing).", "Unclear how well the pose-free method generalizes to highly dynamic or complex scenes.", "Potential runtime and scalability issues are not discussed.", "Scope of user studies and dataset diversity are not specified.", "Comparison to the most recent or state-of-the-art methods is not explicit in the abstract."], "suggestions": ["Provide a detailed ablation study to isolate the impact of TPE and SPE modules.", "Discuss limitations on scene complexity and dynamics, especially for the pose-free 4DGS approach.", "Include runtime and resource requirements, comparing with prior art.", "Expand on user study details: number of participants, criteria, statistical significance.", "Add more direct quantitative comparisons to the strongest recent baselines."], "summary": "This paper presents a well-motivated and novel framework for text-driven 4D scene editing from monocular videos, introducing temporal and spatial propagation modules and a pose-free 4DGS approach. The method is empirically validated and appears practically valuable, but would benefit from more granular module analysis, runtime discussion, and comprehensive comparisons. Recommended for acceptance with minor revisions."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses a novel and practical problem: consistent 4D scene editing from monocular videos with text prompts.\",\n    \"Proposes innovative modules (TPE and SPE) for temporal and spatial consistency.\",\n    \"Eliminates reliance on external SfM tools by adopting a pose-free 4DGS reconstruction approach.\",\n    \"Presents strong empirical results, including both quantitative metrics and user studies.\",\n    \"Progressive, end-to-end framework adaptable for casual, real-world monocular video input.\"\n  ],\n  \"weaknesses\": [\n    \"Details of the TPE and SPE modules are not fully clear (e.g., mechanism and ablations missing).\",\n    \"Unclear how well the pose-free method generalizes to highly dynamic or complex scenes.\",\n    \"Potential runtime and scalability issues are not discussed.\",\n    \"Scope of user studies and dataset diversity are not specified.\",\n    \"Comparison to the most recent or state-of-the-art methods is not explicit in the abstract.\"\n  ],\n  \"suggestions\": [\n    \"Provide a detailed ablation study to isolate the impact of TPE and SPE modules.\",\n    \"Discuss limitations on scene complexity and dynamics, especially for the pose-free 4DGS approach.\",\n    \"Include runtime and resource requirements, comparing with prior art.\",\n    \"Expand on user study details: number of participants, criteria, statistical significance.\",\n    \"Add more direct quantitative comparisons to the strongest recent baselines.\"\n  ],\n  \"summary\": \"This paper presents a well-motivated and novel framework for text-driven 4D scene editing from monocular videos, introducing temporal and spatial propagation modules and a pose-free 4DGS approach. The method is empirically validated and appears practically valuable, but would benefit from more granular module analysis, runtime discussion, and comprehensive comparisons. Recommended for acceptance with minor revisions.\"\n}", "usage": {"prompt_tokens": 370, "completion_tokens": 386, "total_tokens": 756}, "timestamp": "2025-12-29T06:58:41.469221"}
{"paper_id": "qnGir4dyu9", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses an important problem in video editing with minimal user intervention.", "Proposes a unified pipeline accommodating multiple editing operations (removal, addition, modification).", "Automated video-to-paragraph system that enables user-friendly interaction and reduces need for manual annotation.", "Utilizes multi-granular spatiotemporal pooling to improve video description richness.", "Demonstrates significant improvements over baselines in both human and quantitative evaluations.", "Framework appears extensible to downstream or third-party generative models."], "weaknesses": ["Details on the robustness and generalization to highly diverse videos are unclear.", "Few specifics on computational cost and efficiency, given the two-stage process.", "Evaluation on editing fidelity (i.e., how \u201cfaithfully\u201d edits match user intent) is not deeply discussed.", "Human evaluation procedure details, such as annotator expertise and inter-rater agreement, are missing.", "Ablation on the contribution of narrative quality versus editing technique is not explored."], "suggestions": ["Provide more quantitative and qualitative analyses on the system\u2019s robustness to varied, complex input videos.", "Include computational efficiency metrics and potential optimizations for deployment.", "Clarify evaluation methods, particularly human study protocols and statistical significance.", "Add ablation studies to disentangle the contributions of narrative quality versus editing pipeline.", "Discuss limitations in terms of generalization, failure cases, and potential user misuse or biases."], "summary": "RACCooN introduces a flexible and user-friendly framework for instructional video editing by combining automatic video-to-paragraph generation with guided generative editing. The approach lowers entry barriers for complex video editing by automating description and leveraging user refinements, outperforming baselines in edit quality and user satisfaction. However, further transparency on generalization, evaluation, efficiency, and ablation would strengthen the manuscript."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses an important problem in video editing with minimal user intervention.\",\n    \"Proposes a unified pipeline accommodating multiple editing operations (removal, addition, modification).\",\n    \"Automated video-to-paragraph system that enables user-friendly interaction and reduces need for manual annotation.\",\n    \"Utilizes multi-granular spatiotemporal pooling to improve video description richness.\",\n    \"Demonstrates significant improvements over baselines in both human and quantitative evaluations.\",\n    \"Framework appears extensible to downstream or third-party generative models.\"\n  ],\n  \"weaknesses\": [\n    \"Details on the robustness and generalization to highly diverse videos are unclear.\",\n    \"Few specifics on computational cost and efficiency, given the two-stage process.\",\n    \"Evaluation on editing fidelity (i.e., how \u201cfaithfully\u201d edits match user intent) is not deeply discussed.\",\n    \"Human evaluation procedure details, such as annotator expertise and inter-rater agreement, are missing.\",\n    \"Ablation on the contribution of narrative quality versus editing technique is not explored.\"\n  ],\n  \"suggestions\": [\n    \"Provide more quantitative and qualitative analyses on the system\u2019s robustness to varied, complex input videos.\",\n    \"Include computational efficiency metrics and potential optimizations for deployment.\",\n    \"Clarify evaluation methods, particularly human study protocols and statistical significance.\",\n    \"Add ablation studies to disentangle the contributions of narrative quality versus editing pipeline.\",\n    \"Discuss limitations in terms of generalization, failure cases, and potential user misuse or biases.\"\n  ],\n  \"summary\": \"RACCooN introduces a flexible and user-friendly framework for instructional video editing by combining automatic video-to-paragraph generation with guided generative editing. The approach lowers entry barriers for complex video editing by automating description and leveraging user refinements, outperforming baselines in edit quality and user satisfaction. However, further transparency on generalization, evaluation, efficiency, and ablation would strengthen the manuscript.\"\n}", "usage": {"prompt_tokens": 458, "completion_tokens": 407, "total_tokens": 865}, "timestamp": "2025-12-29T06:58:55.392800"}
{"paper_id": "wVmShpwtY0", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Addresses epistasis and structural constraints, key challenges in protein design.", "Innovative use of Hamiltonian dynamics for efficient posterior sampling in discrete domain.", "Combines Bayesian optimization with structure-aware modeling, leveraging mutual constraints.", "Employs a novel position discretization to reconcile continuous modeling with discrete sequences.", "Comprehensive experiments show consistent improvement over state-of-the-art baselines.", "Clearly articulates contributions and provides motivation for component choices."], "weaknesses": ["Limited discussion of computational efficiency and scalability on very large proteins.", "Details of the two-stage encoder-decoder framework and its capacity to generalize are not fully elaborated.", "The impact of hyperparameters (e.g., discretization granularity, Hamiltonian steps) is underexplored.", "Absence of ablation studies isolating the contribution of Hamiltonian dynamics vs. other components.", "Lacks demonstration of experimental validation beyond in-silico or simulated benchmarks."], "suggestions": ["Report training and inference times and discuss scalability to larger protein domains.", "Include more analysis on the impact and design of the encoder-decoder framework.", "Provide ablation studies to isolate and quantify the contribution of each architectural component.", "Consider augmenting with real or semi-realistic wet-lab validation if feasible.", "Explore sensitivity analysis for key hyperparameters such as discretization resolution and Hamiltonian trajectory length."], "summary": "This paper introduces HADES, a principled approach for protein sequence optimization that incorporates structural constraints using Hamiltonian dynamics within a Bayesian optimization framework. The method is technically sound and demonstrates clear empirical benefits over prior work. Strengths include innovation, comprehensive evaluation, and strong justification of the approach. Weaknesses center on detailed methodological transparency, scalability claims, and the absence of ablation or real-world validation. Addressing these would further strengthen the contribution."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses epistasis and structural constraints, key challenges in protein design.\",\n    \"Innovative use of Hamiltonian dynamics for efficient posterior sampling in discrete domain.\",\n    \"Combines Bayesian optimization with structure-aware modeling, leveraging mutual constraints.\",\n    \"Employs a novel position discretization to reconcile continuous modeling with discrete sequences.\",\n    \"Comprehensive experiments show consistent improvement over state-of-the-art baselines.\",\n    \"Clearly articulates contributions and provides motivation for component choices.\"\n  ],\n  \"weaknesses\": [\n    \"Limited discussion of computational efficiency and scalability on very large proteins.\",\n    \"Details of the two-stage encoder-decoder framework and its capacity to generalize are not fully elaborated.\",\n    \"The impact of hyperparameters (e.g., discretization granularity, Hamiltonian steps) is underexplored.\",\n    \"Absence of ablation studies isolating the contribution of Hamiltonian dynamics vs. other components.\",\n    \"Lacks demonstration of experimental validation beyond in-silico or simulated benchmarks.\"\n  ],\n  \"suggestions\": [\n    \"Report training and inference times and discuss scalability to larger protein domains.\",\n    \"Include more analysis on the impact and design of the encoder-decoder framework.\",\n    \"Provide ablation studies to isolate and quantify the contribution of each architectural component.\",\n    \"Consider augmenting with real or semi-realistic wet-lab validation if feasible.\",\n    \"Explore sensitivity analysis for key hyperparameters such as discretization resolution and Hamiltonian trajectory length.\"\n  ],\n  \"summary\": \"This paper introduces HADES, a principled approach for protein sequence optimization that incorporates structural constraints using Hamiltonian dynamics within a Bayesian optimization framework. The method is technically sound and demonstrates clear empirical benefits over prior work. Strengths include innovation, comprehensive evaluation, and strong justification of the approach. Weaknesses center on detailed methodological transparency, scalability claims, and the absence of ablation or real-world validation. Addressing these would further strengthen the contribution.\"\n}", "usage": {"prompt_tokens": 272, "completion_tokens": 410, "total_tokens": 682}, "timestamp": "2025-12-29T06:59:07.407493"}
{"paper_id": "nS2DBNydCC", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses a clear and impactful problem (training instability and codebook collapse) in vector quantization for autoregressive models.", "Identifies the mismatch between feature and codebook distributions as a key issue.", "Proposes a principled solution using Wasserstein distance for distribution alignment.", "Reports both empirical and theoretical support for the method.", "Claims significant improvements on codebook utilization and quantization error."], "weaknesses": ["The abstract lacks details on experimental settings and specific quantitative gains.", "It is unclear how computationally expensive the Wasserstein-based alignment is in practice.", "No mention of baselines or comparisons to other stabilization or anti-collapse techniques.", "May not address the impact on downstream tasks\u2014focus remains on quantization metrics.", "The abstract does not discuss limitations or failure cases."], "suggestions": ["Provide detailed comparison against state-of-the-art vector quantization and distribution matching methods.", "Include a discussion of computational overhead and possible trade-offs.", "Clarify downstream effects, e.g., on autoregressive generation quality.", "Add results on codebook utilization across a variety of datasets and tasks.", "Report on any observed limitations or failure scenarios."], "summary": "The paper proposes an innovative approach to vector quantization by aligning the distributions of features and code vectors via Wasserstein distance, claiming to resolve training instability and codebook collapse. While the method seems principled and promising, the current presentation lacks quantitative rigor and comparative context. Further clarification on experiments, baselines, and practical impact is needed for full assessment."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses a clear and impactful problem (training instability and codebook collapse) in vector quantization for autoregressive models.\",\n    \"Identifies the mismatch between feature and codebook distributions as a key issue.\",\n    \"Proposes a principled solution using Wasserstein distance for distribution alignment.\",\n    \"Reports both empirical and theoretical support for the method.\",\n    \"Claims significant improvements on codebook utilization and quantization error.\"\n  ],\n  \"weaknesses\": [\n    \"The abstract lacks details on experimental settings and specific quantitative gains.\",\n    \"It is unclear how computationally expensive the Wasserstein-based alignment is in practice.\",\n    \"No mention of baselines or comparisons to other stabilization or anti-collapse techniques.\",\n    \"May not address the impact on downstream tasks\u2014focus remains on quantization metrics.\",\n    \"The abstract does not discuss limitations or failure cases.\"\n  ],\n  \"suggestions\": [\n    \"Provide detailed comparison against state-of-the-art vector quantization and distribution matching methods.\",\n    \"Include a discussion of computational overhead and possible trade-offs.\",\n    \"Clarify downstream effects, e.g., on autoregressive generation quality.\",\n    \"Add results on codebook utilization across a variety of datasets and tasks.\",\n    \"Report on any observed limitations or failure scenarios.\"\n  ],\n  \"summary\": \"The paper proposes an innovative approach to vector quantization by aligning the distributions of features and code vectors via Wasserstein distance, claiming to resolve training instability and codebook collapse. While the method seems principled and promising, the current presentation lacks quantitative rigor and comparative context. Further clarification on experiments, baselines, and practical impact is needed for full assessment.\"\n}", "usage": {"prompt_tokens": 256, "completion_tokens": 347, "total_tokens": 603}, "timestamp": "2025-12-29T06:59:16.903499"}
{"paper_id": "DiWRG9JTWZ", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses an important, understudied problem (spurious-correlation in few-shot classification)", "Introduces a new real-world benchmark (MetaCoCo) specifically designed for SC-FSC", "Proposes a quantified metric for spurious correlation using CLIP", "Comprehensive experimental evaluation, including state-of-the-art baselines", "Public release of the dataset and code, likely to benefit the community"], "weaknesses": ["Insufficient details about the benchmark construction in the abstract; unclear what modalities/contexts are covered", "The relevance and limitations of the proposed metric using CLIP are not discussed or ablated", "No description of the scale/diversity of MetaCoCo beyond being 'real-world'; risks of limited representativeness", "Novel methods/algorithms are not proposed\u2014sole contribution is dataset and metric", "Lack of deeper insight into failure cases or recommendations for model robustness improvement"], "suggestions": ["Include more details (dataset composition, number of tasks, types of spurious correlations) to clarify the breadth and rigor of MetaCoCo", "Provide analyses of the failures observed by existing methods to guide future research", "Discuss (and if possible, ablate) the choice of CLIP as the metric\u2014does it merely mirror the biases of FSC models?", "Consider proposing baseline robustness methods to stimulate further progress", "Detail benchmark licensing and maintenance commitments to encourage adoption"], "summary": "This paper introduces MetaCoCo, an important new benchmark targeting spurious-correlation shifts in few-shot classification, addressing a clear gap in evaluation tools for this challenging OOD context. The dataset appears comprehensive, and the authors' release of both the resource and a CLIP-based metric is valuable for the community. However, the work would benefit from more explicit details about dataset construction, deeper exploration of metric limitations, and richer discussion of observed method failures. Its main contribution is infrastructural rather than methodological, but it lays a necessary foundation for future advances in robust few-shot learning."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses an important, understudied problem (spurious-correlation in few-shot classification)\",\n    \"Introduces a new real-world benchmark (MetaCoCo) specifically designed for SC-FSC\",\n    \"Proposes a quantified metric for spurious correlation using CLIP\",\n    \"Comprehensive experimental evaluation, including state-of-the-art baselines\",\n    \"Public release of the dataset and code, likely to benefit the community\"\n  ],\n  \"weaknesses\": [\n    \"Insufficient details about the benchmark construction in the abstract; unclear what modalities/contexts are covered\",\n    \"The relevance and limitations of the proposed metric using CLIP are not discussed or ablated\",\n    \"No description of the scale/diversity of MetaCoCo beyond being 'real-world'; risks of limited representativeness\",\n    \"Novel methods/algorithms are not proposed\u2014sole contribution is dataset and metric\",\n    \"Lack of deeper insight into failure cases or recommendations for model robustness improvement\"\n  ],\n  \"suggestions\": [\n    \"Include more details (dataset composition, number of tasks, types of spurious correlations) to clarify the breadth and rigor of MetaCoCo\",\n    \"Provide analyses of the failures observed by existing methods to guide future research\",\n    \"Discuss (and if possible, ablate) the choice of CLIP as the metric\u2014does it merely mirror the biases of FSC models?\",\n    \"Consider proposing baseline robustness methods to stimulate further progress\",\n    \"Detail benchmark licensing and maintenance commitments to encourage adoption\"\n  ],\n  \"summary\": \"This paper introduces MetaCoCo, an important new benchmark targeting spurious-correlation shifts in few-shot classification, addressing a clear gap in evaluation tools for this challenging OOD context. The dataset appears comprehensive, and the authors' release of both the resource and a CLIP-based metric is valuable for the community. However, the work would benefit from more explicit details about dataset construction, deeper exploration of metric limitations, and richer discussion of observed method failures. Its main contribution is infrastructural rather than methodological, but it lays a necessary foundation for future advances in robust few-shot learning.\"\n}", "usage": {"prompt_tokens": 422, "completion_tokens": 445, "total_tokens": 867}, "timestamp": "2025-12-29T06:59:31.535219"}
{"paper_id": "ZtTgoomrT1", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses a known limitation of FNOs in capturing high-frequency features.", "Proposes a theoretically sound modification via translational equivariant attention.", "Experimental evaluation is broad, covering multiple PDE types and challenging multiscale settings.", "Shows consistent performance improvements, especially for highly oscillatory problems."], "weaknesses": ["Insufficient detail in the abstract regarding the implementation of equivariant attention and integration with FNO.", "Lack of quantitative results or specific numbers to support performance claims.", "No discussion of computational costs or potential trade-offs versus baseline FNO.", "It is unclear whether the improvements generalize beyond the chosen benchmarks."], "suggestions": ["Include quantitative results (e.g., accuracy, error metrics, computational cost) in the manuscript or abstract.", "Clarify how equivariant attention is implemented and interacts with the FNO architecture.", "Provide ablation studies or analysis to isolate the impact of the proposed attention mechanism.", "Discuss computational trade-offs and potential limitations in terms of scalability or generalization."], "summary": "The paper tackles an important limitation of Fourier Neural Operators by proposing a novel translational equivariant attention mechanism, resulting in improved performance on challenging PDE problems. While the conceptual contribution is meaningful and experimentally supported across several benchmarks, essential details about implementation and performance metrics are missing. Clarifications and more quantitative evidence would strengthen the work."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses a known limitation of FNOs in capturing high-frequency features.\",\n    \"Proposes a theoretically sound modification via translational equivariant attention.\",\n    \"Experimental evaluation is broad, covering multiple PDE types and challenging multiscale settings.\",\n    \"Shows consistent performance improvements, especially for highly oscillatory problems.\"\n  ],\n  \"weaknesses\": [\n    \"Insufficient detail in the abstract regarding the implementation of equivariant attention and integration with FNO.\",\n    \"Lack of quantitative results or specific numbers to support performance claims.\",\n    \"No discussion of computational costs or potential trade-offs versus baseline FNO.\",\n    \"It is unclear whether the improvements generalize beyond the chosen benchmarks.\"\n  ],\n  \"suggestions\": [\n    \"Include quantitative results (e.g., accuracy, error metrics, computational cost) in the manuscript or abstract.\",\n    \"Clarify how equivariant attention is implemented and interacts with the FNO architecture.\",\n    \"Provide ablation studies or analysis to isolate the impact of the proposed attention mechanism.\",\n    \"Discuss computational trade-offs and potential limitations in terms of scalability or generalization.\"\n  ],\n  \"summary\": \"The paper tackles an important limitation of Fourier Neural Operators by proposing a novel translational equivariant attention mechanism, resulting in improved performance on challenging PDE problems. While the conceptual contribution is meaningful and experimentally supported across several benchmarks, essential details about implementation and performance metrics are missing. Clarifications and more quantitative evidence would strengthen the work.\"\n}", "usage": {"prompt_tokens": 333, "completion_tokens": 310, "total_tokens": 643}, "timestamp": "2025-12-29T06:59:43.682783"}
{"paper_id": "sOte83GogU", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Addresses an important problem in equivariant architectures\u2014downsampling with anti-aliasing on general groups.", "Provides a generalization of classical sampling theory to group equivariant settings, with clear theoretical foundations.", "Novel algorithmic approach for subgroup selection given downsampling rate and finite group.", "Thorough experimental validation on image classification tasks, demonstrating practical impact\u2014improved accuracy, preserved equivariance, and reduced model size.", "When specialized to cyclic groups, method recovers standard signal processing results, supporting generality and correctness."], "weaknesses": ["Details of the anti-aliasing filter design for arbitrary groups may be underspecified or not practical for large groups.", "Computational overhead of proposed methods relative to standard subsampling is not clearly discussed.", "Experiments are described in broad terms; details about datasets, baselines, and ablations are unclear.", "Possible limitations or failure modes (e.g., groups with no 'natural' subgroups at target rates) are not discussed."], "suggestions": ["Provide more concrete algorithmic details and/or pseudocode about anti-aliasing filter construction for general groups.", "Include a discussion of computational complexity and practical runtime overhead in large-scale settings.", "Expand experimental results section: specify datasets, baseline methods, ablation studies, and statistical significance.", "Discuss limitations: what if subgroup selection is ambiguous, or if the group has few suitable subgroups?"], "summary": "This paper addresses the important and underexplored problem of equivariant downsampling with anti-aliasing in group-equivariant neural networks. The authors generalize classical sampling theory to finite groups, present an algorithm for subgroup selection, and propose a bandlimitedness-based approach for anti-aliasing. Experiments suggest improved accuracy and efficiency. However, further algorithmic detail, comprehensive experiments, and discussion of computational and group-theoretic limitations are necessary to solidify the paper\u2019s contributions."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses an important problem in equivariant architectures\u2014downsampling with anti-aliasing on general groups.\",\n    \"Provides a generalization of classical sampling theory to group equivariant settings, with clear theoretical foundations.\",\n    \"Novel algorithmic approach for subgroup selection given downsampling rate and finite group.\",\n    \"Thorough experimental validation on image classification tasks, demonstrating practical impact\u2014improved accuracy, preserved equivariance, and reduced model size.\",\n    \"When specialized to cyclic groups, method recovers standard signal processing results, supporting generality and correctness.\"\n  ],\n  \"weaknesses\": [\n    \"Details of the anti-aliasing filter design for arbitrary groups may be underspecified or not practical for large groups.\",\n    \"Computational overhead of proposed methods relative to standard subsampling is not clearly discussed.\",\n    \"Experiments are described in broad terms; details about datasets, baselines, and ablations are unclear.\",\n    \"Possible limitations or failure modes (e.g., groups with no 'natural' subgroups at target rates) are not discussed.\"\n  ],\n  \"suggestions\": [\n    \"Provide more concrete algorithmic details and/or pseudocode about anti-aliasing filter construction for general groups.\",\n    \"Include a discussion of computational complexity and practical runtime overhead in large-scale settings.\",\n    \"Expand experimental results section: specify datasets, baseline methods, ablation studies, and statistical significance.\",\n    \"Discuss limitations: what if subgroup selection is ambiguous, or if the group has few suitable subgroups?\"\n  ],\n  \"summary\": \"This paper addresses the important and underexplored problem of equivariant downsampling with anti-aliasing in group-equivariant neural networks. The authors generalize classical sampling theory to finite groups, present an algorithm for subgroup selection, and propose a bandlimitedness-based approach for anti-aliasing. Experiments suggest improved accuracy and efficiency. However, further algorithmic detail, comprehensive experiments, and discussion of computational and group-theoretic limitations are necessary to solidify the paper\u2019s contributions.\"\n}", "usage": {"prompt_tokens": 320, "completion_tokens": 423, "total_tokens": 743}, "timestamp": "2025-12-29T06:59:57.271972"}
{"paper_id": "OUkZXbbwQr", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses an important and underexplored aspect of RL: decision justifiability.", "Proposes a novel debate-based reward model with a clear mechanism for incorporating argumentation.", "Demonstrates practical relevance on a real-world healthcare task (sepsis treatment).", "Compares to both typical RL and an 'ideal' judge baseline, illustrating the approach's efficacy.", "Findings on human-aligned justifications and resilience to refutation are well-motivated."], "weaknesses": ["Clarity on the debate protocol, including the construction and limitation of evidence, is insufficient in the summary.", "Degree of generalization to other domains (beyond healthcare) is not discussed.", "Details about the judge proxy and its fidelity to real human judgment are unclear.", "Empirical evaluation appears limited to one task; broader validation is lacking.", "Performance trade-offs (\"hardly sacrificing any performance\") require quantification."], "suggestions": ["Include more details on the debate protocol, such as rounds, structure, and how evidence is determined.", "Clarify judge proxy construction and to what extent results might transfer to real human judges.", "Extend evaluation to at least one more domain to demonstrate generality.", "Provide quantitative measures for performance and justifiability trade-offs.", "Include ablation studies or further analysis on which aspects of the debate are most impactful.", "Discuss potential challenges in scaling the debate mechanism to complex or high-dimensional tasks."], "summary": "The paper introduces a debate-based reward model for reinforcement learning to train agents that can make justifiable, evidence-supported decisions, demonstrated on a healthcare task. The approach is novel and promising, showing improvements in decision justifiability evaluated by a human proxy while maintaining high task performance. However, key methodological details (debate structure, judge proxy) are under-specified, empirical validation is narrow, and generalization is left unaddressed. Improving clarity, quantitative analysis, and domain coverage would further strengthen the work."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses an important and underexplored aspect of RL: decision justifiability.\",\n    \"Proposes a novel debate-based reward model with a clear mechanism for incorporating argumentation.\",\n    \"Demonstrates practical relevance on a real-world healthcare task (sepsis treatment).\",\n    \"Compares to both typical RL and an 'ideal' judge baseline, illustrating the approach's efficacy.\",\n    \"Findings on human-aligned justifications and resilience to refutation are well-motivated.\"\n  ],\n  \"weaknesses\": [\n    \"Clarity on the debate protocol, including the construction and limitation of evidence, is insufficient in the summary.\",\n    \"Degree of generalization to other domains (beyond healthcare) is not discussed.\",\n    \"Details about the judge proxy and its fidelity to real human judgment are unclear.\",\n    \"Empirical evaluation appears limited to one task; broader validation is lacking.\",\n    \"Performance trade-offs (\\\"hardly sacrificing any performance\\\") require quantification.\"\n  ],\n  \"suggestions\": [\n    \"Include more details on the debate protocol, such as rounds, structure, and how evidence is determined.\",\n    \"Clarify judge proxy construction and to what extent results might transfer to real human judges.\",\n    \"Extend evaluation to at least one more domain to demonstrate generality.\",\n    \"Provide quantitative measures for performance and justifiability trade-offs.\",\n    \"Include ablation studies or further analysis on which aspects of the debate are most impactful.\",\n    \"Discuss potential challenges in scaling the debate mechanism to complex or high-dimensional tasks.\"\n  ],\n  \"summary\": \"The paper introduces a debate-based reward model for reinforcement learning to train agents that can make justifiable, evidence-supported decisions, demonstrated on a healthcare task. The approach is novel and promising, showing improvements in decision justifiability evaluated by a human proxy while maintaining high task performance. However, key methodological details (debate structure, judge proxy) are under-specified, empirical validation is narrow, and generalization is left unaddressed. Improving clarity, quantitative analysis, and domain coverage would further strengthen the work.\"\n}", "usage": {"prompt_tokens": 398, "completion_tokens": 438, "total_tokens": 836}, "timestamp": "2025-12-29T07:00:14.563576"}
{"paper_id": "cNi2EJ8OCh", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses an underexplored but important intersection of Local Differential Privacy (LDP) and functional data classification.", "Proposes practical algorithms for both single-server and multi-server (federated) environments.", "Introduces innovative techniques such as 'model reversal' and a strategic sample allocation for model averaging.", "Provides experimental results demonstrating significant improvements under LDP constraints."], "weaknesses": ["Details of theoretical guarantees (e.g., privacy-utility trade-off) are not evident in the abstract.", "Scalability to high-dimensional or complex functional data is not addressed.", "Comparisons to existing state-of-the-art privacy-preserving functional classification methods are unclear.", "Novelty of 'model reversal' and assessment of its generality beyond the scope of this study are not discussed."], "suggestions": ["Expand on theoretical guarantees, including privacy bounds and formal analysis of accuracy trade-offs.", "Include systematic comparisons against recent baselines in both privacy and non-privacy-preserving settings.", "Clarify the model reversal technique and discuss its potential applicability to other domains beyond this paper\u2019s setup.", "Discuss the scalability and computational overhead of the proposed methods."], "summary": "This paper makes a notable contribution by proposing novel approaches to functional classification under local differential privacy constraints, especially by introducing model reversal and strategic model averaging. While the methodological innovations and empirical results are promising, the work would be further strengthened by deeper theoretical analysis, discussion of scalability, and broader experimental comparison."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses an underexplored but important intersection of Local Differential Privacy (LDP) and functional data classification.\",\n    \"Proposes practical algorithms for both single-server and multi-server (federated) environments.\",\n    \"Introduces innovative techniques such as 'model reversal' and a strategic sample allocation for model averaging.\",\n    \"Provides experimental results demonstrating significant improvements under LDP constraints.\"\n  ],\n  \"weaknesses\": [\n    \"Details of theoretical guarantees (e.g., privacy-utility trade-off) are not evident in the abstract.\",\n    \"Scalability to high-dimensional or complex functional data is not addressed.\",\n    \"Comparisons to existing state-of-the-art privacy-preserving functional classification methods are unclear.\",\n    \"Novelty of 'model reversal' and assessment of its generality beyond the scope of this study are not discussed.\"\n  ],\n  \"suggestions\": [\n    \"Expand on theoretical guarantees, including privacy bounds and formal analysis of accuracy trade-offs.\",\n    \"Include systematic comparisons against recent baselines in both privacy and non-privacy-preserving settings.\",\n    \"Clarify the model reversal technique and discuss its potential applicability to other domains beyond this paper\u2019s setup.\",\n    \"Discuss the scalability and computational overhead of the proposed methods.\"\n  ],\n  \"summary\": \"This paper makes a notable contribution by proposing novel approaches to functional classification under local differential privacy constraints, especially by introducing model reversal and strategic model averaging. While the methodological innovations and empirical results are promising, the work would be further strengthened by deeper theoretical analysis, discussion of scalability, and broader experimental comparison.\"\n}", "usage": {"prompt_tokens": 317, "completion_tokens": 332, "total_tokens": 649}, "timestamp": "2025-12-29T07:00:29.500431"}
{"paper_id": "55oi1LCdDL", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Addresses the critical issue of catastrophic forgetting in domain-incremental learning with pre-trained models.", "Proposes a novel dual consolidation technique at both representation and classifier levels.", "The method of merging backbones and consolidating classifiers is clearly motivated.", "Demonstrates strong empirical results on four diverse benchmark datasets, with state-of-the-art performance claims.", "Well-written and easy-to-follow exposition of the proposed framework."], "weaknesses": ["The merging strategy for backbones and classifiers could use more technical specificity\u2014implementation details are sparse.", "Limited discussion of computational costs, scalability, or overhead from dual consolidation.", "Potential class imbalance or semantic drift across domains is not thoroughly addressed.", "Ablation studies or more granular analysis of the contributions of each component (backbone vs. classifier consolidation) could improve interpretability.", "Experiments focus on standard benchmarks rather than more realistic or large-scale domain shifts."], "suggestions": ["Provide more detailed descriptions, perhaps pseudo-code, of the backbone and classifier merging algorithms.", "Include computational resource measurements and an analysis of algorithmic trade-offs.", "Consider evaluating on additional, more complex or real-world domain-incremental settings.", "Enhance the ablation studies with further breakdowns of dual consolidation\u2019s impact.", "Discuss how class imbalances or semantic shifts are handled (or propose extensions to address them)."], "summary": "This paper makes a notable contribution to the domain-incremental learning community with its dual consolidation framework for pretrained models. The representation and classifier consolidation strategies are novel and empirically robust, setting new state-of-the-art results on several benchmarks. However, greater detail on method implementation and analysis of computational and practical scalability would further strengthen the work."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses the critical issue of catastrophic forgetting in domain-incremental learning with pre-trained models.\",\n    \"Proposes a novel dual consolidation technique at both representation and classifier levels.\",\n    \"The method of merging backbones and consolidating classifiers is clearly motivated.\",\n    \"Demonstrates strong empirical results on four diverse benchmark datasets, with state-of-the-art performance claims.\",\n    \"Well-written and easy-to-follow exposition of the proposed framework.\"\n  ],\n  \"weaknesses\": [\n    \"The merging strategy for backbones and classifiers could use more technical specificity\u2014implementation details are sparse.\",\n    \"Limited discussion of computational costs, scalability, or overhead from dual consolidation.\",\n    \"Potential class imbalance or semantic drift across domains is not thoroughly addressed.\",\n    \"Ablation studies or more granular analysis of the contributions of each component (backbone vs. classifier consolidation) could improve interpretability.\",\n    \"Experiments focus on standard benchmarks rather than more realistic or large-scale domain shifts.\"\n  ],\n  \"suggestions\": [\n    \"Provide more detailed descriptions, perhaps pseudo-code, of the backbone and classifier merging algorithms.\",\n    \"Include computational resource measurements and an analysis of algorithmic trade-offs.\",\n    \"Consider evaluating on additional, more complex or real-world domain-incremental settings.\",\n    \"Enhance the ablation studies with further breakdowns of dual consolidation\u2019s impact.\",\n    \"Discuss how class imbalances or semantic shifts are handled (or propose extensions to address them).\"\n  ],\n  \"summary\": \"This paper makes a notable contribution to the domain-incremental learning community with its dual consolidation framework for pretrained models. The representation and classifier consolidation strategies are novel and empirically robust, setting new state-of-the-art results on several benchmarks. However, greater detail on method implementation and analysis of computational and practical scalability would further strengthen the work.\"\n}", "usage": {"prompt_tokens": 318, "completion_tokens": 385, "total_tokens": 703}, "timestamp": "2025-12-29T07:00:42.267063"}
{"paper_id": "WX9cd9iII4", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses an important and under-explored problem: fairness in off-policy learning from observational data.", "Proposes novel formalizations of fairness notions in the off-policy setting.", "Introduces a neural network-based framework applicable to real-world scenarios.", "Provides theoretical generalization bounds, adding rigor to the proposed method.", "Empirical evaluation on both simulated and real-world data is presented."], "weaknesses": ["Details on the fairness formalizations, neural architecture, and training procedures are unclear from the abstract.", "The soundness and tightness of the generalization bounds are not discussed.", "Comparisons to strong baselines and ablations are not described.", "The fairness definitions may rely on strong assumptions about the observational data, which are not specified.", "No discussion of potential limitations or failure cases."], "suggestions": ["Explicitly clarify the fairness definitions and their practical implications.", "Provide more details on the neural network architecture and training workflow.", "Conduct ablation studies and compare with recent state-of-the-art fairness interventions in offline RL.", "Discuss the limitations and robustness of generalization bounds.", "Include analysis on sensitivity to the degree of bias in the observational data."], "summary": "This paper tackles the crucial problem of fair decision rule learning in off-policy settings, a topic with high societal relevance but relatively limited existing work. The framework is novel and supported by theory and empirical evaluation. However, the paper would benefit from more transparency in method details, stronger comparative analysis, and a clearer discussion of assumptions and limitations."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses an important and under-explored problem: fairness in off-policy learning from observational data.\",\n    \"Proposes novel formalizations of fairness notions in the off-policy setting.\",\n    \"Introduces a neural network-based framework applicable to real-world scenarios.\",\n    \"Provides theoretical generalization bounds, adding rigor to the proposed method.\",\n    \"Empirical evaluation on both simulated and real-world data is presented.\"\n  ],\n  \"weaknesses\": [\n    \"Details on the fairness formalizations, neural architecture, and training procedures are unclear from the abstract.\",\n    \"The soundness and tightness of the generalization bounds are not discussed.\",\n    \"Comparisons to strong baselines and ablations are not described.\",\n    \"The fairness definitions may rely on strong assumptions about the observational data, which are not specified.\",\n    \"No discussion of potential limitations or failure cases.\"\n  ],\n  \"suggestions\": [\n    \"Explicitly clarify the fairness definitions and their practical implications.\",\n    \"Provide more details on the neural network architecture and training workflow.\",\n    \"Conduct ablation studies and compare with recent state-of-the-art fairness interventions in offline RL.\",\n    \"Discuss the limitations and robustness of generalization bounds.\",\n    \"Include analysis on sensitivity to the degree of bias in the observational data.\"\n  ],\n  \"summary\": \"This paper tackles the crucial problem of fair decision rule learning in off-policy settings, a topic with high societal relevance but relatively limited existing work. The framework is novel and supported by theory and empirical evaluation. However, the paper would benefit from more transparency in method details, stronger comparative analysis, and a clearer discussion of assumptions and limitations.\"\n}", "usage": {"prompt_tokens": 268, "completion_tokens": 347, "total_tokens": 615}, "timestamp": "2025-12-29T07:00:54.613214"}
{"paper_id": "iOy2pITOoH", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Substantial reduction in computational cost (3.1\u00d7 fewer FLOPs) with similar parameter count and benchmark performance.", "Innovative use of approximate top-k sparsity in both FFN and Attention, tailored for accelerator efficiency.", "Jointly-trained predictor modules avoid complexity and overhead of post-hoc sparsification.", "Demonstrations of impressive end-to-end speedup in realistic settings (CPU prefill/decoding on 16-core VM)."], "weaknesses": ["Limited discussion of potential downsides or edge cases, such as tasks where sparsity harms accuracy.", "Ablation details on the overhead introduced by predictors (parameter allocation and training stability) are missing.", "Evaluation primarily focused on FLOPs and speedup; deeper analysis of generalization and robustness would strengthen the case.", "No reported results on scaling to larger models or different hardware accelerators (e.g., GPUs/TPUs)."], "suggestions": ["Provide ablations on predictor overhead, both in terms of parameters and any impact on training efficiency or convergence.", "Include qualitative or task-specific analysis where induced sparsity may harm performance, and discuss mitigation.", "Report results on a variety of hardware to demonstrate generality of the speedup benefits.", "Expand evaluation to include more diverse datasets and consider out-of-distribution robustness testing."], "summary": "This paper presents Spark Transformer, achieving impressive FLOPs reduction via sparsity in both FFN and Attention, enabled by efficient approximate top-k selection and predictive gating. The joint training of sparse predictors is a clean architectural innovation, avoiding post-hoc controls and excess parameters. The work convincingly demonstrates significant computational savings without sacrificing quality, although further ablations, broader analysis, and more diverse hardware and dataset evaluations would reinforce the impact and generality."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Substantial reduction in computational cost (3.1\u00d7 fewer FLOPs) with similar parameter count and benchmark performance.\",\n    \"Innovative use of approximate top-k sparsity in both FFN and Attention, tailored for accelerator efficiency.\",\n    \"Jointly-trained predictor modules avoid complexity and overhead of post-hoc sparsification.\",\n    \"Demonstrations of impressive end-to-end speedup in realistic settings (CPU prefill/decoding on 16-core VM).\"\n  ],\n  \"weaknesses\": [\n    \"Limited discussion of potential downsides or edge cases, such as tasks where sparsity harms accuracy.\",\n    \"Ablation details on the overhead introduced by predictors (parameter allocation and training stability) are missing.\",\n    \"Evaluation primarily focused on FLOPs and speedup; deeper analysis of generalization and robustness would strengthen the case.\",\n    \"No reported results on scaling to larger models or different hardware accelerators (e.g., GPUs/TPUs).\"\n  ],\n  \"suggestions\": [\n    \"Provide ablations on predictor overhead, both in terms of parameters and any impact on training efficiency or convergence.\",\n    \"Include qualitative or task-specific analysis where induced sparsity may harm performance, and discuss mitigation.\",\n    \"Report results on a variety of hardware to demonstrate generality of the speedup benefits.\",\n    \"Expand evaluation to include more diverse datasets and consider out-of-distribution robustness testing.\"\n  ],\n  \"summary\": \"This paper presents Spark Transformer, achieving impressive FLOPs reduction via sparsity in both FFN and Attention, enabled by efficient approximate top-k selection and predictive gating. The joint training of sparse predictors is a clean architectural innovation, avoiding post-hoc controls and excess parameters. The work convincingly demonstrates significant computational savings without sacrificing quality, although further ablations, broader analysis, and more diverse hardware and dataset evaluations would reinforce the impact and generality.\"\n}", "usage": {"prompt_tokens": 367, "completion_tokens": 395, "total_tokens": 762}, "timestamp": "2025-12-29T07:01:08.331041"}
{"paper_id": "EXnDAXyVxw", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Novel use of quantization as a regularizer for Domain Generalization (DG).", "Well-motivated connection between quantization, noise, and flat minima.", "Strong analytical argument supporting the claimed benefit.", "Comprehensive empirical evaluation across datasets, architectures, and quantization schemes.", "Demonstrates compatibility with other DG methods.", "Results show tangible improvements over state-of-the-art DG approaches."], "weaknesses": ["The ablation isolating quantization from other potential regularization effects is limited.", "Insufficient analysis on the trade-off between quantization levels and model accuracy.", "Limited theoretical depth regarding the precise flatness measure and its connection to generalization.", "Claims on zero computational/memory overhead for ensembles may not always hold, needs further clarification.", "Some comparisons with strong recent DG baselines are missing."], "suggestions": ["Provide more extensive ablation studies to disentangle quantization effects from other potential regularizations.", "Deepen theoretical discussion on the relationship between quantization, flat minima, and domain generalization.", "Clarify ensemble cost claims and provide empirical measurements where possible.", "Include additional recent DG methods in the comparative evaluation.", "Discuss potential limitations or failure cases of QT-DoG."], "summary": "QT-DoG proposes a novel perspective on weight quantization, treating it as a domain generalization regularizer by inducing flat minima. The paper's main message is clear and substantiated with both theoretical and thorough empirical evidence. However, some claims (especially around ensembles and overhead) would benefit from greater clarity, and broader analysis would strengthen the work. Overall, this is a promising and well-executed contribution to domain generalization."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Novel use of quantization as a regularizer for Domain Generalization (DG).\",\n    \"Well-motivated connection between quantization, noise, and flat minima.\",\n    \"Strong analytical argument supporting the claimed benefit.\",\n    \"Comprehensive empirical evaluation across datasets, architectures, and quantization schemes.\",\n    \"Demonstrates compatibility with other DG methods.\",\n    \"Results show tangible improvements over state-of-the-art DG approaches.\"\n  ],\n  \"weaknesses\": [\n    \"The ablation isolating quantization from other potential regularization effects is limited.\",\n    \"Insufficient analysis on the trade-off between quantization levels and model accuracy.\",\n    \"Limited theoretical depth regarding the precise flatness measure and its connection to generalization.\",\n    \"Claims on zero computational/memory overhead for ensembles may not always hold, needs further clarification.\",\n    \"Some comparisons with strong recent DG baselines are missing.\"\n  ],\n  \"suggestions\": [\n    \"Provide more extensive ablation studies to disentangle quantization effects from other potential regularizations.\",\n    \"Deepen theoretical discussion on the relationship between quantization, flat minima, and domain generalization.\",\n    \"Clarify ensemble cost claims and provide empirical measurements where possible.\",\n    \"Include additional recent DG methods in the comparative evaluation.\",\n    \"Discuss potential limitations or failure cases of QT-DoG.\"\n  ],\n  \"summary\": \"QT-DoG proposes a novel perspective on weight quantization, treating it as a domain generalization regularizer by inducing flat minima. The paper's main message is clear and substantiated with both theoretical and thorough empirical evidence. However, some claims (especially around ensembles and overhead) would benefit from greater clarity, and broader analysis would strengthen the work. Overall, this is a promising and well-executed contribution to domain generalization.\"\n}", "usage": {"prompt_tokens": 334, "completion_tokens": 381, "total_tokens": 715}, "timestamp": "2025-12-29T07:01:24.034824"}
{"paper_id": "aYx7JR20sI", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Novel application of tropical geometry to neural network expressivity analysis.", "Theoretical contributions clarify the relationship between linear regions and network architecture.", "Open-source OSCAR library enables symbolic and precise analysis.", "Algorithm for exact computation of linear regions is valuable for the community.", "Empirical demonstrations substantiate the practical viability and breadth of the approach."], "weaknesses": ["Scope of empirical evaluation is limited to proof-of-concept, lacking benchmarks on large-scale or real-world models.", "The practical impact of improved sampling domain selection remains speculative.", "Accessibility and adoption might be hindered by the mathematical complexity.", "Comparisons with prior approximation or empirical approaches to counting linear regions are not discussed in depth."], "suggestions": ["Expand numerical experiments to include larger and more practically relevant neural network architectures.", "Discuss computational efficiency and scalability of the proposed algorithms versus existing methods.", "Provide concrete case studies showcasing the practical utility of the OSCAR library for network design and analysis.", "Clarify the potential and limitations of tropical geometry methods for general classes of activation functions beyond piecewise linear."], "summary": "This paper presents a rigorous and original framework connecting tropical geometry and the study of neural network expressivity, with valuable theoretical results and an open-source tool for symbolic analysis. While conceptually strong and well-motivated, empirical scope and discussion of practical implications could be improved. The work lays a solid foundation for future research at the intersection of algebraic geometry and deep learning."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Novel application of tropical geometry to neural network expressivity analysis.\",\n    \"Theoretical contributions clarify the relationship between linear regions and network architecture.\",\n    \"Open-source OSCAR library enables symbolic and precise analysis.\",\n    \"Algorithm for exact computation of linear regions is valuable for the community.\",\n    \"Empirical demonstrations substantiate the practical viability and breadth of the approach.\"\n  ],\n  \"weaknesses\": [\n    \"Scope of empirical evaluation is limited to proof-of-concept, lacking benchmarks on large-scale or real-world models.\",\n    \"The practical impact of improved sampling domain selection remains speculative.\",\n    \"Accessibility and adoption might be hindered by the mathematical complexity.\",\n    \"Comparisons with prior approximation or empirical approaches to counting linear regions are not discussed in depth.\"\n  ],\n  \"suggestions\": [\n    \"Expand numerical experiments to include larger and more practically relevant neural network architectures.\",\n    \"Discuss computational efficiency and scalability of the proposed algorithms versus existing methods.\",\n    \"Provide concrete case studies showcasing the practical utility of the OSCAR library for network design and analysis.\",\n    \"Clarify the potential and limitations of tropical geometry methods for general classes of activation functions beyond piecewise linear.\"\n  ],\n  \"summary\": \"This paper presents a rigorous and original framework connecting tropical geometry and the study of neural network expressivity, with valuable theoretical results and an open-source tool for symbolic analysis. While conceptually strong and well-motivated, empirical scope and discussion of practical implications could be improved. The work lays a solid foundation for future research at the intersection of algebraic geometry and deep learning.\"\n}", "usage": {"prompt_tokens": 340, "completion_tokens": 333, "total_tokens": 673}, "timestamp": "2025-12-29T07:01:38.208801"}
{"paper_id": "xoBPfUyLWj", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Novel use of diffusion models for time series OoD detection.", "Unsupervised approach removes dependence on labeled anomaly data.", "Integration of heterogeneous side information is well-motivated.", "Evaluation on diverse datasets (IoT, DDoS, transactions) demonstrates versatility.", "Empirical results suggest state-of-the-art performance."], "weaknesses": ["Methodological details (e.g., side information incorporation, exact model architecture) are lacking in the abstract.", "Imputation-based reconstruction error may struggle with subtle anomalies.", "No comparison against deep generative baselines (e.g., VAE, GAN) is mentioned.", "Scalability or computational overhead of diffusion models for long time series is not discussed.", "Limited insight on interpretability or real-world deployment considerations."], "suggestions": ["Clarify how side information is encoded and fused with time series data in the diffusion framework.", "Provide thorough comparisons to relevant OoD detection baselines, including deep generative models.", "Discuss computational cost and efficiency, especially for real-time settings.", "Include ablation studies to analyze the impact of side information.", "Consider reporting on sample efficiency and robustness to noise."], "summary": "This work proposes an unsupervised time series anomaly/OoD detection method leveraging diffusion models and domain-specific side information. The approach is novel and empirically strong, but the abstract lacks essential methodological specifics, baseline comparisons, and practical insights. Strengthening these areas would significantly improve clarity and impact."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Novel use of diffusion models for time series OoD detection.\",\n    \"Unsupervised approach removes dependence on labeled anomaly data.\",\n    \"Integration of heterogeneous side information is well-motivated.\",\n    \"Evaluation on diverse datasets (IoT, DDoS, transactions) demonstrates versatility.\",\n    \"Empirical results suggest state-of-the-art performance.\"\n  ],\n  \"weaknesses\": [\n    \"Methodological details (e.g., side information incorporation, exact model architecture) are lacking in the abstract.\",\n    \"Imputation-based reconstruction error may struggle with subtle anomalies.\",\n    \"No comparison against deep generative baselines (e.g., VAE, GAN) is mentioned.\",\n    \"Scalability or computational overhead of diffusion models for long time series is not discussed.\",\n    \"Limited insight on interpretability or real-world deployment considerations.\"\n  ],\n  \"suggestions\": [\n    \"Clarify how side information is encoded and fused with time series data in the diffusion framework.\",\n    \"Provide thorough comparisons to relevant OoD detection baselines, including deep generative models.\",\n    \"Discuss computational cost and efficiency, especially for real-time settings.\",\n    \"Include ablation studies to analyze the impact of side information.\",\n    \"Consider reporting on sample efficiency and robustness to noise.\"\n  ],\n  \"summary\": \"This work proposes an unsupervised time series anomaly/OoD detection method leveraging diffusion models and domain-specific side information. The approach is novel and empirically strong, but the abstract lacks essential methodological specifics, baseline comparisons, and practical insights. Strengthening these areas would significantly improve clarity and impact.\"\n}", "usage": {"prompt_tokens": 278, "completion_tokens": 339, "total_tokens": 617}, "timestamp": "2025-12-29T07:01:53.358880"}
{"paper_id": "0t1O8ziRZp", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses an important problem in logic synthesis with direct implications for chip design efficiency.", "Introduces a principled retrieval-guided adjustment mechanism (ABC-RL) to mitigate overfitting of pre-trained reinforcement learning agents.", "Demonstrates impressive empirical improvements: up to 24.8% better Quality of Result (QoR) and up to 9x speedup at iso-QoR, outperforming state-of-the-art baselines.", "The idea of calibrating RL policy recommendations based on instance similarity is intuitively well-motivated and tackles generalization challenges."], "weaknesses": ["The role and computation of the nearest neighbor similarity score and the tuning of the alpha parameter may require further technical details and justification.", "The abstract does not specify the scale and diversity of evaluated benchmarks, nor whether results generalize beyond those closely related to the training set.", "Limited analysis on potential drawbacks, such as failure cases or computational overhead introduced by the retrieval module.", "The contribution of retrieval versus RL is blurred; the novelty appears to be in the tuning mechanism rather than a fundamentally new algorithm.", "Lack of direct comparison to standard domain adaptation or meta-learning baselines."], "suggestions": ["Clarify and rigorously define the computation of the similarity score and the adjustment mechanism (alpha parameter), with ablation studies to validate its effectiveness separately.", "Present detailed benchmark coverage, including results on both seen and novel (\u2018out-of-distribution\u2019) hardware designs.", "Include analysis of runtime and computational overhead introduced by nearest-neighbor retrieval, relative to the total synthesis process.", "Benchmark the approach against alternative adaptation/transfer methods, such as meta-RL or domain adaptation techniques.", "Discuss limitations or scenarios where ABC-RL might not perform well, and suggest future directions."], "summary": "This paper proposes ABC-RL, a retrieval-guided RL framework for Boolean circuit minimization that adaptively tunes policy recommendations by leveraging similarity-based retrieval from prior data. The approach shows strong empirical gains in circuit quality and synthesis speed. However, the description and evaluation of the instance similarity mechanism could be more detailed, and the novelty is primarily in the policy mixing rather than a fundamentally new learning paradigm. Further analysis on generalization and computational trade-offs would strengthen the work."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses an important problem in logic synthesis with direct implications for chip design efficiency.\",\n    \"Introduces a principled retrieval-guided adjustment mechanism (ABC-RL) to mitigate overfitting of pre-trained reinforcement learning agents.\",\n    \"Demonstrates impressive empirical improvements: up to 24.8% better Quality of Result (QoR) and up to 9x speedup at iso-QoR, outperforming state-of-the-art baselines.\",\n    \"The idea of calibrating RL policy recommendations based on instance similarity is intuitively well-motivated and tackles generalization challenges.\"\n  ],\n  \"weaknesses\": [\n    \"The role and computation of the nearest neighbor similarity score and the tuning of the alpha parameter may require further technical details and justification.\",\n    \"The abstract does not specify the scale and diversity of evaluated benchmarks, nor whether results generalize beyond those closely related to the training set.\",\n    \"Limited analysis on potential drawbacks, such as failure cases or computational overhead introduced by the retrieval module.\",\n    \"The contribution of retrieval versus RL is blurred; the novelty appears to be in the tuning mechanism rather than a fundamentally new algorithm.\",\n    \"Lack of direct comparison to standard domain adaptation or meta-learning baselines.\"\n  ],\n  \"suggestions\": [\n    \"Clarify and rigorously define the computation of the similarity score and the adjustment mechanism (alpha parameter), with ablation studies to validate its effectiveness separately.\",\n    \"Present detailed benchmark coverage, including results on both seen and novel (\u2018out-of-distribution\u2019) hardware designs.\",\n    \"Include analysis of runtime and computational overhead introduced by nearest-neighbor retrieval, relative to the total synthesis process.\",\n    \"Benchmark the approach against alternative adaptation/transfer methods, such as meta-RL or domain adaptation techniques.\",\n    \"Discuss limitations or scenarios where ABC-RL might not perform well, and suggest future directions.\"\n  ],\n  \"summary\": \"This paper proposes ABC-RL, a retrieval-guided RL framework for Boolean circuit minimization that adaptively tunes policy recommendations by leveraging similarity-based retrieval from prior data. The approach shows strong empirical gains in circuit quality and synthesis speed. However, the description and evaluation of the instance similarity mechanism could be more detailed, and the novelty is primarily in the policy mixing rather than a fundamentally new learning paradigm. Further analysis on generalization and computational trade-offs would strengthen the work.\"\n}", "usage": {"prompt_tokens": 368, "completion_tokens": 494, "total_tokens": 862}, "timestamp": "2025-12-29T07:02:10.177911"}
{"paper_id": "ukidfml68f", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses a critical bottleneck in high-resolution 3D generation by focusing on the understudied problem of gradient propagation through frozen components.", "Proposes a simple yet effective technique (Pixel-wise Gradient Clipping) that is easy to integrate into existing pipelines.", "Demonstrates broad applicability by targeting commonly used latent representation-based models (e.g., LDM).", "Shows empirical improvements through experiments, indicating practical benefit.", "Maintains low computational overhead."], "weaknesses": ["Lack of detailed quantitative comparisons with other potential gradient control techniques.", "Insufficient ablation studies to disentangle the contribution of PGC versus other training modifications.", "Limited theoretical analysis explaining why PGC preserves texture directions while improving stability.", "No clarity on potential trade-offs or failure cases, especially on diverse datasets.", "Description of integration specifics could be more detailed to facilitate reproducibility."], "suggestions": ["Include more comprehensive quantitative and qualitative comparisons with alternative gradient regularization/clipping methods.", "Add ablation studies isolating PGC from other proposed or standard components.", "Provide theoretical or empirical explanation for why pixel-wise gradient clipping is particularly effective at preserving texture.", "Discuss possible limitations or negative impacts (if any) on convergence or visual quality.", "Expand implementation details, possibly including code, to support reproducibility."], "summary": "This paper proposes Pixel-wise Gradient Clipping (PGC), a straightforward gradient regularization technique to improve high-resolution 3D object generation in latent diffusion model workflows. The approach is compelling and easy to integrate, with empirical results suggesting clear benefits for synthesizing detailed textures. However, the work would be strengthened by more thorough comparisons, ablations, and analysis of underlying mechanisms and trade-offs."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses a critical bottleneck in high-resolution 3D generation by focusing on the understudied problem of gradient propagation through frozen components.\",\n    \"Proposes a simple yet effective technique (Pixel-wise Gradient Clipping) that is easy to integrate into existing pipelines.\",\n    \"Demonstrates broad applicability by targeting commonly used latent representation-based models (e.g., LDM).\",\n    \"Shows empirical improvements through experiments, indicating practical benefit.\",\n    \"Maintains low computational overhead.\"\n  ],\n  \"weaknesses\": [\n    \"Lack of detailed quantitative comparisons with other potential gradient control techniques.\",\n    \"Insufficient ablation studies to disentangle the contribution of PGC versus other training modifications.\",\n    \"Limited theoretical analysis explaining why PGC preserves texture directions while improving stability.\",\n    \"No clarity on potential trade-offs or failure cases, especially on diverse datasets.\",\n    \"Description of integration specifics could be more detailed to facilitate reproducibility.\"\n  ],\n  \"suggestions\": [\n    \"Include more comprehensive quantitative and qualitative comparisons with alternative gradient regularization/clipping methods.\",\n    \"Add ablation studies isolating PGC from other proposed or standard components.\",\n    \"Provide theoretical or empirical explanation for why pixel-wise gradient clipping is particularly effective at preserving texture.\",\n    \"Discuss possible limitations or negative impacts (if any) on convergence or visual quality.\",\n    \"Expand implementation details, possibly including code, to support reproducibility.\"\n  ],\n  \"summary\": \"This paper proposes Pixel-wise Gradient Clipping (PGC), a straightforward gradient regularization technique to improve high-resolution 3D object generation in latent diffusion model workflows. The approach is compelling and easy to integrate, with empirical results suggesting clear benefits for synthesizing detailed textures. However, the work would be strengthened by more thorough comparisons, ablations, and analysis of underlying mechanisms and trade-offs.\"\n}", "usage": {"prompt_tokens": 376, "completion_tokens": 386, "total_tokens": 762}, "timestamp": "2025-12-29T07:02:22.438557"}
{"paper_id": "0uFTqvQhML", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Proposes a novel pipeline for controllable 3D street scene generation, addressing a critical gap in autonomous driving simulation.", "Supports a wide range of controls (BEV maps, 3D objects, text), indicating strong versatility.", "Innovative training process (generation-then-reconstruction) improves controllability and scene quality over prior work.", "Introduces robust methods (deformable Gaussian splatting, monocular depth initialization, exposure correction) to address key reconstruction challenges.", "Comprehensive validation on a real-world dataset (nuScenes), with applications to downstream tasks such as BEV segmentation."], "weaknesses": ["Details on quantitative comparisons and ablation studies are missing in the abstract, limiting assessment of performance gains.", "Handling of dynamic elements (moving objects, changing lighting) is not discussed.", "Potential scalability and computational efficiency issues for large-scale or real-time applications are not addressed.", "Possible overfitting or bias, as method seems tied to a single dataset (nuScenes)."], "suggestions": ["Provide detailed quantitative comparisons against state-of-the-art baselines, including metrics for controllability, realism, and reconstruction accuracy.", "Include ablation studies for each major component (video generation, Gaussian splatting, appearance modeling) to isolate their contributions.", "Discuss how the pipeline handles dynamic scenes and the extent of generalization to unseen scenarios or other datasets.", "Report inference/training times and discuss system scalability.", "Release code and models to facilitate reproducibility and community adoption."], "summary": "MagicDrive3D introduces an innovative and versatile framework for controllable 3D street scene generation, which is highly relevant for simulation and downstream tasks in autonomous driving. The approach is technically sound and validated on challenging data, but would benefit from more thorough quantitative evaluation, ablations, and discussions on scalability and generalization. Overall, this is a strong contribution to 3D scene generation research."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Proposes a novel pipeline for controllable 3D street scene generation, addressing a critical gap in autonomous driving simulation.\",\n    \"Supports a wide range of controls (BEV maps, 3D objects, text), indicating strong versatility.\",\n    \"Innovative training process (generation-then-reconstruction) improves controllability and scene quality over prior work.\",\n    \"Introduces robust methods (deformable Gaussian splatting, monocular depth initialization, exposure correction) to address key reconstruction challenges.\",\n    \"Comprehensive validation on a real-world dataset (nuScenes), with applications to downstream tasks such as BEV segmentation.\"\n  ],\n  \"weaknesses\": [\n    \"Details on quantitative comparisons and ablation studies are missing in the abstract, limiting assessment of performance gains.\",\n    \"Handling of dynamic elements (moving objects, changing lighting) is not discussed.\",\n    \"Potential scalability and computational efficiency issues for large-scale or real-time applications are not addressed.\",\n    \"Possible overfitting or bias, as method seems tied to a single dataset (nuScenes).\"\n  ],\n  \"suggestions\": [\n    \"Provide detailed quantitative comparisons against state-of-the-art baselines, including metrics for controllability, realism, and reconstruction accuracy.\",\n    \"Include ablation studies for each major component (video generation, Gaussian splatting, appearance modeling) to isolate their contributions.\",\n    \"Discuss how the pipeline handles dynamic scenes and the extent of generalization to unseen scenarios or other datasets.\",\n    \"Report inference/training times and discuss system scalability.\",\n    \"Release code and models to facilitate reproducibility and community adoption.\"\n  ],\n  \"summary\": \"MagicDrive3D introduces an innovative and versatile framework for controllable 3D street scene generation, which is highly relevant for simulation and downstream tasks in autonomous driving. The approach is technically sound and validated on challenging data, but would benefit from more thorough quantitative evaluation, ablations, and discussions on scalability and generalization. Overall, this is a strong contribution to 3D scene generation research.\"\n}", "usage": {"prompt_tokens": 307, "completion_tokens": 425, "total_tokens": 732}, "timestamp": "2025-12-29T07:02:34.994200"}
{"paper_id": "GkJCgUmIqA", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Addresses a well-known weakness of standard PINNs, specifically training instability and failure on complex PDEs.", "Proposes an original integration of trust-region SQP with deep learning for hard-constrained training.", "Demonstrates clear empirical improvements (1-3 orders of magnitude) over state-of-the-art hard-constrained PINN methods.", "Addresses computational bottlenecks by using quasi-Newton methods and a practical pretraining step.", "Shows robustness against hyperparameters and problem specifics, and the pretraining technique generalizes to other approaches.", "Substantial experimental validation is claimed."], "weaknesses": ["Details of the experimental setup, such as benchmark PDEs, are unspecified in the abstract, limiting assessment of generality.", "Computational overhead and scalability to larger or real-world PDE problems is not quantified in the abstract.", "Theoretical analysis of convergence and guarantees for the proposed trSQP-PINN is not mentioned.", "Abstract does not discuss any potential limitations or failure modes of trust-region application to neural PDE solvers.", "Comparisons focus on hard-constrained baselines; direct comparison to state-of-the-art soft-constrained approaches or hybrid methods is not clarified."], "suggestions": ["Include comparisons with recent soft-constrained or hybrid PINN variants to contextualize improvements.", "Provide details of computational cost versus accuracy tradeoff, especially for large-scale PDEs.", "Add theoretical analysis or discussion on convergence properties of trSQP-PINN.", "Detail benchmarks and experiment protocols for better reproducibility and to evaluate generalizability.", "Discuss possible limitations or edge cases where trust-region SQP may not offer benefits."], "summary": "This work presents a promising and innovative hard-constrained PINN framework using trust-region sequential quadratic programming, showing substantial accuracy improvements over contemporary methods. The integration of quasi-Newton second-order updates and an effective pretraining strategy addresses both computational and optimization challenges. However, more detail is needed on experimental scope, computational efficiency, and theoretical guarantees to fully assess impact and generalizability."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses a well-known weakness of standard PINNs, specifically training instability and failure on complex PDEs.\",\n    \"Proposes an original integration of trust-region SQP with deep learning for hard-constrained training.\",\n    \"Demonstrates clear empirical improvements (1-3 orders of magnitude) over state-of-the-art hard-constrained PINN methods.\",\n    \"Addresses computational bottlenecks by using quasi-Newton methods and a practical pretraining step.\",\n    \"Shows robustness against hyperparameters and problem specifics, and the pretraining technique generalizes to other approaches.\",\n    \"Substantial experimental validation is claimed.\"\n  ],\n  \"weaknesses\": [\n    \"Details of the experimental setup, such as benchmark PDEs, are unspecified in the abstract, limiting assessment of generality.\",\n    \"Computational overhead and scalability to larger or real-world PDE problems is not quantified in the abstract.\",\n    \"Theoretical analysis of convergence and guarantees for the proposed trSQP-PINN is not mentioned.\",\n    \"Abstract does not discuss any potential limitations or failure modes of trust-region application to neural PDE solvers.\",\n    \"Comparisons focus on hard-constrained baselines; direct comparison to state-of-the-art soft-constrained approaches or hybrid methods is not clarified.\"\n  ],\n  \"suggestions\": [\n    \"Include comparisons with recent soft-constrained or hybrid PINN variants to contextualize improvements.\",\n    \"Provide details of computational cost versus accuracy tradeoff, especially for large-scale PDEs.\",\n    \"Add theoretical analysis or discussion on convergence properties of trSQP-PINN.\",\n    \"Detail benchmarks and experiment protocols for better reproducibility and to evaluate generalizability.\",\n    \"Discuss possible limitations or edge cases where trust-region SQP may not offer benefits.\"\n  ],\n  \"summary\": \"This work presents a promising and innovative hard-constrained PINN framework using trust-region sequential quadratic programming, showing substantial accuracy improvements over contemporary methods. The integration of quasi-Newton second-order updates and an effective pretraining strategy addresses both computational and optimization challenges. However, more detail is needed on experimental scope, computational efficiency, and theoretical guarantees to fully assess impact and generalizability.\"\n}", "usage": {"prompt_tokens": 403, "completion_tokens": 450, "total_tokens": 853}, "timestamp": "2025-12-29T07:02:48.923567"}
{"paper_id": "4X9RpKH4Ls", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Novel application of Transformers to a challenging mathematics problem (enumerative geometry).", "Introduction of the Dynamic Range Activator (DRA) to address issues of recursion and heteroscedasticity.", "Covers an extremely wide dynamic range of intersection numbers.", "Integration of conformal prediction for calibrated uncertainty estimation.", "Deep interpretability analysis, including emergent discovery of mathematical constraints (Virasoro) and asymptotic behavior.", "First work exploring high-variance recursive functions with machine learning at this scale."], "weaknesses": ["Details of the training data, data generation or curation process are unclear.", "Empirical performance metrics are not quantitatively compared to plausible baselines or ablations (e.g., standard Transformers, other architectures).", "No discussion of computational cost (speed and resource usage) relative to traditional or other machine learning baselines.", "Ablation or sensitivity analysis on the DRA and conformal prediction are missing.", "Potential risk of overfitting or learning spurious patterns is not discussed; generalizability is uncertain.", "Interpretability claims (Virasoro discovery) are interesting but require more rigorous, formal evaluation."], "suggestions": ["Provide clearer details on training data generation, especially the range, source, and any domain heuristics used.", "Conduct (and report) systematic ablation studies on the DRA and the uncertainty calibration mechanism.", "Benchmark performance against both prior computational methods and more standard deep learning baselines.", "Include computational resources and efficiency analysis.", "More rigorously quantify the internalization of Virasoro constraints and asymptotic properties, perhaps via targeted probing or formal verification.", "Discuss and, if possible, empirically assess the risk of overfitting to combinatorially generated data."], "summary": "This paper presents an innovative approach using Transformers\u2014and a novel activation function\u2014to tackle the notoriously hard problem of computing \u03c8-class intersection numbers in enumerative geometry. The proposed methodology handles both factorial growth and tremendous variance, offering uncertainty estimates and yielding novel interpretability insights about emergent mathematical structure within the neural network. While promising, the manuscript would benefit from more detailed baselines, ablation studies, data methodology clarity, and deeper rigor in evaluating claims of mathematical discovery and generalizability."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Novel application of Transformers to a challenging mathematics problem (enumerative geometry).\",\n    \"Introduction of the Dynamic Range Activator (DRA) to address issues of recursion and heteroscedasticity.\",\n    \"Covers an extremely wide dynamic range of intersection numbers.\",\n    \"Integration of conformal prediction for calibrated uncertainty estimation.\",\n    \"Deep interpretability analysis, including emergent discovery of mathematical constraints (Virasoro) and asymptotic behavior.\",\n    \"First work exploring high-variance recursive functions with machine learning at this scale.\"\n  ],\n  \"weaknesses\": [\n    \"Details of the training data, data generation or curation process are unclear.\",\n    \"Empirical performance metrics are not quantitatively compared to plausible baselines or ablations (e.g., standard Transformers, other architectures).\",\n    \"No discussion of computational cost (speed and resource usage) relative to traditional or other machine learning baselines.\",\n    \"Ablation or sensitivity analysis on the DRA and conformal prediction are missing.\",\n    \"Potential risk of overfitting or learning spurious patterns is not discussed; generalizability is uncertain.\",\n    \"Interpretability claims (Virasoro discovery) are interesting but require more rigorous, formal evaluation.\"\n  ],\n  \"suggestions\": [\n    \"Provide clearer details on training data generation, especially the range, source, and any domain heuristics used.\",\n    \"Conduct (and report) systematic ablation studies on the DRA and the uncertainty calibration mechanism.\",\n    \"Benchmark performance against both prior computational methods and more standard deep learning baselines.\",\n    \"Include computational resources and efficiency analysis.\",\n    \"More rigorously quantify the internalization of Virasoro constraints and asymptotic properties, perhaps via targeted probing or formal verification.\",\n    \"Discuss and, if possible, empirically assess the risk of overfitting to combinatorially generated data.\"\n  ],\n  \"summary\": \"This paper presents an innovative approach using Transformers\u2014and a novel activation function\u2014to tackle the notoriously hard problem of computing \u03c8-class intersection numbers in enumerative geometry. The proposed methodology handles both factorial growth and tremendous variance, offering uncertainty estimates and yielding novel interpretability insights about emergent mathematical structure within the neural network. While promising, the manuscript would benefit from more detailed baselines, ablation studies, data methodology clarity, and deeper rigor in evaluating claims of mathematical discovery and generalizability.\"\n}", "usage": {"prompt_tokens": 378, "completion_tokens": 498, "total_tokens": 876}, "timestamp": "2025-12-29T07:03:05.461052"}
{"paper_id": "RFqeoVfLHa", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses a timely and under-explored topic of potential regressions in post-training LLM improvements.", "Proposes a novel concept: 'self-improvement reversal', contributing to LLM evaluation literature.", "Claims a comprehensive experimental methodology across diverse tasks.", "Introduces an evaluative framework to distinguish between metric gains and real functional advancements."], "weaknesses": ["The abstract lacks concrete methodological details (dataset scale, baselines, analysis type).", "Unclear how generalizable the findings are across LLM architectures or problem domains.", "The operationalization of 'essential capabilities' and how regressions are measured is not specified.", "It is not evident if the framework outperforms existing evaluation tools."], "suggestions": ["Provide detailed experimental setups, including model types, datasets, and evaluation metrics.", "Clarify the definition and assessment criteria for 'broader, essential capabilities'.", "Compare the proposed evaluative framework more concretely against standard benchmarks.", "Discuss practical implications for LLM deployment and possible mitigation strategies."], "summary": "This paper raises an important concern that post-training improvements in LLMs may lead to regressions in key capabilities, despite overall metric gains. By formalizing 'self-improvement reversal' and presenting a new evaluative framework, the study brings valuable insight to LLM evaluation. However, the abstract leaves out critical methodological clarity and fails to sufficiently ground its claims in specific experimental practices. Addressing these limitations would significantly strengthen the work's impact and credibility."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses a timely and under-explored topic of potential regressions in post-training LLM improvements.\",\n    \"Proposes a novel concept: 'self-improvement reversal', contributing to LLM evaluation literature.\",\n    \"Claims a comprehensive experimental methodology across diverse tasks.\",\n    \"Introduces an evaluative framework to distinguish between metric gains and real functional advancements.\"\n  ],\n  \"weaknesses\": [\n    \"The abstract lacks concrete methodological details (dataset scale, baselines, analysis type).\",\n    \"Unclear how generalizable the findings are across LLM architectures or problem domains.\",\n    \"The operationalization of 'essential capabilities' and how regressions are measured is not specified.\",\n    \"It is not evident if the framework outperforms existing evaluation tools.\"\n  ],\n  \"suggestions\": [\n    \"Provide detailed experimental setups, including model types, datasets, and evaluation metrics.\",\n    \"Clarify the definition and assessment criteria for 'broader, essential capabilities'.\",\n    \"Compare the proposed evaluative framework more concretely against standard benchmarks.\",\n    \"Discuss practical implications for LLM deployment and possible mitigation strategies.\"\n  ],\n  \"summary\": \"This paper raises an important concern that post-training improvements in LLMs may lead to regressions in key capabilities, despite overall metric gains. By formalizing 'self-improvement reversal' and presenting a new evaluative framework, the study brings valuable insight to LLM evaluation. However, the abstract leaves out critical methodological clarity and fails to sufficiently ground its claims in specific experimental practices. Addressing these limitations would significantly strengthen the work's impact and credibility.\"\n}", "usage": {"prompt_tokens": 280, "completion_tokens": 339, "total_tokens": 619}, "timestamp": "2025-12-29T07:03:16.764268"}
{"paper_id": "iriEqxFB4y", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses a relevant challenge in OOD detection\u2014overfitting to specific outlier types.", "Introduces a simple yet effective sampling method (DOS) that promotes diversity among outlier samples.", "Careful empirical evaluation showing clear improvements (e.g., significant FPR95 reduction).", "Connects clustering in feature space with informative sample selection.", "Method appears straightforward to implement and integrate into existing pipelines."], "weaknesses": ["The clustering cost and scalability at large scales is not discussed.", "Potential sensitivity to clustering hyperparameters (e.g., cluster number) is not thoroughly analyzed.", "Limited theoretical motivation; most arguments are empirical.", "Ablation studies on the effect of diversity alone vs. uncertainty are not mentioned.", "Generalization to modalities beyond vision or to different OOD settings is not explored."], "suggestions": ["Provide a more detailed analysis of computational cost and clustering scalability.", "Discuss or experiment with robustness to clustering choices and hyperparameters.", "Include more ablation experiments separating the effects of diversity from uncertainty.", "Explore the applicability of DOS to other domains (e.g., NLP, time series) or OOD settings.", "Consider adding theoretical analysis or justification for why diversity promotes better decision boundaries."], "summary": "The paper proposes DOS, a diverse outlier sampling strategy for OOD detection, showing strong empirical gains by combining feature clustering with uncertainty-based sampling. While the approach is intuitively and practically appealing, more analysis on scalability, ablations, and broader applicability would strengthen the work."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses a relevant challenge in OOD detection\u2014overfitting to specific outlier types.\",\n    \"Introduces a simple yet effective sampling method (DOS) that promotes diversity among outlier samples.\",\n    \"Careful empirical evaluation showing clear improvements (e.g., significant FPR95 reduction).\",\n    \"Connects clustering in feature space with informative sample selection.\",\n    \"Method appears straightforward to implement and integrate into existing pipelines.\"\n  ],\n  \"weaknesses\": [\n    \"The clustering cost and scalability at large scales is not discussed.\",\n    \"Potential sensitivity to clustering hyperparameters (e.g., cluster number) is not thoroughly analyzed.\",\n    \"Limited theoretical motivation; most arguments are empirical.\",\n    \"Ablation studies on the effect of diversity alone vs. uncertainty are not mentioned.\",\n    \"Generalization to modalities beyond vision or to different OOD settings is not explored.\"\n  ],\n  \"suggestions\": [\n    \"Provide a more detailed analysis of computational cost and clustering scalability.\",\n    \"Discuss or experiment with robustness to clustering choices and hyperparameters.\",\n    \"Include more ablation experiments separating the effects of diversity from uncertainty.\",\n    \"Explore the applicability of DOS to other domains (e.g., NLP, time series) or OOD settings.\",\n    \"Consider adding theoretical analysis or justification for why diversity promotes better decision boundaries.\"\n  ],\n  \"summary\": \"The paper proposes DOS, a diverse outlier sampling strategy for OOD detection, showing strong empirical gains by combining feature clustering with uncertainty-based sampling. While the approach is intuitively and practically appealing, more analysis on scalability, ablations, and broader applicability would strengthen the work.\"\n}", "usage": {"prompt_tokens": 303, "completion_tokens": 347, "total_tokens": 650}, "timestamp": "2025-12-29T07:03:32.840624"}
{"paper_id": "yatNm6A6sR", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Releases a large-scale, well-structured world-wide OSM-based graph database (1 billion nodes).", "Demonstrates system utility via three diverse use cases, spanning both established and novel tasks.", "Provides significantly broadened benchmarks for traffic prediction and policy control tasks.", "Cloud-native and accessible system design, supporting downstream research.", "Facilitates multimodal data integration for large-model training."], "weaknesses": ["Limited technical novelty: most contributions are dataset creation and system engineering rather than new algorithms or learning methodology.", "Insufficient experimental evaluation: unclear how the benchmark advances state-of-the-art relative to prior work.", "No quantitative comparison to previous OSM data processing systems or datasets.", "Unclear documentation or interface specifications for the released data/converters.", "Scalability and efficiency claims are asserted but not rigorously benchmarked."], "suggestions": ["Provide detailed benchmarking of storage, query efficiency, and usability compared to prior OSM data systems.", "Include documentation/examples for how third parties can use and extend the dataset and tools.", "Report performance on downstream tasks with established baselines to contextualize the dataset's value.", "Clarify intellectual property, data usage, and maintenance/support plans for longevity.", "Discuss limitations, such as coverage gaps in OSM, possible biases, and update mechanisms."], "summary": "This work introduces OSM+, a cloud-native structured OSM road network database at global scale, and releases comprehensive benchmarks for urban research tasks. The paper's main value lies in engineering effort and enabling large-scale experiments, with impactful datasets and tools. However, it lacks technical novelty and rigorous head-to-head evaluation with previous systems. Strengthening experimental analysis, usability documentation, and comparative benchmarks would improve the submission's robustness and research impact."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Releases a large-scale, well-structured world-wide OSM-based graph database (1 billion nodes).\",\n    \"Demonstrates system utility via three diverse use cases, spanning both established and novel tasks.\",\n    \"Provides significantly broadened benchmarks for traffic prediction and policy control tasks.\",\n    \"Cloud-native and accessible system design, supporting downstream research.\",\n    \"Facilitates multimodal data integration for large-model training.\"\n  ],\n  \"weaknesses\": [\n    \"Limited technical novelty: most contributions are dataset creation and system engineering rather than new algorithms or learning methodology.\",\n    \"Insufficient experimental evaluation: unclear how the benchmark advances state-of-the-art relative to prior work.\",\n    \"No quantitative comparison to previous OSM data processing systems or datasets.\",\n    \"Unclear documentation or interface specifications for the released data/converters.\",\n    \"Scalability and efficiency claims are asserted but not rigorously benchmarked.\"\n  ],\n  \"suggestions\": [\n    \"Provide detailed benchmarking of storage, query efficiency, and usability compared to prior OSM data systems.\",\n    \"Include documentation/examples for how third parties can use and extend the dataset and tools.\",\n    \"Report performance on downstream tasks with established baselines to contextualize the dataset's value.\",\n    \"Clarify intellectual property, data usage, and maintenance/support plans for longevity.\",\n    \"Discuss limitations, such as coverage gaps in OSM, possible biases, and update mechanisms.\"\n  ],\n  \"summary\": \"This work introduces OSM+, a cloud-native structured OSM road network database at global scale, and releases comprehensive benchmarks for urban research tasks. The paper's main value lies in engineering effort and enabling large-scale experiments, with impactful datasets and tools. However, it lacks technical novelty and rigorous head-to-head evaluation with previous systems. Strengthening experimental analysis, usability documentation, and comparative benchmarks would improve the submission's robustness and research impact.\"\n}", "usage": {"prompt_tokens": 286, "completion_tokens": 397, "total_tokens": 683}, "timestamp": "2025-12-29T07:03:46.207821"}
{"paper_id": "Fb0q2uI4Ha", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Introduction of a large-scale, well-annotated multimodal dataset (TAU-106K) focusing on traffic accident understanding, which addresses a clear gap in the field.", "Novel pipeline for video-to-image annotation enhances data quality and label comprehensiveness.", "Development of TABot, a task-specialized MLLM, demonstrates thoughtful adaptation to domain-specific requirements (traffic anomaly comprehension).", "Experimental results indicate clear performance gains on challenging tasks (accident recognition, spatial-temporal grounding) compared to baselines.", "Commitment to openness: code and data will be publicly released, supporting reproducibility and future research."], "weaknesses": ["Limited discussion of dataset limitations (e.g., potential domain bias, generalization to non-sourced accident types).", "Evaluation is centered around the introduced benchmark\u2014external validation or real-world deployment evidence is lacking.", "Comparisons against state-of-the-art MLLMs and related models are only briefly mentioned; depth and breadth of baselines unclear.", "Ablation studies on annotation pipeline components or auxiliary description tasks appear missing or limited.", "Details on annotation quality control and inter-annotator agreement are sparse."], "suggestions": ["Include a more comprehensive analysis of the dataset's coverage, potential biases, and limitations; consider adding statistics or case studies highlighting diversity.", "Provide more detailed experimental comparisons to a wider range of existing models, including recent general-purpose MLLMs and specialized anomaly detectors.", "Offer ablation studies on the annotation pipeline and each component/task in the two-step training process to clarify their individual contributions.", "Supplement the experimental section with a small-scale real-world evaluation or generalization study beyond the constructed benchmark.", "Elaborate on annotation practices, including quality control processes, annotator instructions, and agreement measures."], "summary": "This work presents TAU-106K, a significant multimodal dataset and benchmark in the underexplored area of traffic accident understanding, alongside TABot, a tailored MLLM exhibiting strong experimental results. While the dataset and model contributions are sound and fill a relevant gap, the work would be notably strengthened by deeper discussion of dataset properties, more exhaustive baselines, detailed ablations, and evidence of real-world applicability. The public release of data and code strongly enhances the impact and reproducibility."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Introduction of a large-scale, well-annotated multimodal dataset (TAU-106K) focusing on traffic accident understanding, which addresses a clear gap in the field.\",\n    \"Novel pipeline for video-to-image annotation enhances data quality and label comprehensiveness.\",\n    \"Development of TABot, a task-specialized MLLM, demonstrates thoughtful adaptation to domain-specific requirements (traffic anomaly comprehension).\",\n    \"Experimental results indicate clear performance gains on challenging tasks (accident recognition, spatial-temporal grounding) compared to baselines.\",\n    \"Commitment to openness: code and data will be publicly released, supporting reproducibility and future research.\"\n  ],\n  \"weaknesses\": [\n    \"Limited discussion of dataset limitations (e.g., potential domain bias, generalization to non-sourced accident types).\",\n    \"Evaluation is centered around the introduced benchmark\u2014external validation or real-world deployment evidence is lacking.\",\n    \"Comparisons against state-of-the-art MLLMs and related models are only briefly mentioned; depth and breadth of baselines unclear.\",\n    \"Ablation studies on annotation pipeline components or auxiliary description tasks appear missing or limited.\",\n    \"Details on annotation quality control and inter-annotator agreement are sparse.\"\n  ],\n  \"suggestions\": [\n    \"Include a more comprehensive analysis of the dataset's coverage, potential biases, and limitations; consider adding statistics or case studies highlighting diversity.\",\n    \"Provide more detailed experimental comparisons to a wider range of existing models, including recent general-purpose MLLMs and specialized anomaly detectors.\",\n    \"Offer ablation studies on the annotation pipeline and each component/task in the two-step training process to clarify their individual contributions.\",\n    \"Supplement the experimental section with a small-scale real-world evaluation or generalization study beyond the constructed benchmark.\",\n    \"Elaborate on annotation practices, including quality control processes, annotator instructions, and agreement measures.\"\n  ],\n  \"summary\": \"This work presents TAU-106K, a significant multimodal dataset and benchmark in the underexplored area of traffic accident understanding, alongside TABot, a tailored MLLM exhibiting strong experimental results. While the dataset and model contributions are sound and fill a relevant gap, the work would be notably strengthened by deeper discussion of dataset properties, more exhaustive baselines, detailed ablations, and evidence of real-world applicability. The public release of data and code strongly enhances the impact and reproducibility.\"\n}", "usage": {"prompt_tokens": 324, "completion_tokens": 503, "total_tokens": 827}, "timestamp": "2025-12-29T07:03:59.968946"}
{"paper_id": "QkDUdPRcma", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Identifies and addresses an underexplored aspect of deep SNNs (direct spike encoding).", "Introduces a novel, biologically-inspired approach (homeostasis) to improve encoding.", "Proposes concrete technical innovations: dynamic feature encoding loss, adaptive threshold, feature diversity loss.", "Experimental results show consistent improvements across several image classification datasets and network architectures.", "Well-written and motivated."], "weaknesses": ["Methodological details are sparse in the abstract; it's unclear how each loss term or adaptation is concretely implemented.", "The scope of evaluation (datasets, network types, baselines) is only briefly mentioned\u2014unclear how comprehensive or rigorous it is.", "Potential computational or training overhead of the proposed method is not discussed.", "Lacks theoretical analysis to support why homeostatic mechanisms improve encoding beyond empirical justification."], "suggestions": ["Provide thorough ablation studies isolating the effect of each component (encoding loss, adaptive threshold, diversity loss).", "Include more rigorous comparisons to strong SNN and ANN baselines.", "Analyze the computational and training efficiency trade-offs introduced by H-Direct.", "Discuss generalization to non-vision tasks and other sensory modalities.", "Consider theoretical justification or analysis of homeostatic regulation in the encoding process."], "summary": "This paper proposes H-Direct, a novel homeostasis-aware direct spike encoding scheme for deep SNNs, inspired by biological principles. The approach introduces adaptive and diversity-focused mechanisms to improve information encoding and demonstrates empirical gains in both performance and efficiency across several image classifications and architectures. However, the submission could be strengthened by including fuller methodological details, broader benchmarking, efficiency analysis, and more comprehensive theoretical insight."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Identifies and addresses an underexplored aspect of deep SNNs (direct spike encoding).\",\n    \"Introduces a novel, biologically-inspired approach (homeostasis) to improve encoding.\",\n    \"Proposes concrete technical innovations: dynamic feature encoding loss, adaptive threshold, feature diversity loss.\",\n    \"Experimental results show consistent improvements across several image classification datasets and network architectures.\",\n    \"Well-written and motivated.\"\n  ],\n  \"weaknesses\": [\n    \"Methodological details are sparse in the abstract; it's unclear how each loss term or adaptation is concretely implemented.\",\n    \"The scope of evaluation (datasets, network types, baselines) is only briefly mentioned\u2014unclear how comprehensive or rigorous it is.\",\n    \"Potential computational or training overhead of the proposed method is not discussed.\",\n    \"Lacks theoretical analysis to support why homeostatic mechanisms improve encoding beyond empirical justification.\"\n  ],\n  \"suggestions\": [\n    \"Provide thorough ablation studies isolating the effect of each component (encoding loss, adaptive threshold, diversity loss).\",\n    \"Include more rigorous comparisons to strong SNN and ANN baselines.\",\n    \"Analyze the computational and training efficiency trade-offs introduced by H-Direct.\",\n    \"Discuss generalization to non-vision tasks and other sensory modalities.\",\n    \"Consider theoretical justification or analysis of homeostatic regulation in the encoding process.\"\n  ],\n  \"summary\": \"This paper proposes H-Direct, a novel homeostasis-aware direct spike encoding scheme for deep SNNs, inspired by biological principles. The approach introduces adaptive and diversity-focused mechanisms to improve information encoding and demonstrates empirical gains in both performance and efficiency across several image classifications and architectures. However, the submission could be strengthened by including fuller methodological details, broader benchmarking, efficiency analysis, and more comprehensive theoretical insight.\"\n}", "usage": {"prompt_tokens": 326, "completion_tokens": 379, "total_tokens": 705}, "timestamp": "2025-12-29T07:04:11.933874"}
{"paper_id": "r0kY4SS7ts", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Addresses an underexplored class of Markov games (RPMGs) distinguished from standard MPGs.", "Clarifies theoretical hardness: proves PPAD-hardness for both stationary and nonstationary NE computation in RPMGs under various conditions.", "Provides a positive tractability result\u2014an efficient algorithm for approximate NE in RPMGs with additive transitions.", "Conceptually novel: introduces and formalizes Adversarial RPMGs, expanding the applicability of the framework.", "Clear positioning with respect to recent MPGs literature."], "weaknesses": ["Limited discussion/examples of real-world applications, given in the abstract.", "No direct comparison (empirical or conceptual) to existing MPG algorithms or complexity results.", "The scope and limitations of the efficient algorithm (for additive transitions) not fully elaborated in the abstract.", "Strength of assumptions (e.g., additive transitions) for tractability result could be more clearly justified."], "suggestions": ["Provide concrete motivating examples or applications of RPMGs to demonstrate practical impact.", "Discuss how the hardness results relate to or differ from analogous results in MPGs or general Markov games.", "Expand on the empirical applicability or the limitations of the additive transitions assumption.", "If space permits, compare performance or runtime empirically (or qualitatively) with existing algorithms for MPGs.", "Clarify the broader significance of adversarial RPMGs and potential applications therein."], "summary": "This work makes strong theoretical contributions by distinguishing reward-potential Markov games (RPMGs) from Markov potential games (MPGs), proving new complexity-theoretic results, and introducing efficient algorithms for an important subclass. The investigation of adversarial extensions is novel. However, its practical scope, empirical relevance, and relation to existing results could be further clarified."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses an underexplored class of Markov games (RPMGs) distinguished from standard MPGs.\",\n    \"Clarifies theoretical hardness: proves PPAD-hardness for both stationary and nonstationary NE computation in RPMGs under various conditions.\",\n    \"Provides a positive tractability result\u2014an efficient algorithm for approximate NE in RPMGs with additive transitions.\",\n    \"Conceptually novel: introduces and formalizes Adversarial RPMGs, expanding the applicability of the framework.\",\n    \"Clear positioning with respect to recent MPGs literature.\"\n  ],\n  \"weaknesses\": [\n    \"Limited discussion/examples of real-world applications, given in the abstract.\",\n    \"No direct comparison (empirical or conceptual) to existing MPG algorithms or complexity results.\",\n    \"The scope and limitations of the efficient algorithm (for additive transitions) not fully elaborated in the abstract.\",\n    \"Strength of assumptions (e.g., additive transitions) for tractability result could be more clearly justified.\"\n  ],\n  \"suggestions\": [\n    \"Provide concrete motivating examples or applications of RPMGs to demonstrate practical impact.\",\n    \"Discuss how the hardness results relate to or differ from analogous results in MPGs or general Markov games.\",\n    \"Expand on the empirical applicability or the limitations of the additive transitions assumption.\",\n    \"If space permits, compare performance or runtime empirically (or qualitatively) with existing algorithms for MPGs.\",\n    \"Clarify the broader significance of adversarial RPMGs and potential applications therein.\"\n  ],\n  \"summary\": \"This work makes strong theoretical contributions by distinguishing reward-potential Markov games (RPMGs) from Markov potential games (MPGs), proving new complexity-theoretic results, and introducing efficient algorithms for an important subclass. The investigation of adversarial extensions is novel. However, its practical scope, empirical relevance, and relation to existing results could be further clarified.\"\n}", "usage": {"prompt_tokens": 339, "completion_tokens": 394, "total_tokens": 733}, "timestamp": "2025-12-29T07:04:25.796387"}
{"paper_id": "0yTf37PXcH", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Clear modular improvements: MM-LoRA and QLadder are well-motivated and directly target vision-language challenges.", "Sound methodology: Disentangled LoRA design for modality-specific adaptation is novel and intuitive.", "QLadder adapter's deep aggregation for visual encoders potentially leverages more visual information without retraining large vision backbones.", "Experiments and ablations are claimed to be extensive, showing empirical effectiveness."], "weaknesses": ["Details on architecture, training procedure, and specific benchmarks used are lacking in the abstract.", "Comparative analysis with existing SOTA vision-language models is not described.", "Limited discussion of potential downsides, such as increased model complexity or computational costs.", "Lack of qualitative results or concrete numbers in the abstract prevents assessment of practical impact."], "suggestions": ["Provide more detailed quantitative results in the main paper, including comparisons with strong baselines.", "Include analysis of computational overhead introduced by MM-LoRA and QLadder.", "Discuss failure cases or scenarios where the proposed techniques do not yield improvements.", "Clarify if the approach generalizes to domains outside of the commonly used CLIP image encoder."], "summary": "This work introduces Arcana, a multimodal LLM with two novel modules\u2014MM-LoRA for disentangled modality adaptation and QLadder for improved visual encoding. The approach is well-grounded, and initial claims of empirical success are promising. However, the abstract lacks specific experimental details, comparative analysis, and discussion of limitations. Clarification of these aspects and inclusion of quantitative results will strengthen the contribution."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Clear modular improvements: MM-LoRA and QLadder are well-motivated and directly target vision-language challenges.\",\n    \"Sound methodology: Disentangled LoRA design for modality-specific adaptation is novel and intuitive.\",\n    \"QLadder adapter's deep aggregation for visual encoders potentially leverages more visual information without retraining large vision backbones.\",\n    \"Experiments and ablations are claimed to be extensive, showing empirical effectiveness.\"\n  ],\n  \"weaknesses\": [\n    \"Details on architecture, training procedure, and specific benchmarks used are lacking in the abstract.\",\n    \"Comparative analysis with existing SOTA vision-language models is not described.\",\n    \"Limited discussion of potential downsides, such as increased model complexity or computational costs.\",\n    \"Lack of qualitative results or concrete numbers in the abstract prevents assessment of practical impact.\"\n  ],\n  \"suggestions\": [\n    \"Provide more detailed quantitative results in the main paper, including comparisons with strong baselines.\",\n    \"Include analysis of computational overhead introduced by MM-LoRA and QLadder.\",\n    \"Discuss failure cases or scenarios where the proposed techniques do not yield improvements.\",\n    \"Clarify if the approach generalizes to domains outside of the commonly used CLIP image encoder.\"\n  ],\n  \"summary\": \"This work introduces Arcana, a multimodal LLM with two novel modules\u2014MM-LoRA for disentangled modality adaptation and QLadder for improved visual encoding. The approach is well-grounded, and initial claims of empirical success are promising. However, the abstract lacks specific experimental details, comparative analysis, and discussion of limitations. Clarification of these aspects and inclusion of quantitative results will strengthen the contribution.\"\n}", "usage": {"prompt_tokens": 317, "completion_tokens": 354, "total_tokens": 671}, "timestamp": "2025-12-29T07:04:36.851954"}
{"paper_id": "yIKjkRZBrX", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses important limitation of fixed-length skills in RL skill learning.", "Novel use of a state-action novelty module for decision point identification.", "Demonstrates clear empirical improvements over prior baselines on complex, long-horizon tasks.", "General applicability to various domains (robotic manipulation, maze navigation).", "Highlights an understudied aspect\u2014variable granularity of decision points\u2014for more efficient learning."], "weaknesses": ["Details regarding the novelty module\u2019s design and training are not provided in the abstract.", "Limited discussion of failure cases or limitations of the proposed method.", "Potential computational overhead of decision point identification is not addressed.", "Unclear if transfer of knowledge across tasks is significant or only anecdotal.", "Evaluation dependent on empirical performance\u2014no theoretical guarantees discussed."], "suggestions": ["Clarify the architectural and training specifics of the state-action novelty module.", "Include ablation studies to show the unique contribution of decision point identification.", "Discuss the computational costs associated with the novelty-based module.", "Explore more quantitative comparisons of knowledge transfer across diverse tasks.", "Provide analysis or discussion of scenarios where the method may fail or underperform."], "summary": "The paper introduces a method for learning variable-length skills via decision point identification using a state-action novelty module, addressing weaknesses of fixed-length skills in policy learning. The proposed NBDI approach shows promising empirical results on long-horizon RL tasks, but the work would benefit from more detailed methodological exposition, discussion of failure modes, and quantitative evidence of knowledge transfer."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses important limitation of fixed-length skills in RL skill learning.\",\n    \"Novel use of a state-action novelty module for decision point identification.\",\n    \"Demonstrates clear empirical improvements over prior baselines on complex, long-horizon tasks.\",\n    \"General applicability to various domains (robotic manipulation, maze navigation).\",\n    \"Highlights an understudied aspect\u2014variable granularity of decision points\u2014for more efficient learning.\"\n  ],\n  \"weaknesses\": [\n    \"Details regarding the novelty module\u2019s design and training are not provided in the abstract.\",\n    \"Limited discussion of failure cases or limitations of the proposed method.\",\n    \"Potential computational overhead of decision point identification is not addressed.\",\n    \"Unclear if transfer of knowledge across tasks is significant or only anecdotal.\",\n    \"Evaluation dependent on empirical performance\u2014no theoretical guarantees discussed.\"\n  ],\n  \"suggestions\": [\n    \"Clarify the architectural and training specifics of the state-action novelty module.\",\n    \"Include ablation studies to show the unique contribution of decision point identification.\",\n    \"Discuss the computational costs associated with the novelty-based module.\",\n    \"Explore more quantitative comparisons of knowledge transfer across diverse tasks.\",\n    \"Provide analysis or discussion of scenarios where the method may fail or underperform.\"\n  ],\n  \"summary\": \"The paper introduces a method for learning variable-length skills via decision point identification using a state-action novelty module, addressing weaknesses of fixed-length skills in policy learning. The proposed NBDI approach shows promising empirical results on long-horizon RL tasks, but the work would benefit from more detailed methodological exposition, discussion of failure modes, and quantitative evidence of knowledge transfer.\"\n}", "usage": {"prompt_tokens": 312, "completion_tokens": 346, "total_tokens": 658}, "timestamp": "2025-12-29T07:04:47.776854"}
{"paper_id": "GkWA6NjePN", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Addresses a fundamental challenge of cooperation in multi-agent learning with self-interested agents.", "Proposes a novel, unbiased, higher-derivative-free policy gradient algorithm enabling learning-aware reinforcement learning.", "Leverages sequence models to process long observation histories, enabling policies to capture and exploit the learning dynamics of other agents.", "Empirical results show improvements in cooperative behavior and returns on standard social dilemmas, including temporally-extended coordination tasks.", "Provides a theoretical explanation for emergence of cooperation among learning-aware agents."], "weaknesses": ["Algorithmic details and precise comparisons to prior learning-aware methods are unclear from the abstract.", "Lack of discussion regarding computational complexity and scalability as history length grows.", "Generalization to larger numbers of agents or more complex environments is not addressed.", "No ablation studies mentioned to isolate impact of the policy gradient novelty versus sequence modeling."], "suggestions": ["Provide clearer algorithmic details and explicit differences with prior learning-aware policy gradient approaches.", "Include scalability analysis, both computationally and in terms of learning stability, as observation history length increases.", "Evaluate or discuss performance in settings with more agents and more complex or partially observed environments.", "Present ablation experiments to clarify the impact of each component (e.g., derivative-free technique vs. history modeling) on cooperation.", "Release code and environments to facilitate reproducibility."], "summary": "The paper contributes a novel policy gradient algorithm enabling learning-aware agents to achieve cooperation in multi-agent reinforcement learning settings. By leveraging efficient sequence models and conditioning on long observation histories, the method addresses key challenges in social dilemmas and demonstrates improved cooperative behavior. The theoretical analysis and empirical results are promising, though more detail and evaluation are needed to fully assess scalability, generalization, and component contributions."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses a fundamental challenge of cooperation in multi-agent learning with self-interested agents.\",\n    \"Proposes a novel, unbiased, higher-derivative-free policy gradient algorithm enabling learning-aware reinforcement learning.\",\n    \"Leverages sequence models to process long observation histories, enabling policies to capture and exploit the learning dynamics of other agents.\",\n    \"Empirical results show improvements in cooperative behavior and returns on standard social dilemmas, including temporally-extended coordination tasks.\",\n    \"Provides a theoretical explanation for emergence of cooperation among learning-aware agents.\"\n  ],\n  \"weaknesses\": [\n    \"Algorithmic details and precise comparisons to prior learning-aware methods are unclear from the abstract.\",\n    \"Lack of discussion regarding computational complexity and scalability as history length grows.\",\n    \"Generalization to larger numbers of agents or more complex environments is not addressed.\",\n    \"No ablation studies mentioned to isolate impact of the policy gradient novelty versus sequence modeling.\"\n  ],\n  \"suggestions\": [\n    \"Provide clearer algorithmic details and explicit differences with prior learning-aware policy gradient approaches.\",\n    \"Include scalability analysis, both computationally and in terms of learning stability, as observation history length increases.\",\n    \"Evaluate or discuss performance in settings with more agents and more complex or partially observed environments.\",\n    \"Present ablation experiments to clarify the impact of each component (e.g., derivative-free technique vs. history modeling) on cooperation.\",\n    \"Release code and environments to facilitate reproducibility.\"\n  ],\n  \"summary\": \"The paper contributes a novel policy gradient algorithm enabling learning-aware agents to achieve cooperation in multi-agent reinforcement learning settings. By leveraging efficient sequence models and conditioning on long observation histories, the method addresses key challenges in social dilemmas and demonstrates improved cooperative behavior. The theoretical analysis and empirical results are promising, though more detail and evaluation are needed to fully assess scalability, generalization, and component contributions.\"\n}", "usage": {"prompt_tokens": 260, "completion_tokens": 393, "total_tokens": 653}, "timestamp": "2025-12-29T07:05:00.793418"}
{"paper_id": "b42wmsdwmB", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses an important practical limitation in current multimodal human sensing\u2014rigidity to modality changes.", "Novel X-fusion mechanism for flexible integration of arbitrary modality combinations.", "Transformer architecture enables variable input size, supporting extensibility.", "Strong empirical results on multiple datasets and tasks (HPE and HAR), with six distinct modalities.", "No need for retraining when modalities are added or removed, improving usability."], "weaknesses": ["Details of the X-fusion mechanism are insufficiently explained in the abstract\u2014is the novelty in architecture, training, or both?", "Limited discussion on compute/resource costs, especially with increased modalities at inference.", "Generalization to unseen modalities is not explicitly addressed.", "Experimental results, while stated as SOTA, lack quantitative details and ablation studies in abstract."], "suggestions": ["Clearly articulate and compare the architectural details and benefits of X-fusion versus prior fusion techniques.", "Include more specifics on performance (quantitative gains, efficiency) and ablations in the main paper.", "Discuss the scalability in terms of compute and real-world constraints when more modalities are used.", "Demonstrate generalization to modalities not seen during training, if possible."], "summary": "This paper introduces X-Fi, a flexible and modality-invariant foundation model for multimodal human sensing, addressing rigidity in existing fusion approaches. Its transformer-based design and novel X-fusion mechanism allow seamless addition or removal of modalities without retraining, with strong experimental performance. Greater clarity on technical components and experimental analysis would strengthen the work."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses an important practical limitation in current multimodal human sensing\u2014rigidity to modality changes.\",\n    \"Novel X-fusion mechanism for flexible integration of arbitrary modality combinations.\",\n    \"Transformer architecture enables variable input size, supporting extensibility.\",\n    \"Strong empirical results on multiple datasets and tasks (HPE and HAR), with six distinct modalities.\",\n    \"No need for retraining when modalities are added or removed, improving usability.\"\n  ],\n  \"weaknesses\": [\n    \"Details of the X-fusion mechanism are insufficiently explained in the abstract\u2014is the novelty in architecture, training, or both?\",\n    \"Limited discussion on compute/resource costs, especially with increased modalities at inference.\",\n    \"Generalization to unseen modalities is not explicitly addressed.\",\n    \"Experimental results, while stated as SOTA, lack quantitative details and ablation studies in abstract.\"\n  ],\n  \"suggestions\": [\n    \"Clearly articulate and compare the architectural details and benefits of X-fusion versus prior fusion techniques.\",\n    \"Include more specifics on performance (quantitative gains, efficiency) and ablations in the main paper.\",\n    \"Discuss the scalability in terms of compute and real-world constraints when more modalities are used.\",\n    \"Demonstrate generalization to modalities not seen during training, if possible.\"\n  ],\n  \"summary\": \"This paper introduces X-Fi, a flexible and modality-invariant foundation model for multimodal human sensing, addressing rigidity in existing fusion approaches. Its transformer-based design and novel X-fusion mechanism allow seamless addition or removal of modalities without retraining, with strong experimental performance. Greater clarity on technical components and experimental analysis would strengthen the work.\"\n}", "usage": {"prompt_tokens": 328, "completion_tokens": 346, "total_tokens": 674}, "timestamp": "2025-12-29T07:05:13.175835"}
{"paper_id": "3wde105NL2", "model": "openai/gpt-4.1", "response": {"score": 1, "strengths": [], "weaknesses": ["The submission lacks substantive content.", "Neither the title nor the abstract provide information about the research problem, methods, or contributions."], "suggestions": ["Provide a clear and descriptive title.", "Include a meaningful abstract summarizing the research motivation, methodology, results, and contributions."], "summary": "The submission does not provide any content for evaluation. Please update the title and abstract to reflect the research and its contributions."}, "raw": "{\n  \"score\": 1,\n  \"strengths\": [],\n  \"weaknesses\": [\n    \"The submission lacks substantive content.\",\n    \"Neither the title nor the abstract provide information about the research problem, methods, or contributions.\"\n  ],\n  \"suggestions\": [\n    \"Provide a clear and descriptive title.\",\n    \"Include a meaningful abstract summarizing the research motivation, methodology, results, and contributions.\"\n  ],\n  \"summary\": \"The submission does not provide any content for evaluation. Please update the title and abstract to reflect the research and its contributions.\"\n}", "usage": {"prompt_tokens": 66, "completion_tokens": 118, "total_tokens": 184}, "timestamp": "2025-12-29T07:05:22.224664"}
{"paper_id": "4EjdYiNRzE", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Presents an improved convergence rate (O(d/T)) for DDPMs, surpassing prior results.", "Minimal assumptions: only finite first moment for the data distribution; does not require smoothness or tail decay.", "Addresses both theoretical interest and practical relevance for score-based diffusion models.", "New analytical tools for stepwise error propagation analysis.", "Clear link to practical algorithm parameters (dimension d, steps T)."], "weaknesses": ["Relies on availability of \u00057ll_2\u00057-accurate score estimates, which may be challenging in practice.", "Limited discussion on how assumptions compare to real-world data and score learning errors.", "The empirical verification and how these bounds manifest in actual DDPM implementations is not mentioned.", "Potential dependence on unspecified logarithmic factors is hand-waved."], "suggestions": ["Clarify or relax the form/feasibility of l2-accurate score function estimation in practical settings.", "Add discussion or experiments on empirical tightness of the derived bounds for real-world diffusion models.", "Explicitly characterize or bound the ignored logarithmic factors.", "Compare and contrast assumption sharpness and coverage with previous theoretical work.", "Discuss limitations if score estimates have higher-order moments or non-l2 errors."], "summary": "This paper advances the theoretical foundation of denoising diffusion models by proving an O(d/T) convergence rate under minimal and broadly applicable assumptions. The approach reduces barriers found in previous work, but practical aspects of score estimation and empirical realism warrant further attention. The contribution is strong for theory, though some clarifications and connections to practice would strengthen the work."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Presents an improved convergence rate (O(d/T)) for DDPMs, surpassing prior results.\",\n    \"Minimal assumptions: only finite first moment for the data distribution; does not require smoothness or tail decay.\",\n    \"Addresses both theoretical interest and practical relevance for score-based diffusion models.\",\n    \"New analytical tools for stepwise error propagation analysis.\",\n    \"Clear link to practical algorithm parameters (dimension d, steps T).\"\n  ],\n  \"weaknesses\": [\n    \"Relies on availability of \\u00057ll_2\\u00057-accurate score estimates, which may be challenging in practice.\",\n    \"Limited discussion on how assumptions compare to real-world data and score learning errors.\",\n    \"The empirical verification and how these bounds manifest in actual DDPM implementations is not mentioned.\",\n    \"Potential dependence on unspecified logarithmic factors is hand-waved.\"\n  ],\n  \"suggestions\": [\n    \"Clarify or relax the form/feasibility of l2-accurate score function estimation in practical settings.\",\n    \"Add discussion or experiments on empirical tightness of the derived bounds for real-world diffusion models.\",\n    \"Explicitly characterize or bound the ignored logarithmic factors.\",\n    \"Compare and contrast assumption sharpness and coverage with previous theoretical work.\",\n    \"Discuss limitations if score estimates have higher-order moments or non-l2 errors.\"\n  ],\n  \"summary\": \"This paper advances the theoretical foundation of denoising diffusion models by proving an O(d/T) convergence rate under minimal and broadly applicable assumptions. The approach reduces barriers found in previous work, but practical aspects of score estimation and empirical realism warrant further attention. The contribution is strong for theory, though some clarifications and connections to practice would strengthen the work.\"\n}", "usage": {"prompt_tokens": 313, "completion_tokens": 369, "total_tokens": 682}, "timestamp": "2025-12-29T07:05:31.874151"}
{"paper_id": "eJFBMqCE4X", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Presents a unifying theoretical framework bridging discriminative and generative self-supervised learning.", "Offers a graphical model analysis, clarifying implicit assumptions of popular contrastive methods.", "Demonstrates improved performance over previous VAE-based methods on standard benchmarks.", "Highlights the information-preserving potential of generative representations."], "weaknesses": ["Experiments are limited to relatively simple or small-scale benchmarks; lack results on challenging datasets (e.g., ImageNet).", "Comparative improvement is shown only over previous VAEs, while the remaining gap to state-of-the-art discriminative methods appears unresolved.", "Limited discussion on scalability and computational cost of the proposed framework.", "Practical utility and transfer to diverse downstream tasks remain unclear beyond the listed benchmarks."], "suggestions": ["Extend empirical evaluation to larger, more challenging datasets (e.g., ImageNet) to better assess scalability and competitiveness.", "Provide direct quantitative comparison to state-of-the-art discriminative methods, clarifying the remaining gap.", "Include analysis of downstream task transfer (e.g., object detection, segmentation) beyond classification.", "Discuss computational complexity and training stability of the proposed approach."], "summary": "The paper offers a promising theoretical and empirical advance in bridging the gap between discriminative and generative self-supervised representation learning. The unifying framework and improved results over standard VAE baselines are valuable contributions. However, the current scope is limited to small-scale datasets, and further work is necessary to demonstrate practical impact and scalability."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Presents a unifying theoretical framework bridging discriminative and generative self-supervised learning.\",\n    \"Offers a graphical model analysis, clarifying implicit assumptions of popular contrastive methods.\",\n    \"Demonstrates improved performance over previous VAE-based methods on standard benchmarks.\",\n    \"Highlights the information-preserving potential of generative representations.\"\n  ],\n  \"weaknesses\": [\n    \"Experiments are limited to relatively simple or small-scale benchmarks; lack results on challenging datasets (e.g., ImageNet).\",\n    \"Comparative improvement is shown only over previous VAEs, while the remaining gap to state-of-the-art discriminative methods appears unresolved.\",\n    \"Limited discussion on scalability and computational cost of the proposed framework.\",\n    \"Practical utility and transfer to diverse downstream tasks remain unclear beyond the listed benchmarks.\"\n  ],\n  \"suggestions\": [\n    \"Extend empirical evaluation to larger, more challenging datasets (e.g., ImageNet) to better assess scalability and competitiveness.\",\n    \"Provide direct quantitative comparison to state-of-the-art discriminative methods, clarifying the remaining gap.\",\n    \"Include analysis of downstream task transfer (e.g., object detection, segmentation) beyond classification.\",\n    \"Discuss computational complexity and training stability of the proposed approach.\"\n  ],\n  \"summary\": \"The paper offers a promising theoretical and empirical advance in bridging the gap between discriminative and generative self-supervised representation learning. The unifying framework and improved results over standard VAE baselines are valuable contributions. However, the current scope is limited to small-scale datasets, and further work is necessary to demonstrate practical impact and scalability.\"\n}", "usage": {"prompt_tokens": 337, "completion_tokens": 340, "total_tokens": 677}, "timestamp": "2025-12-29T07:05:43.129690"}
{"paper_id": "7qMrDf9zFU", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses an important challenge in instruction tuning: balancing data quality and diversity.", "Proposes a novel use of noise injection to assess instruction data quality.", "Empirically demonstrates that careful data selection can outperform models trained on the full dataset, using only 12% of the data.", "Considers both inter-class and intra-class diversity, an aspect often overlooked.", "Makes the selected high-quality instruction dataset publicly available."], "weaknesses": ["The definition and operationalization of 'quality' via noise injection are not described in detail in the abstract.", "It's unclear how diversity is measured or enforced during selection.", "Experimental results summary is vague; improvement magnitude and evaluation benchmarks are not specified.", "Methodological novelty of noise injection for data selection, versus prior filtering or scoring techniques, requires better clarification."], "suggestions": ["Provide clearer definitions and algorithmic details for how noise injection quantifies data quality.", "Elaborate on the specific diversity metrics used and how inter- and intra-class diversity are balanced.", "Include quantitative experimental results (e.g., absolute scores, tasks, and baselines) in the main text.", "Compare to more recent data selection and filtering methods to clarify the incremental value.", "Discuss potential limitations or assumptions of using noise injection for data assessment."], "summary": "This paper contributes a promising method for selecting high-quality instruction data using noise injection and diversity-aware strategies. The approach is empirically validated to yield more efficient model tuning, though the exposition lacks crucial methodological and quantitative detail. The work would benefit from clearer explanations of its technical components and direct comparison to related selection techniques."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses an important challenge in instruction tuning: balancing data quality and diversity.\",\n    \"Proposes a novel use of noise injection to assess instruction data quality.\",\n    \"Empirically demonstrates that careful data selection can outperform models trained on the full dataset, using only 12% of the data.\",\n    \"Considers both inter-class and intra-class diversity, an aspect often overlooked.\",\n    \"Makes the selected high-quality instruction dataset publicly available.\"\n  ],\n  \"weaknesses\": [\n    \"The definition and operationalization of 'quality' via noise injection are not described in detail in the abstract.\",\n    \"It's unclear how diversity is measured or enforced during selection.\",\n    \"Experimental results summary is vague; improvement magnitude and evaluation benchmarks are not specified.\",\n    \"Methodological novelty of noise injection for data selection, versus prior filtering or scoring techniques, requires better clarification.\"\n  ],\n  \"suggestions\": [\n    \"Provide clearer definitions and algorithmic details for how noise injection quantifies data quality.\",\n    \"Elaborate on the specific diversity metrics used and how inter- and intra-class diversity are balanced.\",\n    \"Include quantitative experimental results (e.g., absolute scores, tasks, and baselines) in the main text.\",\n    \"Compare to more recent data selection and filtering methods to clarify the incremental value.\",\n    \"Discuss potential limitations or assumptions of using noise injection for data assessment.\"\n  ],\n  \"summary\": \"This paper contributes a promising method for selecting high-quality instruction data using noise injection and diversity-aware strategies. The approach is empirically validated to yield more efficient model tuning, though the exposition lacks crucial methodological and quantitative detail. The work would benefit from clearer explanations of its technical components and direct comparison to related selection techniques.\"\n}", "usage": {"prompt_tokens": 296, "completion_tokens": 363, "total_tokens": 659}, "timestamp": "2025-12-29T07:05:55.913181"}
{"paper_id": "Yk87CwhBDx", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Novel benchmark for assessing LLMs' spatial-semantic reasoning using symbolic graphics programs.", "Procedurally generated, scalable, and minimally human-dependent dataset.", "Clever use of transformations that change program structure while keeping visual semantics invariant.", "Comprehensive evaluation of both commercial and open-source LLMs.", "Introduction of Symbolic Instruction Tuning (SIT) as a practical finetuning approach.", "Demonstration that SIT improves general reasoning beyond the targeted task."], "weaknesses": ["No clear description of benchmark task formats (question types, answer schema, evaluation metrics).", "Limited discussion of failure cases or qualitative error analysis.", "Potential leakage of program types or syntactic patterns from training data not fully addressed.", "Generalizability across non-graphics symbolic domains is claimed but not empirically supported.", "No comparison to models or baselines that do include explicit visual grounding."], "suggestions": ["Provide detailed breakdown of task types, evaluation metrics, and example benchmark items in the main paper.", "Include qualitative analysis of where LLMs succeed/fail, with specific types of reasoning errors.", "Clarify dataset construction process to ensure independence from training data distributions of major LLMs.", "Empirically evaluate SIT's transfer effects on other symbolic domains (e.g., math, code).", "Consider direct comparison to multimodal models or vision-language models, even if only as a reference point."], "summary": "This work proposes a creative and practically useful new benchmark for evaluating large language models' ability to perform spatial-semantic reasoning with symbolic graphics programs. The methodology is sound, and the introduction of Symbolic Instruction Tuning is an interesting contribution, with evidence for improvements in both targeted and general reasoning tasks. However, the manuscript would benefit from more methodological transparency, qualitative insights, and broader empirical comparisons. As it stands, the work is a strong contribution to the empirical evaluation of LLMs, with room for clarification and expansion in future revisions."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Novel benchmark for assessing LLMs' spatial-semantic reasoning using symbolic graphics programs.\",\n    \"Procedurally generated, scalable, and minimally human-dependent dataset.\",\n    \"Clever use of transformations that change program structure while keeping visual semantics invariant.\",\n    \"Comprehensive evaluation of both commercial and open-source LLMs.\",\n    \"Introduction of Symbolic Instruction Tuning (SIT) as a practical finetuning approach.\",\n    \"Demonstration that SIT improves general reasoning beyond the targeted task.\"\n  ],\n  \"weaknesses\": [\n    \"No clear description of benchmark task formats (question types, answer schema, evaluation metrics).\",\n    \"Limited discussion of failure cases or qualitative error analysis.\",\n    \"Potential leakage of program types or syntactic patterns from training data not fully addressed.\",\n    \"Generalizability across non-graphics symbolic domains is claimed but not empirically supported.\",\n    \"No comparison to models or baselines that do include explicit visual grounding.\"\n  ],\n  \"suggestions\": [\n    \"Provide detailed breakdown of task types, evaluation metrics, and example benchmark items in the main paper.\",\n    \"Include qualitative analysis of where LLMs succeed/fail, with specific types of reasoning errors.\",\n    \"Clarify dataset construction process to ensure independence from training data distributions of major LLMs.\",\n    \"Empirically evaluate SIT's transfer effects on other symbolic domains (e.g., math, code).\",\n    \"Consider direct comparison to multimodal models or vision-language models, even if only as a reference point.\"\n  ],\n  \"summary\": \"This work proposes a creative and practically useful new benchmark for evaluating large language models' ability to perform spatial-semantic reasoning with symbolic graphics programs. The methodology is sound, and the introduction of Symbolic Instruction Tuning is an interesting contribution, with evidence for improvements in both targeted and general reasoning tasks. However, the manuscript would benefit from more methodological transparency, qualitative insights, and broader empirical comparisons. As it stands, the work is a strong contribution to the empirical evaluation of LLMs, with room for clarification and expansion in future revisions.\"\n}", "usage": {"prompt_tokens": 421, "completion_tokens": 441, "total_tokens": 862}, "timestamp": "2025-12-29T07:06:10.648886"}
{"paper_id": "kSBIEkHzon", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Addresses a pressing and underexplored challenge in graph foundation models.", "Clearly defined notion of 'task-trees' as learning instances and theoretical motivation.", "Comprehensive empirical evaluation across diverse graphs and tasks.", "Demonstrates strong performance in fine-tuning, in-context, and zero-shot settings.", "Provides both theoretical analysis and practical instantiation."], "weaknesses": ["Limited explanation of task-tree construction and its scalability to real-world graph sizes.", "Theoretical claims depend on certain assumptions that may not hold for all graph types.", "Ablation studies isolating the contribution of each component (e.g., task-tree structure vs. reconstruction objective) are not detailed.", "Potential computational overhead of pretraining on large task-tree collections is not discussed."], "suggestions": ["Expand on the details and limitations of constructing task-trees from very large or dynamic graphs.", "Provide theoretical discussion or empirical analysis on situations where the task-tree assumption may fail.", "Include further ablations or qualitative studies to isolate the benefits of task-trees versus other possible graph units.", "Discuss possible optimizations to reduce computational cost in pretraining."], "summary": "This paper proposes an innovative approach towards graph foundation models by introducing task-trees as units for learning generalities across graphs. The method is well-motivated both theoretically and empirically, showing promising adaptability across domains and tasks. Some details around scalability, ablation, and assumptions could be improved or clarified to further strengthen its impact."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses a pressing and underexplored challenge in graph foundation models.\",\n    \"Clearly defined notion of 'task-trees' as learning instances and theoretical motivation.\",\n    \"Comprehensive empirical evaluation across diverse graphs and tasks.\",\n    \"Demonstrates strong performance in fine-tuning, in-context, and zero-shot settings.\",\n    \"Provides both theoretical analysis and practical instantiation.\"\n  ],\n  \"weaknesses\": [\n    \"Limited explanation of task-tree construction and its scalability to real-world graph sizes.\",\n    \"Theoretical claims depend on certain assumptions that may not hold for all graph types.\",\n    \"Ablation studies isolating the contribution of each component (e.g., task-tree structure vs. reconstruction objective) are not detailed.\",\n    \"Potential computational overhead of pretraining on large task-tree collections is not discussed.\"\n  ],\n  \"suggestions\": [\n    \"Expand on the details and limitations of constructing task-trees from very large or dynamic graphs.\",\n    \"Provide theoretical discussion or empirical analysis on situations where the task-tree assumption may fail.\",\n    \"Include further ablations or qualitative studies to isolate the benefits of task-trees versus other possible graph units.\",\n    \"Discuss possible optimizations to reduce computational cost in pretraining.\"\n  ],\n  \"summary\": \"This paper proposes an innovative approach towards graph foundation models by introducing task-trees as units for learning generalities across graphs. The method is well-motivated both theoretically and empirically, showing promising adaptability across domains and tasks. Some details around scalability, ablation, and assumptions could be improved or clarified to further strengthen its impact.\"\n}", "usage": {"prompt_tokens": 393, "completion_tokens": 338, "total_tokens": 731}, "timestamp": "2025-12-29T07:06:23.044619"}
{"paper_id": "aNuQyV30Yw", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses a clear limitation in existing prompt learning approaches for multi-concept image synthesis.", "Proposes novel regularization techniques (AttnMask, PromptCL, Bind adj.) to improve disentanglement and correlation.", "Extensive and diverse experimental evaluation, including generation, editing, and attention visualization tasks.", "Introduces a new dataset and evaluation protocol tailored to multi-concept prompt learning.", "Quantitative results and ablations appear comprehensive."], "weaknesses": ["Details regarding the new dataset and evaluation protocol are limited in the abstract; generalizability and potential biases are unclear.", "The exact implementation and computational overhead of the proposed regularization techniques are unclear.", "It is not clear how well the method scales to highly complex scenes with many objects.", "Possible confounding due to phrase ambiguity in the prompt, not discussed.", "Limited discussion of negative results or failure modes."], "suggestions": ["Provide a more detailed description and analysis of the introduced dataset and evaluation protocol, including possible biases.", "Offer clearer ablation on the impact of each proposed regularizer, possibly including failure cases.", "Discuss scalability to scenes with more than a few objects and potential limitations in such scenarios.", "Examine impact of ambiguous or complicated natural language prompts.", "Include comparisons with more recent baseline or state-of-the-art prompt learning methods."], "summary": "This paper presents a promising framework for simultaneous learning of multiple object-level concepts in text-to-image synthesis, introducing novel regularization strategies and a new evaluation benchmark. The results demonstrate improved disentanglement and word-concept correlation over prior methods. However, further clarification on dataset details, scalability, ablation studies, and potential limitations would strengthen the submission."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses a clear limitation in existing prompt learning approaches for multi-concept image synthesis.\",\n    \"Proposes novel regularization techniques (AttnMask, PromptCL, Bind adj.) to improve disentanglement and correlation.\",\n    \"Extensive and diverse experimental evaluation, including generation, editing, and attention visualization tasks.\",\n    \"Introduces a new dataset and evaluation protocol tailored to multi-concept prompt learning.\",\n    \"Quantitative results and ablations appear comprehensive.\"\n  ],\n  \"weaknesses\": [\n    \"Details regarding the new dataset and evaluation protocol are limited in the abstract; generalizability and potential biases are unclear.\",\n    \"The exact implementation and computational overhead of the proposed regularization techniques are unclear.\",\n    \"It is not clear how well the method scales to highly complex scenes with many objects.\",\n    \"Possible confounding due to phrase ambiguity in the prompt, not discussed.\",\n    \"Limited discussion of negative results or failure modes.\"\n  ],\n  \"suggestions\": [\n    \"Provide a more detailed description and analysis of the introduced dataset and evaluation protocol, including possible biases.\",\n    \"Offer clearer ablation on the impact of each proposed regularizer, possibly including failure cases.\",\n    \"Discuss scalability to scenes with more than a few objects and potential limitations in such scenarios.\",\n    \"Examine impact of ambiguous or complicated natural language prompts.\",\n    \"Include comparisons with more recent baseline or state-of-the-art prompt learning methods.\"\n  ],\n  \"summary\": \"This paper presents a promising framework for simultaneous learning of multiple object-level concepts in text-to-image synthesis, introducing novel regularization strategies and a new evaluation benchmark. The results demonstrate improved disentanglement and word-concept correlation over prior methods. However, further clarification on dataset details, scalability, ablation studies, and potential limitations would strengthen the submission.\"\n}", "usage": {"prompt_tokens": 318, "completion_tokens": 380, "total_tokens": 698}, "timestamp": "2025-12-29T07:06:36.946221"}
{"paper_id": "pxI5IPeWgW", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Novel approach\u2014introducing closed-form ODEs as alternatives to neural networks for longitudinal heterogeneous treatment effects inference.", "Addresses interpretability, which is a major limitation of neural network-based methods.", "Handles irregular sampling naturally, which is important for longitudinal data.", "General framework that allows adaptation of ODE discovery methods to the treatment effect estimation domain.", "Potential to inspire further innovations in treatment effect methodology."], "weaknesses": ["Lack of empirical validation details in the abstract makes it unclear how performance compares to state-of-the-art neural network methods.", "No discussion on how the method deals with confounding and assignment bias beyond standard assumptions.", "Limited information on the scalability and computational requirements of the approach.", "Possible constraints in modeling highly nonlinear or high-dimensional data compared to neural networks.", "Unclear to what degree identification assumptions differ or weaken compared to established NN-based frameworks."], "suggestions": ["Provide thorough empirical comparisons against recent neural network-based methods on benchmark longitudinal treatment effect tasks.", "Clarify the identification assumptions required for this ODE approach and how they differ from neural network-based methods.", "Include discussion or experiments on handling high-dimensional covariates.", "Expand on computational efficiency and scalability, particularly for large real-world datasets.", "Demonstrate interpretability advantages with concrete case studies or visualizations."], "summary": "The paper presents a novel closed-form ODE-based framework for longitudinal heterogeneous treatment effect inference, challenging the dominance of neural network methods. The approach improves interpretability, manages irregular time sampling, and opens new directions in the field. However, the submission would benefit from stronger empirical evidence, clearer exposition of advantages and limitations compared to neural approaches, and a detailed analysis of assumptions and practical scalability."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Novel approach\u2014introducing closed-form ODEs as alternatives to neural networks for longitudinal heterogeneous treatment effects inference.\",\n    \"Addresses interpretability, which is a major limitation of neural network-based methods.\",\n    \"Handles irregular sampling naturally, which is important for longitudinal data.\",\n    \"General framework that allows adaptation of ODE discovery methods to the treatment effect estimation domain.\",\n    \"Potential to inspire further innovations in treatment effect methodology.\"\n  ],\n  \"weaknesses\": [\n    \"Lack of empirical validation details in the abstract makes it unclear how performance compares to state-of-the-art neural network methods.\",\n    \"No discussion on how the method deals with confounding and assignment bias beyond standard assumptions.\",\n    \"Limited information on the scalability and computational requirements of the approach.\",\n    \"Possible constraints in modeling highly nonlinear or high-dimensional data compared to neural networks.\",\n    \"Unclear to what degree identification assumptions differ or weaken compared to established NN-based frameworks.\"\n  ],\n  \"suggestions\": [\n    \"Provide thorough empirical comparisons against recent neural network-based methods on benchmark longitudinal treatment effect tasks.\",\n    \"Clarify the identification assumptions required for this ODE approach and how they differ from neural network-based methods.\",\n    \"Include discussion or experiments on handling high-dimensional covariates.\",\n    \"Expand on computational efficiency and scalability, particularly for large real-world datasets.\",\n    \"Demonstrate interpretability advantages with concrete case studies or visualizations.\"\n  ],\n  \"summary\": \"The paper presents a novel closed-form ODE-based framework for longitudinal heterogeneous treatment effect inference, challenging the dominance of neural network methods. The approach improves interpretability, manages irregular time sampling, and opens new directions in the field. However, the submission would benefit from stronger empirical evidence, clearer exposition of advantages and limitations compared to neural approaches, and a detailed analysis of assumptions and practical scalability.\"\n}", "usage": {"prompt_tokens": 304, "completion_tokens": 386, "total_tokens": 690}, "timestamp": "2025-12-29T07:06:50.310340"}
{"paper_id": "LPfLsSqrQJ", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Novel decomposition-incorporation framework leveraging spatio-temporal knowledge from LLMs.", "Explicit decoupling of spatial and temporal knowledge provides new context beyond action names.", "Well-designed SKC (Spatial Knowledge Compensator) and TKC (Temporal Knowledge Compensator) modules.", "Experimental results demonstrate state-of-the-art performance on four standard FSAR datasets.", "The approach adds interpretability through multi-granularity prototypes."], "weaknesses": ["Details on how spatio-temporal attributes are extracted from LLMs are unclear.", "Ablation studies on the contribution of SKC vs. TKC modules are not discussed in the abstract.", "Potential increase in annotation complexity (e.g., for attributes) is not addressed.", "Some novelty may be incremental given similar methods of LLM-based knowledge injection.", "No mention of computational overhead or inference efficiency."], "suggestions": ["Provide detailed methodology on extracting and curating spatial and temporal attributes from LLMs.", "Add ablation experiments to quantify the individual impact of SKC and TKC.", "Discuss the computational overhead and feasibility for real-world applications.", "Clarify whether manual effort is needed in attribute acquisition.", "Compare with prior LLM-based FSAR approaches in terms of both performance and efficiency."], "summary": "This paper introduces DiST, a framework that improves few-shot action recognition by leveraging spatio-temporal attributes sourced via LLMs, and processing these through dedicated spatial and temporal compensator modules. The decoupling and use of multi-granular prototypes is innovative and achieves strong empirical results, but further detail on methodology, ablation, and efficiency analysis would strengthen the work."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Novel decomposition-incorporation framework leveraging spatio-temporal knowledge from LLMs.\",\n    \"Explicit decoupling of spatial and temporal knowledge provides new context beyond action names.\",\n    \"Well-designed SKC (Spatial Knowledge Compensator) and TKC (Temporal Knowledge Compensator) modules.\",\n    \"Experimental results demonstrate state-of-the-art performance on four standard FSAR datasets.\",\n    \"The approach adds interpretability through multi-granularity prototypes.\"\n  ],\n  \"weaknesses\": [\n    \"Details on how spatio-temporal attributes are extracted from LLMs are unclear.\",\n    \"Ablation studies on the contribution of SKC vs. TKC modules are not discussed in the abstract.\",\n    \"Potential increase in annotation complexity (e.g., for attributes) is not addressed.\",\n    \"Some novelty may be incremental given similar methods of LLM-based knowledge injection.\",\n    \"No mention of computational overhead or inference efficiency.\"\n  ],\n  \"suggestions\": [\n    \"Provide detailed methodology on extracting and curating spatial and temporal attributes from LLMs.\",\n    \"Add ablation experiments to quantify the individual impact of SKC and TKC.\",\n    \"Discuss the computational overhead and feasibility for real-world applications.\",\n    \"Clarify whether manual effort is needed in attribute acquisition.\",\n    \"Compare with prior LLM-based FSAR approaches in terms of both performance and efficiency.\"\n  ],\n  \"summary\": \"This paper introduces DiST, a framework that improves few-shot action recognition by leveraging spatio-temporal attributes sourced via LLMs, and processing these through dedicated spatial and temporal compensator modules. The decoupling and use of multi-granular prototypes is innovative and achieves strong empirical results, but further detail on methodology, ablation, and efficiency analysis would strengthen the work.\"\n}", "usage": {"prompt_tokens": 431, "completion_tokens": 380, "total_tokens": 811}, "timestamp": "2025-12-29T07:07:02.663971"}
{"paper_id": "gHLWTzKiZV", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Proposes a novel generalization of Flow Matching, Unbalanced Flow Matching, specifically tailored for molecular docking tasks.", "Addresses important challenges in the field: modeling protein flexibility and avoiding nonphysical poses.", "Empirical results demonstrate a significant improvement in energetically favorable pose generation (from 30% to 73% on PDBBind).", "Method is applicable to both flexible docking and structure relaxation, indicating versatility."], "weaknesses": ["The paper lacks detail on computational cost and efficiency of Unbalanced FM compared to baselines.", "Unclear how sensitive the method is to hyperparameters controlling the trade-off between sample efficiency and approximation accuracy.", "Limited discussion on generalizability to datasets beyond PDBBind.", "Ablation studies separating the contribution of protein flexibility modeling from other innovations are not mentioned."], "suggestions": ["Provide additional analysis on computational requirements and scalability for large-scale docking tasks.", "Include hyperparameter sensitivity analysis to demonstrate robustness.", "Expand empirical evaluation to other datasets and tasks within molecular modeling.", "Add ablation studies to clarify contribution of specific method components, particularly the flexibility modeling."], "summary": "The paper introduces Unbalanced Flow Matching, offering a principled and practical advance for molecular docking\u2014most notably improving the proportion of energetically favorable poses via better handling of protein flexibility and complex distribution transport. While empirical results are strong, the study would benefit from broader validation, computational analysis, and finer-grained ablations to fully establish its contributions."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Proposes a novel generalization of Flow Matching, Unbalanced Flow Matching, specifically tailored for molecular docking tasks.\",\n    \"Addresses important challenges in the field: modeling protein flexibility and avoiding nonphysical poses.\",\n    \"Empirical results demonstrate a significant improvement in energetically favorable pose generation (from 30% to 73% on PDBBind).\",\n    \"Method is applicable to both flexible docking and structure relaxation, indicating versatility.\"\n  ],\n  \"weaknesses\": [\n    \"The paper lacks detail on computational cost and efficiency of Unbalanced FM compared to baselines.\",\n    \"Unclear how sensitive the method is to hyperparameters controlling the trade-off between sample efficiency and approximation accuracy.\",\n    \"Limited discussion on generalizability to datasets beyond PDBBind.\",\n    \"Ablation studies separating the contribution of protein flexibility modeling from other innovations are not mentioned.\"\n  ],\n  \"suggestions\": [\n    \"Provide additional analysis on computational requirements and scalability for large-scale docking tasks.\",\n    \"Include hyperparameter sensitivity analysis to demonstrate robustness.\",\n    \"Expand empirical evaluation to other datasets and tasks within molecular modeling.\",\n    \"Add ablation studies to clarify contribution of specific method components, particularly the flexibility modeling.\"\n  ],\n  \"summary\": \"The paper introduces Unbalanced Flow Matching, offering a principled and practical advance for molecular docking\u2014most notably improving the proportion of energetically favorable poses via better handling of protein flexibility and complex distribution transport. While empirical results are strong, the study would benefit from broader validation, computational analysis, and finer-grained ablations to fully establish its contributions.\"\n}", "usage": {"prompt_tokens": 237, "completion_tokens": 335, "total_tokens": 572}, "timestamp": "2025-12-29T07:07:18.074656"}
{"paper_id": "nzvoDKEvU1", "model": "openai/gpt-4.1", "response": {"score": 6, "strengths": ["Addresses an important VR problem: personalized, real-time hand meshes.", "Reduces dependence on large datasets and specialized equipment.", "Introduces a novel pose approximation algorithm and silhouette-guided loss.", "Demonstrates real-time inference capability.", "Application for resource-constrained or at-home scenarios is clear."], "weaknesses": ["Empirical evaluation details are missing (quantitative results, baselines, or ablation studies not described).", "Reported artefacts in extreme poses indicate model limitations.", "Limited discussion of generalization beyond grasping poses/trained scenarios.", "Insufficient clarity on how much performance is preserved vs. full-data DHM."], "suggestions": ["Provide quantitative comparisons with DHM and other baselines on accuracy, realism, and dataset size.", "Detail how the silhouette-based loss contributes through ablation studies.", "Clarify to what extent artefacts impact practical utility and whether post-processing can help.", "Discuss potential generalization to unseen poses or other hand scans.", "Include user studies or perceptual metrics if realism is a primary concern."], "summary": "The paper introduces a promising approach for learning subject-specific hand meshes from limited data using novel weak supervision. The contribution is timely for resource-constrained VR applications. However, the empirical evaluation is not clearly described, the model shows limits in extreme poses, and additional analysis would strengthen the claim that practical utility is retained with much less data."}, "raw": "{\n  \"score\": 6,\n  \"strengths\": [\n    \"Addresses an important VR problem: personalized, real-time hand meshes.\",\n    \"Reduces dependence on large datasets and specialized equipment.\",\n    \"Introduces a novel pose approximation algorithm and silhouette-guided loss.\",\n    \"Demonstrates real-time inference capability.\",\n    \"Application for resource-constrained or at-home scenarios is clear.\"\n  ],\n  \"weaknesses\": [\n    \"Empirical evaluation details are missing (quantitative results, baselines, or ablation studies not described).\",\n    \"Reported artefacts in extreme poses indicate model limitations.\",\n    \"Limited discussion of generalization beyond grasping poses/trained scenarios.\",\n    \"Insufficient clarity on how much performance is preserved vs. full-data DHM.\"\n  ],\n  \"suggestions\": [\n    \"Provide quantitative comparisons with DHM and other baselines on accuracy, realism, and dataset size.\",\n    \"Detail how the silhouette-based loss contributes through ablation studies.\",\n    \"Clarify to what extent artefacts impact practical utility and whether post-processing can help.\",\n    \"Discuss potential generalization to unseen poses or other hand scans.\",\n    \"Include user studies or perceptual metrics if realism is a primary concern.\"\n  ],\n  \"summary\": \"The paper introduces a promising approach for learning subject-specific hand meshes from limited data using novel weak supervision. The contribution is timely for resource-constrained VR applications. However, the empirical evaluation is not clearly described, the model shows limits in extreme poses, and additional analysis would strengthen the claim that practical utility is retained with much less data.\"\n}", "usage": {"prompt_tokens": 282, "completion_tokens": 325, "total_tokens": 607}, "timestamp": "2025-12-29T07:07:29.569918"}
{"paper_id": "LvuSFvGShf", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Novel analysis of the internal dynamics in LLMs, specifically regarding the role of 'massive weights' in decoding.", "Clear identification of crucial parameters (top-k massive weights) that influence model function and performance.", "Introduction of a simple, practical method (MacDrop) to regularize fine-tuning, grounded in empirical observations.", "Empirical validation with reported improvements on zero-shot and generation tasks.", "The findings have impactful implications for model compression and parameter-efficient training."], "weaknesses": ["Experiments lack sufficient diversity in tasks and data domains to confirm generalizability.", "Methodological details such as precise selection criteria for 'massive weights,' how k is chosen, and ablation on MacDrop schedule are underexplored in the abstract.", "Potential confounders (e.g., impact of architecture choice, layer selection, initialization) are not discussed.", "No direct comparison with other pruning or regularization techniques is mentioned.", "The abstract does not clarify whether the efficiency improvements come with any trade-offs (e.g., inference speed, memory), or the scalability for larger models."], "suggestions": ["Expand experiments to cover more architectures, tasks, and datasets to validate generality.", "Provide stronger baselines and comparisons, including other pruning, dropout, or regularization methods.", "Elaborate in the paper on the sensitivity of MacDrop to hyperparameters (dropout rate, k value, layer selection).", "Discuss potential negative side effects, such as the risk of catastrophic forgetting or reduced interpretability.", "Include analysis or visualization that explains why massive weights become dominant during pre-training."], "summary": "The paper offers a novel and insightful analysis of parameter importance in LLMs, identifying 'massive weights' as disproportionately crucial to model function. The MacDrop regularization approach, inspired by these findings, is promising and shows empirical gains. However, the work would benefit from broader experimental validation, detailed methodological exposition, and more direct comparisons to existing techniques. Overall, this is a solid and intriguing contribution, but further clarification and experimentation are needed to fully establish its impact and generality."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Novel analysis of the internal dynamics in LLMs, specifically regarding the role of 'massive weights' in decoding.\",\n    \"Clear identification of crucial parameters (top-k massive weights) that influence model function and performance.\",\n    \"Introduction of a simple, practical method (MacDrop) to regularize fine-tuning, grounded in empirical observations.\",\n    \"Empirical validation with reported improvements on zero-shot and generation tasks.\",\n    \"The findings have impactful implications for model compression and parameter-efficient training.\"\n  ],\n  \"weaknesses\": [\n    \"Experiments lack sufficient diversity in tasks and data domains to confirm generalizability.\",\n    \"Methodological details such as precise selection criteria for 'massive weights,' how k is chosen, and ablation on MacDrop schedule are underexplored in the abstract.\",\n    \"Potential confounders (e.g., impact of architecture choice, layer selection, initialization) are not discussed.\",\n    \"No direct comparison with other pruning or regularization techniques is mentioned.\",\n    \"The abstract does not clarify whether the efficiency improvements come with any trade-offs (e.g., inference speed, memory), or the scalability for larger models.\"\n  ],\n  \"suggestions\": [\n    \"Expand experiments to cover more architectures, tasks, and datasets to validate generality.\",\n    \"Provide stronger baselines and comparisons, including other pruning, dropout, or regularization methods.\",\n    \"Elaborate in the paper on the sensitivity of MacDrop to hyperparameters (dropout rate, k value, layer selection).\",\n    \"Discuss potential negative side effects, such as the risk of catastrophic forgetting or reduced interpretability.\",\n    \"Include analysis or visualization that explains why massive weights become dominant during pre-training.\"\n  ],\n  \"summary\": \"The paper offers a novel and insightful analysis of parameter importance in LLMs, identifying 'massive weights' as disproportionately crucial to model function. The MacDrop regularization approach, inspired by these findings, is promising and shows empirical gains. However, the work would benefit from broader experimental validation, detailed methodological exposition, and more direct comparisons to existing techniques. Overall, this is a solid and intriguing contribution, but further clarification and experimentation are needed to fully establish its impact and generality.\"\n}", "usage": {"prompt_tokens": 361, "completion_tokens": 467, "total_tokens": 828}, "timestamp": "2025-12-29T07:07:54.188190"}
{"paper_id": "WRxCuhTMB2", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Addresses an under-explored yet important problem of evaluating uncertainty disentanglement in ML regression.", "Proposes a systematic methodology with new indicators for evaluation, filling a gap where ground truth is unavailable.", "Covers both synthetic and real-world datasets, enhancing practical relevance.", "Benchmarks multiple state-of-the-art UQ approaches, providing a comparative view.", "Offers reproducibility via public code and data."], "weaknesses": ["The abstract does not specify the nature or theoretical justification of the proposed indicators, making novelty and rigor unclear.", "Details on the epistemic variability injection mechanism are lacking.", "Limited discussion of the limitations or failure cases of the methodology.", "Evaluation is restricted to gas demand datasets, possibly narrowing generalizability.", "Absent consideration of how results transfer to other regression tasks or domains."], "suggestions": ["Clarify how the proposed indicators are defined and validated\u2014do they rigorously correlate with meaningful disentanglement?", "Provide more detail on the epistemic variability injection mechanism, and justify its design.", "Discuss limitations of the indicators and methodology, such as known or potential failure modes.", "Add experiments or commentary on other datasets/domains to assess generalizability.", "Where possible, include analysis comparing with alternative evaluation strategies or existing disentanglement metrics."], "summary": "This paper addresses a pressing need for robust methodologies to assess uncertainty disentanglement in regression models, proposing new evaluation indicators and an empirical benchmarking framework. It shows promise in both synthetic and real-world use-cases and contributes code for reproducibility. However, the technical novelty and robustness of the evaluation deserve more clarification, especially regarding the proposed indicators and generalizability across domains."}, "raw": "{\n  \"score\": 7,\n  \"strengths\": [\n    \"Addresses an under-explored yet important problem of evaluating uncertainty disentanglement in ML regression.\",\n    \"Proposes a systematic methodology with new indicators for evaluation, filling a gap where ground truth is unavailable.\",\n    \"Covers both synthetic and real-world datasets, enhancing practical relevance.\",\n    \"Benchmarks multiple state-of-the-art UQ approaches, providing a comparative view.\",\n    \"Offers reproducibility via public code and data.\"\n  ],\n  \"weaknesses\": [\n    \"The abstract does not specify the nature or theoretical justification of the proposed indicators, making novelty and rigor unclear.\",\n    \"Details on the epistemic variability injection mechanism are lacking.\",\n    \"Limited discussion of the limitations or failure cases of the methodology.\",\n    \"Evaluation is restricted to gas demand datasets, possibly narrowing generalizability.\",\n    \"Absent consideration of how results transfer to other regression tasks or domains.\"\n  ],\n  \"suggestions\": [\n    \"Clarify how the proposed indicators are defined and validated\u2014do they rigorously correlate with meaningful disentanglement?\",\n    \"Provide more detail on the epistemic variability injection mechanism, and justify its design.\",\n    \"Discuss limitations of the indicators and methodology, such as known or potential failure modes.\",\n    \"Add experiments or commentary on other datasets/domains to assess generalizability.\",\n    \"Where possible, include analysis comparing with alternative evaluation strategies or existing disentanglement metrics.\"\n  ],\n  \"summary\": \"This paper addresses a pressing need for robust methodologies to assess uncertainty disentanglement in regression models, proposing new evaluation indicators and an empirical benchmarking framework. It shows promise in both synthetic and real-world use-cases and contributes code for reproducibility. However, the technical novelty and robustness of the evaluation deserve more clarification, especially regarding the proposed indicators and generalizability across domains.\"\n}", "usage": {"prompt_tokens": 346, "completion_tokens": 378, "total_tokens": 724}, "timestamp": "2025-12-29T07:08:05.917571"}
{"paper_id": "PacBhLzeGO", "model": "openai/gpt-4.1", "response": {"score": 7, "strengths": ["Novel perspective on leveraging degradation labels as weak supervision for universal image restoration pre-training.", "Demonstrates applicability to both CNN and transformer architectures.", "Shows strong empirical gains (up to 2.55 dB and 6.53 dB) on competitive benchmarks.", "Method is simple, resource-efficient, and compatible with existing architectures.", "Public release of code/models encourages reproducibility."], "weaknesses": ["Limited details on experiment setup and the diversity of the degradation types used.", "Comparisons to other pre-training methods could be more extensive and rigorous.", "Ablation studies (e.g., decoder architecture, amount of supervision used) are not mentioned.", "Motivational justification for why degradation classification is useful for downstream restoration could be stronger.", "Clarity lacking on how transfer learning is facilitated (limited mechanism explanation)."], "suggestions": ["Provide more quantitative comparisons to state-of-the-art self-supervised pre-training methods on multiple restoration benchmarks.", "Expand ablation analysis to cover the impact of the decoder choice and degradation set.", "Clarify and theoretically motivate why degradation classification pre-training improves restoration performance.", "Discuss potential limitations, such as sensitivity to the number and type of degradation classes.", "Include qualitative examples illustrating the benefit of DCPT over baselines."], "summary": "This paper introduces an intriguing method for universal image restoration pre-training via degradation classification, showing clear empirical benefits and practical simplicity. However, the presentation would benefit from deeper experimental analysis, more rigorous comparative evaluation, and expanded theoretical motivation. The approach is promising and relevant, with minor revisions likely to improve both its clarity and impact."}, "raw": "{\n    \"score\": 7,\n    \"strengths\": [\n        \"Novel perspective on leveraging degradation labels as weak supervision for universal image restoration pre-training.\",\n        \"Demonstrates applicability to both CNN and transformer architectures.\",\n        \"Shows strong empirical gains (up to 2.55 dB and 6.53 dB) on competitive benchmarks.\",\n        \"Method is simple, resource-efficient, and compatible with existing architectures.\",\n        \"Public release of code/models encourages reproducibility.\"\n    ],\n    \"weaknesses\": [\n        \"Limited details on experiment setup and the diversity of the degradation types used.\",\n        \"Comparisons to other pre-training methods could be more extensive and rigorous.\",\n        \"Ablation studies (e.g., decoder architecture, amount of supervision used) are not mentioned.\",\n        \"Motivational justification for why degradation classification is useful for downstream restoration could be stronger.\",\n        \"Clarity lacking on how transfer learning is facilitated (limited mechanism explanation).\"\n    ],\n    \"suggestions\": [\n        \"Provide more quantitative comparisons to state-of-the-art self-supervised pre-training methods on multiple restoration benchmarks.\",\n        \"Expand ablation analysis to cover the impact of the decoder choice and degradation set.\",\n        \"Clarify and theoretically motivate why degradation classification pre-training improves restoration performance.\",\n        \"Discuss potential limitations, such as sensitivity to the number and type of degradation classes.\",\n        \"Include qualitative examples illustrating the benefit of DCPT over baselines.\"\n    ],\n    \"summary\": \"This paper introduces an intriguing method for universal image restoration pre-training via degradation classification, showing clear empirical benefits and practical simplicity. However, the presentation would benefit from deeper experimental analysis, more rigorous comparative evaluation, and expanded theoretical motivation. The approach is promising and relevant, with minor revisions likely to improve both its clarity and impact.\"\n}", "usage": {"prompt_tokens": 369, "completion_tokens": 367, "total_tokens": 736}, "timestamp": "2025-12-29T07:08:18.217789"}
{"paper_id": "QfyZ28FpVY", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Addresses two major sources of noise (distribution noise and systematic shift) in DEL data.", "Proposes a novel dual-perspective ranking strategy (Pair-wise Soft Rank and List-wise Global Rank) for denoising.", "Introduces a biologically-motivated Activity-Referenced Correction (ARC) module for bridging the gap between read counts and binding affinity.", "Releases three comprehensive and experimentally validated DEL datasets including 2D, 3D, and activity labels, which is valuable for the community.", "Demonstrates state-of-the-art performance with strong generalization across diverse protein targets.", "Provides evidence of interpretability by identifying key functional groups associated with binding."], "weaknesses": ["Details on the implementation and computational complexity of PSR and LGR constraints are limited in the abstract.", "Evaluation details (e.g., baselines compared, statistical significance) are unclear.", "Generalization across protein targets is asserted but specifics (e.g., out-of-distribution performance or protein diversity) are not elaborated.", "ARC iterative refinement and 'biological consistency enforcement' are not clearly described\u2014may require more clarification.", "Repetition regarding new datasets in abstract could be streamlined."], "suggestions": ["Expand on the specific implementation and computational cost of the PSR and LGR modules in the main paper.", "Provide detailed baselines and statistical significance analyses in the results section.", "Clarify how ARC module enforces biological consistency, ideally with ablation or qualitative results.", "Include or refer to more information on dataset diversity and the generalization assessment protocol.", "Edit the text to reduce redundancy about the dataset contributions for clarity."], "summary": "The paper introduces DEL-Ranking, a novel framework targeting two core challenges in DNA-encoded library affinity prediction: distribution noise and the disconnect between read count and binding affinity. Through an innovative ranking-based denoising approach and an activity-referenced correction module, the framework achieves improved accuracy and interpretability. The release of well-annotated DEL datasets is a significant resource. While the methodology is promising and results are compelling, clarifications on implementation, evaluation, and generalization are necessary to fully assess its impact."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses two major sources of noise (distribution noise and systematic shift) in DEL data.\",\n    \"Proposes a novel dual-perspective ranking strategy (Pair-wise Soft Rank and List-wise Global Rank) for denoising.\",\n    \"Introduces a biologically-motivated Activity-Referenced Correction (ARC) module for bridging the gap between read counts and binding affinity.\",\n    \"Releases three comprehensive and experimentally validated DEL datasets including 2D, 3D, and activity labels, which is valuable for the community.\",\n    \"Demonstrates state-of-the-art performance with strong generalization across diverse protein targets.\",\n    \"Provides evidence of interpretability by identifying key functional groups associated with binding.\"\n  ],\n  \"weaknesses\": [\n    \"Details on the implementation and computational complexity of PSR and LGR constraints are limited in the abstract.\",\n    \"Evaluation details (e.g., baselines compared, statistical significance) are unclear.\",\n    \"Generalization across protein targets is asserted but specifics (e.g., out-of-distribution performance or protein diversity) are not elaborated.\",\n    \"ARC iterative refinement and 'biological consistency enforcement' are not clearly described\u2014may require more clarification.\",\n    \"Repetition regarding new datasets in abstract could be streamlined.\"\n  ],\n  \"suggestions\": [\n    \"Expand on the specific implementation and computational cost of the PSR and LGR modules in the main paper.\",\n    \"Provide detailed baselines and statistical significance analyses in the results section.\",\n    \"Clarify how ARC module enforces biological consistency, ideally with ablation or qualitative results.\",\n    \"Include or refer to more information on dataset diversity and the generalization assessment protocol.\",\n    \"Edit the text to reduce redundancy about the dataset contributions for clarity.\"\n  ],\n  \"summary\": \"The paper introduces DEL-Ranking, a novel framework targeting two core challenges in DNA-encoded library affinity prediction: distribution noise and the disconnect between read count and binding affinity. Through an innovative ranking-based denoising approach and an activity-referenced correction module, the framework achieves improved accuracy and interpretability. The release of well-annotated DEL datasets is a significant resource. While the methodology is promising and results are compelling, clarifications on implementation, evaluation, and generalization are necessary to fully assess its impact.\"\n}", "usage": {"prompt_tokens": 378, "completion_tokens": 478, "total_tokens": 856}, "timestamp": "2025-12-29T07:08:31.357110"}
{"paper_id": "9poxbngJzR", "model": "openai/gpt-4.1", "response": {"score": 8, "strengths": ["Addresses an important problem: overconfident errors in LLMs, especially in high-stakes domains.", "Introduces CP-OPT, an optimization framework for learning score functions for conformal prediction, improving over heuristics.", "Proposes CROQ, an innovative downstream method leveraging conformal prediction sets to actively improve LLM answer accuracy.", "Empirical validation across multiple datasets (MMLU, ToolAlpaca, TruthfulQA) and models (Llama-3, Phi-3).", "Demonstrates both theoretical and practical improvement (coverage guarantee and reduced prediction set size)."], "weaknesses": ["Details on the optimization method for CP-OPT and how scores are learned are not evident from the abstract.", "Extent of coverage guarantee retention and concrete set size reductions are unclear without quantitative results.", "Potential computational overhead of score optimization and sequential conformal revision not discussed.", "Generalizability of CROQ beyond MCQs or to non-multiple-choice settings is not addressed."], "suggestions": ["Provide detailed methodology for CP-OPT, including objective, constraints, and optimization technique.", "Include comprehensive quantitative results (set size distributions, accuracy deltas, runtime) to substantiate claims.", "Analyze scenarios where the coverage guarantee may degrade or if there are distributional/shift limitations.", "Discuss limitations and potential avenues for extending CROQ to broader question types or tasks."], "summary": "This paper presents a novel framework for improving LLM reliability by learning optimal scores for conformal prediction (CP-OPT) and extends CP utility with conformal revision of questions (CROQ) inspired by the Monty Hall problem. The approach addresses both uncertainty quantification and active accuracy improvement for decision-making tasks, validated on strong baselines. The work is innovative and promising, though more methodological clarity, quantitative detail, and discussion of generalizability would enhance the contribution."}, "raw": "{\n  \"score\": 8,\n  \"strengths\": [\n    \"Addresses an important problem: overconfident errors in LLMs, especially in high-stakes domains.\",\n    \"Introduces CP-OPT, an optimization framework for learning score functions for conformal prediction, improving over heuristics.\",\n    \"Proposes CROQ, an innovative downstream method leveraging conformal prediction sets to actively improve LLM answer accuracy.\",\n    \"Empirical validation across multiple datasets (MMLU, ToolAlpaca, TruthfulQA) and models (Llama-3, Phi-3).\",\n    \"Demonstrates both theoretical and practical improvement (coverage guarantee and reduced prediction set size).\"\n  ],\n  \"weaknesses\": [\n    \"Details on the optimization method for CP-OPT and how scores are learned are not evident from the abstract.\",\n    \"Extent of coverage guarantee retention and concrete set size reductions are unclear without quantitative results.\",\n    \"Potential computational overhead of score optimization and sequential conformal revision not discussed.\",\n    \"Generalizability of CROQ beyond MCQs or to non-multiple-choice settings is not addressed.\"\n  ],\n  \"suggestions\": [\n    \"Provide detailed methodology for CP-OPT, including objective, constraints, and optimization technique.\",\n    \"Include comprehensive quantitative results (set size distributions, accuracy deltas, runtime) to substantiate claims.\",\n    \"Analyze scenarios where the coverage guarantee may degrade or if there are distributional/shift limitations.\",\n    \"Discuss limitations and potential avenues for extending CROQ to broader question types or tasks.\"\n  ],\n  \"summary\": \"This paper presents a novel framework for improving LLM reliability by learning optimal scores for conformal prediction (CP-OPT) and extends CP utility with conformal revision of questions (CROQ) inspired by the Monty Hall problem. The approach addresses both uncertainty quantification and active accuracy improvement for decision-making tasks, validated on strong baselines. The work is innovative and promising, though more methodological clarity, quantitative detail, and discussion of generalizability would enhance the contribution.\"\n}", "usage": {"prompt_tokens": 403, "completion_tokens": 420, "total_tokens": 823}, "timestamp": "2025-12-29T07:08:43.150547"}
{"paper_id": "cPmLjxedbD", "model": "openai/gpt-4.1", "response": {"score": 4, "strengths": ["Interesting analogy between animal olfaction and machine intelligence.", "Emphasizes the importance of interpretability in artificial sensing.", "Proposes use of linear mixture models (LMMs) over deep learning, aiming for analytic tractability."], "weaknesses": ["Lacks substantive technical contributions or empirical results.", "The proposal is high-level and abstract, with no clear experimental plan or validation.", "Insufficient justification for why LMMs would outperform or complement modern deep learning approaches in real-world chemosensing tasks.", "Does not reference or compare to related work in chemosensing or interpretable machine learning."], "suggestions": ["Provide concrete mathematical formulations or demonstrate derived analytical limits for specific chemosensing scenarios.", "Include empirical validation or simulated experiments to support claims.", "Clarify the scope and novelty of the engineering theory being proposed.", "Discuss potential limitations of LMMs and how they compare quantitatively and qualitatively to deep learning methods.", "Review and cite related work in machine olfaction, interpretable ML, and statistical learning."], "summary": "The paper raises a thought-provoking question regarding the value of interpretable models in primitive sensory intelligence, using chemosensing as a case study. However, it remains speculative, lacking rigorous technical development or any empirical evaluation. To be competitive, the paper should significantly elaborate on its theoretical contributions, provide experimental evidence, and situate itself within the context of existing literature."}, "raw": "{\n  \"score\": 4,\n  \"strengths\": [\n    \"Interesting analogy between animal olfaction and machine intelligence.\",\n    \"Emphasizes the importance of interpretability in artificial sensing.\",\n    \"Proposes use of linear mixture models (LMMs) over deep learning, aiming for analytic tractability.\"\n  ],\n  \"weaknesses\": [\n    \"Lacks substantive technical contributions or empirical results.\",\n    \"The proposal is high-level and abstract, with no clear experimental plan or validation.\",\n    \"Insufficient justification for why LMMs would outperform or complement modern deep learning approaches in real-world chemosensing tasks.\",\n    \"Does not reference or compare to related work in chemosensing or interpretable machine learning.\"\n  ],\n  \"suggestions\": [\n    \"Provide concrete mathematical formulations or demonstrate derived analytical limits for specific chemosensing scenarios.\",\n    \"Include empirical validation or simulated experiments to support claims.\",\n    \"Clarify the scope and novelty of the engineering theory being proposed.\",\n    \"Discuss potential limitations of LMMs and how they compare quantitatively and qualitatively to deep learning methods.\",\n    \"Review and cite related work in machine olfaction, interpretable ML, and statistical learning.\"\n  ],\n  \"summary\": \"The paper raises a thought-provoking question regarding the value of interpretable models in primitive sensory intelligence, using chemosensing as a case study. However, it remains speculative, lacking rigorous technical development or any empirical evaluation. To be competitive, the paper should significantly elaborate on its theoretical contributions, provide experimental evidence, and situate itself within the context of existing literature.\"\n}", "usage": {"prompt_tokens": 237, "completion_tokens": 327, "total_tokens": 564}, "timestamp": "2025-12-29T07:08:55.503721"}
